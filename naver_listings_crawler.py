#!/usr/bin/env python3
"""
ë„¤ì´ë²„ë¶€ë™ì‚° ë§¤ë¬¼í˜¸ê°€ ì „ìš© í¬ë¡¤ëŸ¬
- í˜„ì¬ ì‹œì¥ì— ë‚˜ì˜¨ ë§¤ë¬¼ë“¤ì˜ í˜¸ê°€ ì •ë³´ ìˆ˜ì§‘
- ë§¤ë§¤/ì „ì„¸/ì›”ì„¸ êµ¬ë¶„í•˜ì—¬ ìˆ˜ì§‘
- ì‹¤ê±°ë˜ê°€ì™€ ë¶„ë¦¬ëœ ë³„ë„ ë°ì´í„°ë² ì´ìŠ¤
"""

import asyncio
import aiohttp
import sqlite3
import json
import time
import random
import logging
from datetime import datetime
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
import re

# Playwright ì‚¬ìš©
from playwright.async_api import async_playwright, Page, Browser, BrowserContext

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('naver_listings_crawling.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class PropertyListing:
    """ë§¤ë¬¼ í˜¸ê°€ ì •ë³´"""
    listing_id: str
    complex_id: str
    complex_name: str
    deal_type: str  # ë§¤ë§¤, ì „ì„¸, ì›”ì„¸
    price: str
    monthly_rent: Optional[str] = None
    deposit: Optional[str] = None
    area_m2: Optional[float] = None
    area_pyeong: Optional[float] = None
    floor: Optional[int] = None
    total_floor: Optional[int] = None
    direction: Optional[str] = None
    room_type: Optional[str] = None
    maintenance_cost: Optional[str] = None
    move_in_date: Optional[str] = None
    heating_type: Optional[str] = None
    parking: Optional[str] = None
    elevator: Optional[str] = None
    agent_name: Optional[str] = None
    agent_phone: Optional[str] = None
    region: str = ""
    district: str = ""
    address: str = ""
    listing_url: str = ""
    image_urls: List[str] = None
    description: str = ""
    created_date: Optional[str] = None
    updated_date: Optional[str] = None
    view_count: Optional[int] = None
    crawled_at: str = ""

class NaverListingsCrawler:
    """ë„¤ì´ë²„ë¶€ë™ì‚° ë§¤ë¬¼í˜¸ê°€ ì „ìš© í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.db_path = "naver_property_listings.db"
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        
        self.init_database()
        
        # ì£¼ìš” ë„ì‹œ/ì§€ì—­ URL ë§¤í•‘
        self.target_regions = {
            # ì„œìš¸ ì£¼ìš” êµ¬
            'ê°•ë‚¨êµ¬': 'https://new.land.naver.com/complexes?ms=37.5172,127.0473,15&a=APT:ABYG:JGC:PRE&e=RETAIL&cortarNo=11680',
            'ì„œì´ˆêµ¬': 'https://new.land.naver.com/complexes?ms=37.4837,127.0324,15&a=APT:ABYG:JGC:PRE&e=RETAIL&cortarNo=11650',
            'ì†¡íŒŒêµ¬': 'https://new.land.naver.com/complexes?ms=37.5145,127.1066,15&a=APT:ABYG:JGC:PRE&e=RETAIL&cortarNo=11710',
            'ê°•ì„œêµ¬': 'https://new.land.naver.com/complexes?ms=37.5509,126.8495,15&a=APT:ABYG:JGC:PRE&e=RETAIL&cortarNo=11500',
            'ë§ˆí¬êµ¬': 'https://new.land.naver.com/complexes?ms=37.5615,126.9087,15&a=APT:ABYG:JGC:PRE&e=RETAIL&cortarNo=11440',
            
            # ê²½ê¸°ë„ ì£¼ìš” ì§€ì—­
            'ìˆ˜ì›ì‹œ': 'https://new.land.naver.com/complexes?ms=37.2636,127.0286,13&a=APT:ABYG:JGC:PRE&e=RETAIL&cortarNo=41110',
            'ì„±ë‚¨ì‹œ': 'https://new.land.naver.com/complexes?ms=37.4201,127.1262,13&a=APT:ABYG:JGC:PRE&e=RETAIL&cortarNo=41130',
            'ê³ ì–‘ì‹œ': 'https://new.land.naver.com/complexes?ms=37.6583,126.8320,13&a=APT:ABYG:JGC:PRE&e=RETAIL&cortarNo=41280',
            'ìš©ì¸ì‹œ': 'https://new.land.naver.com/complexes?ms=37.2342,127.2017,13&a=APT:ABYG:JGC:PRE&e=RETAIL&cortarNo=41460',
            
            # ë¶€ì‚° ì£¼ìš” êµ¬
            'í•´ìš´ëŒ€êµ¬': 'https://new.land.naver.com/complexes?ms=35.1796,129.1756,15&a=APT:ABYG:JGC:PRE&e=RETAIL&cortarNo=26350',
            'ë¶€ì‚°ì§„êµ¬': 'https://new.land.naver.com/complexes?ms=35.1641,129.0534,15&a=APT:ABYG:JGC:PRE&e=RETAIL&cortarNo=26230',
        }
        
        self.stats = {
            'regions_processed': 0,
            'complexes_found': 0,
            'listings_collected': 0,
            'start_time': datetime.now()
        }
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        logger.info("ë§¤ë¬¼í˜¸ê°€ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ë§¤ë¬¼ í˜¸ê°€ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS property_listings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                listing_id TEXT UNIQUE NOT NULL,
                complex_id TEXT,
                complex_name TEXT,
                deal_type TEXT,
                price TEXT,
                monthly_rent TEXT,
                deposit TEXT,
                area_m2 REAL,
                area_pyeong REAL,
                floor INTEGER,
                total_floor INTEGER,
                direction TEXT,
                room_type TEXT,
                maintenance_cost TEXT,
                move_in_date TEXT,
                heating_type TEXT,
                parking TEXT,
                elevator TEXT,
                agent_name TEXT,
                agent_phone TEXT,
                region TEXT,
                district TEXT,
                address TEXT,
                listing_url TEXT,
                image_urls TEXT,
                description TEXT,
                created_date TEXT,
                updated_date TEXT,
                view_count INTEGER,
                crawled_at TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # ì•„íŒŒíŠ¸ ë‹¨ì§€ ê¸°ë³¸ ì •ë³´ (ë§¤ë¬¼í˜¸ê°€ìš©)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS apartment_complexes_listings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                complex_id TEXT UNIQUE NOT NULL,
                complex_name TEXT,
                address TEXT,
                region TEXT,
                district TEXT,
                total_households INTEGER,
                parking_spaces INTEGER,
                construction_company TEXT,
                completion_date TEXT,
                heating_type TEXT,
                current_listings_count INTEGER DEFAULT 0,
                avg_price_sale REAL,
                avg_price_jeonse REAL,
                avg_price_monthly REAL,
                min_area REAL,
                max_area REAL,
                complex_url TEXT,
                last_updated TEXT,
                crawled_at TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # í¬ë¡¤ë§ ë¡œê·¸ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS crawling_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                region TEXT,
                action TEXT,
                status TEXT,
                items_found INTEGER DEFAULT 0,
                error_message TEXT,
                start_time TEXT,
                end_time TEXT,
                duration_seconds INTEGER,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_listing_complex ON property_listings(complex_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_listing_deal_type ON property_listings(deal_type)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_listing_region ON property_listings(region)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_complex_region ON apartment_complexes_listings(region)")
        
        conn.commit()
        conn.close()
        logger.info("ë§¤ë¬¼í˜¸ê°€ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def init_playwright(self):
        """Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
        logger.info("Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì¤‘...")
        
        self.playwright = await async_playwright().start()
        
        # ì•ˆí‹° ë””í…ì…˜ ë¸Œë¼ìš°ì € ì„¤ì •
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-features=VizDisplayCompositor',
                '--disable-web-security',
                '--disable-features=TranslateUI',
                '--no-first-run',
                '--no-default-browser-check',
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-extensions'
            ]
        )
        
        # ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        
        # í˜ì´ì§€ ìƒì„±
        self.page = await self.context.new_page()
        
        # ì•ˆí‹° ë””í…ì…˜ ìŠ¤í¬ë¦½íŠ¸
        await self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
            
            delete window.console.debug;
            delete window.console.clear;
            
            Object.defineProperty(navigator, 'languages', {
                get: () => ['ko-KR', 'ko', 'en-US', 'en'],
            });
        """)
        
        logger.info("Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def extract_complex_ids_from_region(self, region_url: str, region_name: str) -> List[str]:
        """ì§€ì—­ í˜ì´ì§€ì—ì„œ ì•„íŒŒíŠ¸ ë‹¨ì§€ ID ëª©ë¡ ì¶”ì¶œ"""
        logger.info(f"{region_name} ì§€ì—­ì˜ ì•„íŒŒíŠ¸ ë‹¨ì§€ ëª©ë¡ ì¶”ì¶œ ì¤‘...")
        
        try:
            await self.page.goto(region_url, wait_until="networkidle", timeout=30000)
            await asyncio.sleep(5)
            
            # í˜ì´ì§€ì—ì„œ ì•„íŒŒíŠ¸ ë‹¨ì§€ ë§í¬ ì¶”ì¶œ
            complex_ids = await self.page.evaluate("""
                () => {
                    const complexIds = new Set();
                    
                    // ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ë‹¨ì§€ ë§í¬ ì°¾ê¸°
                    const linkSelectors = [
                        'a[href*="/complexes/"]',
                        '[data-complex-no]',
                        '.complex_link',
                        '.item_link'
                    ];
                    
                    for (const selector of linkSelectors) {
                        const elements = document.querySelectorAll(selector);
                        elements.forEach(el => {
                            // hrefì—ì„œ ë‹¨ì§€ ID ì¶”ì¶œ
                            const href = el.href || el.getAttribute('href');
                            if (href) {
                                const match = href.match(/\/complexes\/(\d+)/);
                                if (match) {
                                    complexIds.add(match[1]);
                                }
                            }
                            
                            // data-complex-noì—ì„œ ë‹¨ì§€ ID ì¶”ì¶œ
                            const complexNo = el.getAttribute('data-complex-no');
                            if (complexNo && /^\d+$/.test(complexNo)) {
                                complexIds.add(complexNo);
                            }
                        });
                    }
                    
                    // JavaScript ë³€ìˆ˜ì—ì„œ ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ ì‹œë„
                    const scripts = document.querySelectorAll('script');
                    scripts.forEach(script => {
                        if (script.textContent) {
                            const complexMatches = script.textContent.match(/complexNo[\"']?\s*:\s*[\"']?(\d+)[\"']?/g);
                            if (complexMatches) {
                                complexMatches.forEach(match => {
                                    const idMatch = match.match(/(\d+)/);
                                    if (idMatch) {
                                        complexIds.add(idMatch[1]);
                                    }
                                });
                            }
                        }
                    });
                    
                    return Array.from(complexIds);
                }
            """)
            
            logger.info(f"{region_name}: {len(complex_ids)}ê°œ ì•„íŒŒíŠ¸ ë‹¨ì§€ ë°œê²¬")
            return complex_ids
            
        except Exception as e:
            logger.error(f"{region_name} ë‹¨ì§€ ëª©ë¡ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    async def crawl_complex_listings(self, complex_id: str) -> Dict[str, Any]:
        """ê°œë³„ ì•„íŒŒíŠ¸ ë‹¨ì§€ì˜ ë§¤ë¬¼ ì •ë³´ í¬ë¡¤ë§"""
        logger.info(f"ë‹¨ì§€ {complex_id} ë§¤ë¬¼ ì •ë³´ í¬ë¡¤ë§ ì¤‘...")
        
        result = {
            'complex_info': None,
            'listings': [],
            'error': None
        }
        
        try:
            # ë‹¨ì§€ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
            complex_url = f"https://new.land.naver.com/complexes/{complex_id}"
            await self.page.goto(complex_url, wait_until="networkidle", timeout=30000)
            await asyncio.sleep(3)
            
            # ë‹¨ì§€ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            complex_info = await self.page.evaluate("""
                () => {
                    const info = {
                        complex_id: window.location.pathname.match(/\/complexes\/(\d+)/)?.[1] || '',
                        complex_name: '',
                        address: '',
                        total_households: null,
                        completion_date: '',
                        heating_type: ''
                    };
                    
                    // ë‹¨ì§€ëª… ì¶”ì¶œ
                    const nameSelectors = [
                        'h1.complex_title',
                        '.complex_name',
                        'h1',
                        '[class*="title"]'
                    ];
                    
                    for (const selector of nameSelectors) {
                        const el = document.querySelector(selector);
                        if (el && el.textContent.trim()) {
                            info.complex_name = el.textContent.trim();
                            break;
                        }
                    }
                    
                    // ì£¼ì†Œ ì¶”ì¶œ
                    const addressSelectors = [
                        '.complex_address',
                        '.address',
                        '[class*="address"]'
                    ];
                    
                    for (const selector of addressSelectors) {
                        const el = document.querySelector(selector);
                        if (el && el.textContent.trim()) {
                            info.address = el.textContent.trim();
                            break;
                        }
                    }
                    
                    // ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
                    const fullText = document.body.textContent;
                    
                    // ì„¸ëŒ€ìˆ˜
                    const householdMatch = fullText.match(/(\d+)\s*ì„¸ëŒ€/);
                    if (householdMatch) {
                        info.total_households = parseInt(householdMatch[1]);
                    }
                    
                    // ì¤€ê³µë…„ë„
                    const yearMatch = fullText.match(/(19|20)\d{2}ë…„?\s*ì¤€?ê³µ?/);
                    if (yearMatch) {
                        info.completion_date = yearMatch[0];
                    }
                    
                    // ë‚œë°©ë°©ì‹
                    const heatingMatch = fullText.match(/(ê°œë³„ë‚œë°©|ì¤‘ì•™ë‚œë°©|ì§€ì—­ë‚œë°©|ë„ì‹œê°€ìŠ¤)/);
                    if (heatingMatch) {
                        info.heating_type = heatingMatch[0];
                    }
                    
                    return info;
                }
            """)
            
            result['complex_info'] = complex_info
            
            # ë§¤ë¬¼ ì •ë³´ ì¶”ì¶œ (ê±°ë˜ìœ í˜•ë³„)
            deal_types = [
                {'type': 'ë§¤ë§¤', 'selector': 'text="ë§¤ë§¤"'},
                {'type': 'ì „ì„¸', 'selector': 'text="ì „ì„¸"'},
                {'type': 'ì›”ì„¸', 'selector': 'text="ì›”ì„¸"'}
            ]
            
            for deal_type_info in deal_types:
                deal_type = deal_type_info['type']
                logger.info(f"  {deal_type} ë§¤ë¬¼ ìˆ˜ì§‘ ì¤‘...")
                
                try:
                    # ê±°ë˜ìœ í˜• íƒ­ í´ë¦­ ì‹œë„
                    await self.page.click(deal_type_info['selector'], timeout=5000)
                    await asyncio.sleep(2)
                except:
                    logger.warning(f"  {deal_type} íƒ­ í´ë¦­ ì‹¤íŒ¨, ê¸°ë³¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œ")
                
                # ë§¤ë¬¼ ëª©ë¡ ì¶”ì¶œ
                listings = await self.page.evaluate(f"""
                    (dealType) => {{
                        const listings = [];
                        
                        // ë§¤ë¬¼ ê´€ë ¨ ì…€ë ‰í„°ë“¤
                        const listingSelectors = [
                            '.item_link',
                            '.article_item', 
                            '.property_item',
                            '[class*="item"]',
                            '[class*="article"]',
                            '.list_item'
                        ];
                        
                        for (const selector of listingSelectors) {{
                            const elements = document.querySelectorAll(selector);
                            
                            elements.forEach((el, index) => {{
                                const text = el.textContent.trim();
                                
                                // ë¶€ë™ì‚° ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                                if (text && (
                                    text.includes('ì–µ') || 
                                    text.includes('ë§Œì›') ||
                                    text.includes('ì „ì„¸') ||
                                    text.includes('ì›”ì„¸') ||
                                    text.includes('ë§¤ë§¤') ||
                                    text.includes('ã¡') ||
                                    text.includes('í‰') ||
                                    text.includes('ì¸µ')
                                ) && text.length > 20) {{
                                    
                                    // ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                                    const priceMatch = text.match(/(\d+)ì–µ\s*(\d+)?/);
                                    const monthlyMatch = text.match(/ì›”ì„¸\s*(\d+)/);
                                    const depositMatch = text.match(/ë³´ì¦ê¸ˆ\s*(\d+)ì–µ?\s*(\d+)?/);
                                    const areaMatch = text.match(/(\d+\.?\d*)ã¡/);
                                    const pyeongMatch = text.match(/(\d+\.?\d*)í‰/);
                                    const floorMatch = text.match(/(\d+)ì¸µ/);
                                    const roomMatch = text.match(/(\d+)ë°©/);
                                    
                                    // ë§í¬ URL ì¶”ì¶œ
                                    const linkEl = el.querySelector('a') || el;
                                    const href = linkEl.href || '';
                                    
                                    const listing = {{
                                        listing_id: `{complex_id}_${{dealType}}_${{index}}_${{Date.now()}}`,
                                        deal_type: dealType,
                                        price: priceMatch ? priceMatch[0] : '',
                                        monthly_rent: monthlyMatch ? monthlyMatch[0] : '',
                                        deposit: depositMatch ? depositMatch[0] : '',
                                        area_m2: areaMatch ? parseFloat(areaMatch[1]) : null,
                                        area_pyeong: pyeongMatch ? parseFloat(pyeongMatch[1]) : null,
                                        floor: floorMatch ? parseInt(floorMatch[1]) : null,
                                        room_type: roomMatch ? roomMatch[0] : '',
                                        listing_url: href,
                                        raw_text: text.substring(0, 300),
                                        direction: '',
                                        maintenance_cost: '',
                                        agent_name: '',
                                        agent_phone: ''
                                    }};
                                    
                                    // ë°©í–¥ ì •ë³´ ì¶”ì¶œ
                                    const directionMatch = text.match(/(ë‚¨í–¥|ë¶í–¥|ë™í–¥|ì„œí–¥|ë‚¨ë™í–¥|ë‚¨ì„œí–¥|ë¶ë™í–¥|ë¶ì„œí–¥)/);
                                    if (directionMatch) {{
                                        listing.direction = directionMatch[0];
                                    }}
                                    
                                    // ê´€ë¦¬ë¹„ ì¶”ì¶œ
                                    const maintenanceMatch = text.match(/ê´€ë¦¬ë¹„\s*(\d+)ë§Œ?ì›?/);
                                    if (maintenanceMatch) {{
                                        listing.maintenance_cost = maintenanceMatch[0];
                                    }}
                                    
                                    listings.push(listing);
                                }}
                            }});
                            
                            if (listings.length > 0) break; // ë§¤ë¬¼ì„ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
                        }}
                        
                        return listings.slice(0, 30); // ìµœëŒ€ 30ê°œ
                    }}
                """, deal_type)
                
                # ê²°ê³¼ì— ì¶”ê°€
                for listing in listings:
                    listing['complex_id'] = complex_id
                    listing['complex_name'] = complex_info.get('complex_name', '')
                    listing['address'] = complex_info.get('address', '')
                    listing['crawled_at'] = datetime.now().isoformat()
                
                result['listings'].extend(listings)
                logger.info(f"    {deal_type}: {len(listings)}ê°œ ë§¤ë¬¼ ë°œê²¬")
                
                await asyncio.sleep(1)
            
            logger.info(f"ë‹¨ì§€ {complex_id} ì´ ë§¤ë¬¼: {len(result['listings'])}ê°œ")
            return result
            
        except Exception as e:
            error_msg = f"ë‹¨ì§€ {complex_id} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}"
            logger.error(error_msg)
            result['error'] = error_msg
            return result
    
    def save_complex_info(self, complex_info: Dict[str, Any], region: str):
        """ì•„íŒŒíŠ¸ ë‹¨ì§€ ì •ë³´ ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT OR REPLACE INTO apartment_complexes_listings 
                (complex_id, complex_name, address, region, total_households, 
                 completion_date, heating_type, complex_url, last_updated, crawled_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                complex_info.get('complex_id', ''),
                complex_info.get('complex_name', ''),
                complex_info.get('address', ''),
                region,
                complex_info.get('total_households'),
                complex_info.get('completion_date', ''),
                complex_info.get('heating_type', ''),
                f"https://new.land.naver.com/complexes/{complex_info.get('complex_id', '')}",
                datetime.now().isoformat(),
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ë‹¨ì§€ ì •ë³´ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def save_listings(self, listings: List[Dict[str, Any]], region: str):
        """ë§¤ë¬¼ ì •ë³´ ì €ì¥"""
        if not listings:
            return
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for listing in listings:
                cursor.execute("""
                    INSERT OR REPLACE INTO property_listings 
                    (listing_id, complex_id, complex_name, deal_type, price, monthly_rent, deposit,
                     area_m2, area_pyeong, floor, direction, room_type, maintenance_cost,
                     agent_name, agent_phone, region, address, listing_url, crawled_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    listing.get('listing_id', ''),
                    listing.get('complex_id', ''),
                    listing.get('complex_name', ''),
                    listing.get('deal_type', ''),
                    listing.get('price', ''),
                    listing.get('monthly_rent', ''),
                    listing.get('deposit', ''),
                    listing.get('area_m2'),
                    listing.get('area_pyeong'),
                    listing.get('floor'),
                    listing.get('direction', ''),
                    listing.get('room_type', ''),
                    listing.get('maintenance_cost', ''),
                    listing.get('agent_name', ''),
                    listing.get('agent_phone', ''),
                    region,
                    listing.get('address', ''),
                    listing.get('listing_url', ''),
                    listing.get('crawled_at', '')
                ))
            
            conn.commit()
            conn.close()
            
            self.stats['listings_collected'] += len(listings)
            logger.info(f"{len(listings)}ê°œ ë§¤ë¬¼ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë§¤ë¬¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def log_crawling_activity(self, region: str, action: str, status: str, 
                             items_found: int = 0, error_msg: str = None, duration: int = 0):
        """í¬ë¡¤ë§ í™œë™ ë¡œê·¸"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT INTO crawling_logs 
                (region, action, status, items_found, error_message, 
                 start_time, end_time, duration_seconds)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                region, action, status, items_found, error_msg,
                datetime.now().isoformat(), datetime.now().isoformat(), duration
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ë¡œê·¸ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    async def crawl_region(self, region_name: str, region_url: str):
        """ì§€ì—­ë³„ ë§¤ë¬¼í˜¸ê°€ í¬ë¡¤ë§"""
        logger.info(f"\n=== {region_name} ë§¤ë¬¼í˜¸ê°€ í¬ë¡¤ë§ ì‹œì‘ ===")
        start_time = datetime.now()
        
        try:
            # 1ë‹¨ê³„: ì•„íŒŒíŠ¸ ë‹¨ì§€ ëª©ë¡ ì¶”ì¶œ
            complex_ids = await self.extract_complex_ids_from_region(region_url, region_name)
            
            if not complex_ids:
                logger.warning(f"{region_name}: ì•„íŒŒíŠ¸ ë‹¨ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                self.log_crawling_activity(region_name, "ë‹¨ì§€ëª©ë¡ì¶”ì¶œ", "ì‹¤íŒ¨", 0, "ë‹¨ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return
            
            logger.info(f"{region_name}: {len(complex_ids)}ê°œ ë‹¨ì§€ì—ì„œ ë§¤ë¬¼ ìˆ˜ì§‘ ì‹œì‘")
            
            # 2ë‹¨ê³„: ê° ë‹¨ì§€ë³„ ë§¤ë¬¼ ì •ë³´ ìˆ˜ì§‘
            region_listings = 0
            
            for i, complex_id in enumerate(complex_ids, 1):
                try:
                    logger.info(f"  [{i}/{len(complex_ids)}] ë‹¨ì§€ {complex_id} ì²˜ë¦¬ ì¤‘...")
                    
                    # ë‹¨ì§€ ë§¤ë¬¼ í¬ë¡¤ë§
                    result = await self.crawl_complex_listings(complex_id)
                    
                    if result['complex_info']:
                        # ë‹¨ì§€ ì •ë³´ ì €ì¥
                        self.save_complex_info(result['complex_info'], region_name)
                        self.stats['complexes_found'] += 1
                    
                    if result['listings']:
                        # ë§¤ë¬¼ ì •ë³´ ì €ì¥
                        self.save_listings(result['listings'], region_name)
                        region_listings += len(result['listings'])
                    
                    if result['error']:
                        logger.warning(f"    ì˜¤ë¥˜: {result['error']}")
                    
                    # ë‹¨ì§€ë³„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                    await asyncio.sleep(random.uniform(3, 6))
                    
                except Exception as e:
                    logger.error(f"    ë‹¨ì§€ {complex_id} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            # ì™„ë£Œ ë¡œê·¸
            duration = (datetime.now() - start_time).total_seconds()
            self.log_crawling_activity(region_name, "ì „ì²´í¬ë¡¤ë§", "ì™„ë£Œ", region_listings, None, int(duration))
            
            logger.info(f"=== {region_name} ì™„ë£Œ: ë‹¨ì§€ {len(complex_ids)}ê°œ, ë§¤ë¬¼ {region_listings}ê°œ ===")
            self.stats['regions_processed'] += 1
            
        except Exception as e:
            logger.error(f"{region_name} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            duration = (datetime.now() - start_time).total_seconds()
            self.log_crawling_activity(region_name, "ì „ì²´í¬ë¡¤ë§", "ì˜¤ë¥˜", 0, str(e), int(duration))
    
    async def start_listings_crawling(self):
        """ë§¤ë¬¼í˜¸ê°€ í¬ë¡¤ë§ ì‹œì‘"""
        logger.info("ğŸ  ë„¤ì´ë²„ë¶€ë™ì‚° ë§¤ë¬¼í˜¸ê°€ í¬ë¡¤ë§ ì‹œì‘!")
        logger.info(f"ğŸ“Š ëŒ€ìƒ ì§€ì—­: {len(self.target_regions)}ê°œ")
        
        try:
            await self.init_playwright()
            
            for region_name, region_url in self.target_regions.items():
                await self.crawl_region(region_name, region_url)
                
                # ì§€ì—­ê°„ ë”œë ˆì´
                await asyncio.sleep(random.uniform(10, 20))
            
            # ìµœì¢… ê²°ê³¼
            self.print_final_results()
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            await self.cleanup()
    
    def print_final_results(self):
        """ìµœì¢… ê²°ê³¼ ì¶œë ¥"""
        duration = datetime.now() - self.stats['start_time']
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ ë„¤ì´ë²„ë¶€ë™ì‚° ë§¤ë¬¼í˜¸ê°€ í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info("="*60)
        logger.info(f"ğŸ“Š ìµœì¢… í†µê³„:")
        logger.info(f"  ğŸ™ï¸ ì²˜ë¦¬ëœ ì§€ì—­: {self.stats['regions_processed']}ê°œ")
        logger.info(f"  ğŸ¢ ë°œê²¬ëœ ë‹¨ì§€: {self.stats['complexes_found']}ê°œ")
        logger.info(f"  ğŸ  ìˆ˜ì§‘ëœ ë§¤ë¬¼: {self.stats['listings_collected']}ê°œ")
        logger.info(f"  â±ï¸ ì†Œìš” ì‹œê°„: {duration}")
        
        if duration.total_seconds() > 0:
            listings_per_hour = self.stats['listings_collected'] / (duration.total_seconds() / 3600)
            logger.info(f"  ğŸ“ˆ ì‹œê°„ë‹¹ í‰ê· : {listings_per_hour:.0f}ê°œ ë§¤ë¬¼/ì‹œê°„")
        
        logger.info("="*60)
    
    async def cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        logger.info("ì •ë¦¬ ì‘ì—… ì¤‘...")
        
        if self.page:
            await self.page.close()
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        
        logger.info("ì •ë¦¬ ì™„ë£Œ")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ë„¤ì´ë²„ë¶€ë™ì‚° ë§¤ë¬¼í˜¸ê°€ ì „ìš© í¬ë¡¤ëŸ¬ v1.0")
    print("="*40)
    print("- í˜„ì¬ ì‹œì¥ì— ë‚˜ì˜¨ ë§¤ë¬¼ë“¤ì˜ í˜¸ê°€ ì •ë³´ ìˆ˜ì§‘")
    print("- ë§¤ë§¤/ì „ì„¸/ì›”ì„¸ êµ¬ë¶„í•˜ì—¬ ìˆ˜ì§‘")
    print("- ì‹¤ê±°ë˜ê°€ì™€ ë¶„ë¦¬ëœ ë³„ë„ ë°ì´í„°ë² ì´ìŠ¤")
    print("="*40)
    
    crawler = NaverListingsCrawler()
    
    try:
        await crawler.start_listings_crawling()
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())