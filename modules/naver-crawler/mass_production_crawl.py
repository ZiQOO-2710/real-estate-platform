#!/usr/bin/env python3

"""
ê¸°ì¡´ Enhanced Crawlerë¥¼ í™œìš©í•œ ëŒ€ëŸ‰ í”„ë¡œë•ì…˜ í¬ë¡¤ë§
VPN + ìŠ¤í…”ìŠ¤ ëª¨ë“œë¡œ ì•ˆì „í•˜ê²Œ 300ê°œ/ì¼ ëª©í‘œ
"""

import asyncio
import sqlite3
import random
from datetime import datetime
from core.enhanced_naver_crawler import crawl_enhanced_single
import time

class MassProductionCrawler:
    def __init__(self):
        self.db_path = '/Users/seongjunkim/projects/real-estate-platform/api/data/full_integrated_real_estate.db'
        self.daily_target = 50  # ì•ˆì „í•œ ì‹œì‘: 50ê°œ/ì¼
        self.delay_range = (10, 20)  # 10-20ì´ˆ ì§€ì—° (ì•ˆì „)
        
        self.stats = {
            'processed': 0,
            'success': 0,
            'errors': 0,
            'start_time': datetime.now()
        }

    def get_target_complexes(self, limit=50):
        """í¬ë¡¤ë§ ëŒ€ìƒ ë‹¨ì§€ URL ìƒì„±"""
        print(f"ğŸ“‹ í¬ë¡¤ë§ ëŒ€ìƒ {limit}ê°œ ë‹¨ì§€ ì¡°íšŒ ì¤‘...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ë§¤ë¬¼ ì—†ëŠ” ê³ ìš°ì„ ìˆœìœ„ ë‹¨ì§€ ì¡°íšŒ (ë„¤ì´ë²„ ë‹¨ì§€ ID í¬í•¨)
        query = """
        SELECT id, apartment_name, sigungu, eup_myeon_dong, total_transactions
        FROM apartment_complexes ac
        WHERE NOT EXISTS (
            SELECT 1 FROM current_listings cl 
            WHERE cl.complex_id = ac.id AND cl.source_type = 'naver'
        )
        AND ac.total_transactions > 5  -- ìµœì†Œ ê±°ë˜ëŸ‰ í•„í„°
        ORDER BY ac.crawling_priority ASC, ac.total_transactions DESC
        LIMIT ?
        """
        
        cursor.execute(query, (limit,))
        complexes = cursor.fetchall()
        conn.close()
        
        # ë„¤ì´ë²„ ë‹¨ì§€ URL ìƒì„± (ì¶”ì •)
        target_urls = []
        for complex_data in complexes:
            complex_id, name, sigungu, dong, transactions = complex_data
            
            # ë„¤ì´ë²„ ë‹¨ì§€ IDëŠ” ìˆœì°¨ì ì´ë¯€ë¡œ ì¶”ì • ë²”ìœ„ ìƒì„±
            estimated_ids = [
                complex_id,  # ë™ì¼ ID
                complex_id + 1000,  # ì˜¤í”„ì…‹ ì¶”ê°€
                complex_id + 2000,
                complex_id + 3000,
                complex_id + 5000,
                random.randint(1000, 15000)  # ëœë¤ ID
            ]
            
            for est_id in estimated_ids[:2]:  # ê° ë‹¨ì§€ë‹¹ 2ê°œ URL ì‹œë„
                naver_url = f"https://new.land.naver.com/complexes/{est_id}"
                target_urls.append({
                    'url': naver_url,
                    'complex_id': complex_id,
                    'name': name,
                    'location': f"{sigungu} {dong}"
                })
        
        print(f"âœ… {len(target_urls)}ê°œ ëŒ€ìƒ URL ìƒì„± ì™„ë£Œ")
        return target_urls[:limit]  # ì œí•œ

    async def run_mass_crawling(self):
        """ëŒ€ëŸ‰ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("ğŸš€ Enhanced Crawler ëŒ€ëŸ‰ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 60)
        print(f"ğŸ¯ ëª©í‘œ: {self.daily_target}ê°œ ë‹¨ì§€")
        print(f"â±ï¸  ì§€ì—°: {self.delay_range[0]}-{self.delay_range[1]}ì´ˆ")
        print(f"ğŸŒ VPN: Cloudflare WARP ì—°ê²°ë¨")
        print("=" * 60)
        
        try:
            # ëŒ€ìƒ URL ì¡°íšŒ
            target_urls = self.get_target_complexes(self.daily_target)
            
            if not target_urls:
                print("âŒ í¬ë¡¤ë§ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤")
                return
            
            print(f"ğŸ” {len(target_urls)}ê°œ URL í¬ë¡¤ë§ ì‹œì‘...\n")
            
            # ìˆœì°¨ í¬ë¡¤ë§ (ì•ˆì „ì„± ìš°ì„ )
            for i, target in enumerate(target_urls):
                print(f"[{i+1}/{len(target_urls)}] {target['name']} ({target['location']})")
                print(f"ğŸ† URL: {target['url']}")
                
                try:
                    # Enhanced Crawler í˜¸ì¶œ
                    result = await crawl_enhanced_single(target['url'])
                    
                    if result:
                        print(f"âœ… ì„±ê³µ: {target['name']}")
                        self.stats['success'] += 1
                    else:
                        print(f"âŒ ì‹¤íŒ¨: {target['name']}")
                        self.stats['errors'] += 1
                        
                except Exception as e:
                    print(f"âŒ ì˜¤ë¥˜: {target['name']} - {e}")
                    self.stats['errors'] += 1
                
                self.stats['processed'] += 1
                
                # ëœë¤ ì§€ì—° (ì¸ê°„ì  íŒ¨í„´)
                if i < len(target_urls) - 1:  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´
                    delay = random.randint(*self.delay_range)
                    print(f"â±ï¸  {delay}ì´ˆ ëŒ€ê¸°...\n")
                    await asyncio.sleep(delay)
            
            # ê²°ê³¼ ì¶œë ¥
            self.print_results()
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤í–‰ ì˜¤ë¥˜: {e}")

    def print_results(self):
        """ê²°ê³¼ ì¶œë ¥"""
        elapsed = datetime.now() - self.stats['start_time']
        success_rate = (self.stats['success'] / self.stats['processed'] * 100) if self.stats['processed'] > 0 else 0
        
        print("\n" + "=" * 60)
        print("ğŸ‰ Enhanced Crawler ëŒ€ëŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ê²°ê³¼:")
        print(f"   â€¢ ì²˜ë¦¬ëœ URL: {self.stats['processed']}ê°œ")
        print(f"   â€¢ ì„±ê³µ: {self.stats['success']}ê°œ")
        print(f"   â€¢ ì‹¤íŒ¨: {self.stats['errors']}ê°œ")
        print(f"   â€¢ ì„±ê³µë¥ : {success_rate:.1f}%")
        print(f"   â€¢ ì†Œìš”ì‹œê°„: {elapsed}")
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: data/output/")
        print(f"ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤: data/naver_real_estate.db")
        print("=" * 60)

if __name__ == "__main__":
    crawler = MassProductionCrawler()
    asyncio.run(crawler.run_mass_crawling())