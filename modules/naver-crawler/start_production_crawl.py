#!/usr/bin/env python3

"""
ì‹¤ì œ í”„ë¡œë•ì…˜ í¬ë¡¤ë§ ì‹œì‘
VPN ë©€í‹°ë ˆì´ì–´ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ í¬ë¡¤ë§ ì‹¤í–‰
"""

import asyncio
import sys
import json
import sqlite3
from pathlib import Path
from playwright.async_api import async_playwright
from datetime import datetime
import random

class ProductionCrawler:
    def __init__(self):
        self.db_path = '/Users/seongjunkim/projects/real-estate-platform/api/data/full_integrated_real_estate.db'
        self.output_dir = Path('/Users/seongjunkim/projects/real-estate-platform/modules/naver-crawler/data/output')
        self.daily_target = 20  # ì•ˆì „í•œ ì‹œì‘: í•˜ë£¨ 20ê°œ
        self.delay_range = (3, 7)  # 3-7ì´ˆ ëœë¤ ì§€ì—°
        
        self.stats = {
            'processed': 0,
            'success': 0,
            'errors': 0,
            'start_time': datetime.now()
        }

    async def get_target_complexes(self, limit=20):
        """í¬ë¡¤ë§ ëŒ€ìƒ ë‹¨ì§€ ì¡°íšŒ"""
        print(f"ğŸ“‹ í¬ë¡¤ë§ ëŒ€ìƒ {limit}ê°œ ë‹¨ì§€ ì¡°íšŒ ì¤‘...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ë§¤ë¬¼ ì—†ëŠ” ê³ ìš°ì„ ìˆœìœ„ ë‹¨ì§€ ì¡°íšŒ
        query = """
        SELECT id, apartment_name, sigungu, eup_myeon_dong, total_transactions
        FROM apartment_complexes ac
        WHERE NOT EXISTS (
            SELECT 1 FROM current_listings cl 
            WHERE cl.complex_id = ac.id
        )
        ORDER BY crawling_priority ASC, total_transactions DESC
        LIMIT ?
        """
        
        cursor.execute(query, (limit,))
        complexes = cursor.fetchall()
        conn.close()
        
        print(f"âœ… {len(complexes)}ê°œ ëŒ€ìƒ ë‹¨ì§€ ì„ ì • ì™„ë£Œ")
        return complexes

    async def crawl_single_complex(self, browser, complex_data):
        """ë‹¨ì¼ ë‹¨ì§€ í¬ë¡¤ë§"""
        complex_id, name, sigungu, dong, transactions = complex_data
        
        try:
            print(f"ğŸ¢ í¬ë¡¤ë§ ì‹œì‘: {name} ({sigungu} {dong})")
            
            page = await browser.new_page()
            
            # ëœë¤ User-Agent ì„¤ì •
            user_agents = [
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
            ]
            
            await page.set_extra_http_headers({
                'User-Agent': random.choice(user_agents)
            })
            
            # ë„¤ì´ë²„ ë¶€ë™ì‚° ê²€ìƒ‰
            search_url = f"https://new.land.naver.com/search?keyword={name}+{sigungu}"
            await page.goto(search_url, wait_until='domcontentloaded')
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            await asyncio.sleep(2)
            
            # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
            try:
                # ë‹¨ì§€ ë§í¬ ì°¾ê¸°
                complex_links = await page.query_selector_all('a[href*="/complexes/"]')
                
                if complex_links:
                    # ì²« ë²ˆì§¸ ë§¤ì¹­ ë‹¨ì§€ ì„ íƒ
                    first_link = complex_links[0]
                    href = await first_link.get_attribute('href')
                    
                    if href:
                        complex_url = f"https://new.land.naver.com{href}"
                        print(f"ğŸ”— ë‹¨ì§€ í˜ì´ì§€ ë°œê²¬: {complex_url}")
                        
                        # ë‹¨ì§€ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
                        await page.goto(complex_url, wait_until='domcontentloaded')
                        await asyncio.sleep(3)
                        
                        # ê¸°ë³¸ ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ
                        complex_info = await self.extract_complex_info(page, complex_id, name)
                        
                        if complex_info:
                            # íŒŒì¼ ì €ì¥
                            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                            filename = f"complex_{complex_id}_{timestamp}.json"
                            filepath = self.output_dir / filename
                            
                            with open(filepath, 'w', encoding='utf-8') as f:
                                json.dump(complex_info, f, ensure_ascii=False, indent=2)
                            
                            print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}")
                            self.stats['success'] += 1
                        else:
                            print("âš ï¸  ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                            self.stats['errors'] += 1
                    else:
                        print("âš ï¸  ë‹¨ì§€ ë§í¬ ì—†ìŒ")
                        self.stats['errors'] += 1
                else:
                    print("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    self.stats['errors'] += 1
                    
            except Exception as e:
                print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                self.stats['errors'] += 1
            
            await page.close()
            self.stats['processed'] += 1
            
            # ëœë¤ ì§€ì—°
            delay = random.randint(*self.delay_range)
            print(f"â±ï¸  {delay}ì´ˆ ëŒ€ê¸°...")
            await asyncio.sleep(delay)
            
        except Exception as e:
            print(f"âŒ ì „ì²´ ì˜¤ë¥˜: {e}")
            self.stats['errors'] += 1

    async def extract_complex_info(self, page, complex_id, name):
        """ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ê¸°ë³¸ ì •ë³´
            complex_info = {
                'complex_id': complex_id,
                'name': name,
                'crawled_at': datetime.now().isoformat(),
                'source': 'naver_production',
                'basic_info': {},
                'listings': []
            }
            
            # ë‹¨ì§€ëª… ì¬í™•ì¸
            try:
                title_element = await page.query_selector('h1, .complex_title, [class*="title"]')
                if title_element:
                    title = await title_element.text_content()
                    complex_info['basic_info']['confirmed_name'] = title.strip()
            except:
                pass
            
            # ì£¼ì†Œ ì •ë³´
            try:
                address_element = await page.query_selector('[class*="address"], [class*="location"]')
                if address_element:
                    address = await address_element.text_content()
                    complex_info['basic_info']['address'] = address.strip()
            except:
                pass
            
            # ë§¤ë¬¼ ì •ë³´ (ê°„ë‹¨í•œ ì¶”ì¶œ)
            try:
                listing_elements = await page.query_selector_all('[class*="item"], [class*="listing"]')
                
                for i, listing in enumerate(listing_elements[:5]):  # ìµœëŒ€ 5ê°œë§Œ
                    try:
                        listing_text = await listing.text_content()
                        if listing_text and ('ì–µ' in listing_text or 'ë§Œ' in listing_text):
                            complex_info['listings'].append({
                                'index': i,
                                'raw_text': listing_text.strip()
                            })
                    except:
                        continue
                        
            except:
                pass
            
            return complex_info if complex_info['basic_info'] else None
            
        except Exception as e:
            print(f"ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    async def run_production_crawl(self):
        """í”„ë¡œë•ì…˜ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("ğŸš€ í”„ë¡œë•ì…˜ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 60)
        
        try:
            # ëŒ€ìƒ ë‹¨ì§€ ì¡°íšŒ
            complexes = await self.get_target_complexes(self.daily_target)
            
            if not complexes:
                print("âŒ í¬ë¡¤ë§ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ë¸Œë¼ìš°ì € ì‹œì‘
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-dev-shm-usage']
                )
                
                print(f"ğŸ” {len(complexes)}ê°œ ë‹¨ì§€ í¬ë¡¤ë§ ì‹œì‘...")
                
                # ê° ë‹¨ì§€ í¬ë¡¤ë§
                for i, complex_data in enumerate(complexes):
                    print(f"\n[{i+1}/{len(complexes)}] ", end="")
                    await self.crawl_single_complex(browser, complex_data)
                
                await browser.close()
            
            # ê²°ê³¼ ì¶œë ¥
            self.print_results()
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤í–‰ ì˜¤ë¥˜: {e}")

    def print_results(self):
        """ê²°ê³¼ ì¶œë ¥"""
        elapsed = datetime.now() - self.stats['start_time']
        success_rate = (self.stats['success'] / self.stats['processed'] * 100) if self.stats['processed'] > 0 else 0
        
        print("\n" + "=" * 60)
        print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ê²°ê³¼:")
        print(f"   â€¢ ì²˜ë¦¬ëœ ë‹¨ì§€: {self.stats['processed']}ê°œ")
        print(f"   â€¢ ì„±ê³µ: {self.stats['success']}ê°œ")
        print(f"   â€¢ ì‹¤íŒ¨: {self.stats['errors']}ê°œ")
        print(f"   â€¢ ì„±ê³µë¥ : {success_rate:.1f}%")
        print(f"   â€¢ ì†Œìš”ì‹œê°„: {elapsed}")
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")

if __name__ == "__main__":
    crawler = ProductionCrawler()
    asyncio.run(crawler.run_production_crawl())