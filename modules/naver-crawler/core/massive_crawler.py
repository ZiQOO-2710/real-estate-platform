"""
ì „êµ­ ëª¨ë“  ì•„íŒŒíŠ¸ ë‹¨ì§€ ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
ë°œê²¬ëœ ëª¨ë“  ë‹¨ì§€ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í¬ë¡¤ë§
"""

import asyncio
import time
import logging
from datetime import datetime
from typing import List, Dict
import sqlite3
from pathlib import Path
import random

from .complex_discovery import ComplexDiscovery
from .enhanced_naver_crawler import crawl_enhanced_single
from database.simple_data_processor import process_json_file

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MassiveCrawler:
    """ì „êµ­ ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_concurrent=3, batch_size=10):
        self.max_concurrent = max_concurrent
        self.batch_size = batch_size
        self.discovery = ComplexDiscovery()
        self.progress_db = 'data/massive_crawling_progress.db'
        self.init_progress_db()
        
    def init_progress_db(self):
        """í¬ë¡¤ë§ ì§„í–‰ë¥  ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        Path(self.progress_db).parent.mkdir(parents=True, exist_ok=True)
        
        conn = sqlite3.connect(self.progress_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS massive_crawling_progress (
                complex_id TEXT PRIMARY KEY,
                complex_name TEXT,
                region_name TEXT,
                url TEXT,
                status TEXT DEFAULT 'pending',
                listings_count INTEGER DEFAULT 0,
                error_message TEXT,
                crawled_at TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info(f"ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì§„í–‰ë¥  DB ì´ˆê¸°í™”: {self.progress_db}")
        
    def load_all_complexes(self) -> List[Dict]:
        """ë°œê²¬ëœ ëª¨ë“  ë‹¨ì§€ ëª©ë¡ ë¡œë“œ"""
        # ë¨¼ì € ë°œê²¬ëœ ë‹¨ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
        stats = self.discovery.get_discovery_stats()
        
        if stats['total_complexes'] == 0:
            logger.warning("ë°œê²¬ëœ ë‹¨ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë‹¨ì§€ ë°œê²¬ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return []
            
        return self.discovery.get_all_discovered_complexes()
        
    def get_pending_complexes(self, all_complexes: List[Dict]) -> List[Dict]:
        """ë¯¸ì²˜ë¦¬ ë‹¨ì§€ ëª©ë¡ ë°˜í™˜"""
        conn = sqlite3.connect(self.progress_db)
        cursor = conn.cursor()
        
        # ì´ë¯¸ ì™„ë£Œëœ ë‹¨ì§€ ID ì¡°íšŒ
        cursor.execute("SELECT complex_id FROM massive_crawling_progress WHERE status = 'completed'")
        completed_ids = {row[0] for row in cursor.fetchall()}
        
        conn.close()
        
        # ë¯¸ì²˜ë¦¬ ë‹¨ì§€ í•„í„°ë§
        pending = [complex_info for complex_info in all_complexes 
                  if complex_info['complex_id'] not in completed_ids]
        
        return pending
        
    def update_progress(self, complex_id: str, status: str, listings_count: int = 0, error_message: str = None):
        """í¬ë¡¤ë§ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        conn = sqlite3.connect(self.progress_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO massive_crawling_progress
            (complex_id, status, listings_count, error_message, crawled_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            complex_id,
            status,
            listings_count,
            error_message,
            datetime.now().isoformat() if status == 'completed' else None,
            datetime.now().isoformat()
        ))
        
        conn.commit()
        conn.close()
        
    async def crawl_single_complex(self, complex_info: Dict) -> bool:
        """ë‹¨ì¼ ë‹¨ì§€ í¬ë¡¤ë§"""
        complex_id = complex_info['complex_id']
        complex_name = complex_info['complex_name']
        url = complex_info['url']
        
        try:
            logger.info(f"ğŸ¢ í¬ë¡¤ë§ ì‹œì‘: {complex_name} ({complex_id})")
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            result = await crawl_enhanced_single(url, complex_name, headless=True)
            
            if result['success']:
                # DB ì €ì¥
                json_file = result['files']['json_file']
                db_success = process_json_file(json_file, {'database': 'data/naver_real_estate.db'})
                
                if db_success:
                    listings_count = result['data_summary']['listings_count']
                    self.update_progress(complex_id, 'completed', listings_count)
                    logger.info(f"âœ… ì„±ê³µ: {complex_name} - ë§¤ë¬¼ {listings_count}ê°œ")
                    return True
                else:
                    self.update_progress(complex_id, 'failed', error_message="DB ì €ì¥ ì‹¤íŒ¨")
                    logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {complex_name}")
                    return False
            else:
                error_msg = result.get('error', 'Unknown error')
                self.update_progress(complex_id, 'failed', error_message=error_msg)
                logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {complex_name} - {error_msg}")
                return False
                
        except Exception as e:
            self.update_progress(complex_id, 'failed', error_message=str(e))
            logger.error(f"âŒ ì˜ˆì™¸ ë°œìƒ: {complex_name} - {e}")
            return False
            
    async def crawl_batch(self, batch: List[Dict]):
        """ë°°ì¹˜ ë‹¨ìœ„ ë³‘ë ¬ í¬ë¡¤ë§"""
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def crawl_with_semaphore(complex_info):
            async with semaphore:
                success = await self.crawl_single_complex(complex_info)
                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                await asyncio.sleep(random.uniform(5, 10))
                return success
                
        tasks = [crawl_with_semaphore(complex_info) for complex_info in batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        success_count = sum(1 for r in results if r is True)
        return success_count, len(results)
        
    async def run_massive_crawling(self):
        """ì „êµ­ ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("ğŸš€ ì „êµ­ ëŒ€ê·œëª¨ ì•„íŒŒíŠ¸ í¬ë¡¤ë§ ì‹œì‘")
        
        # 1. ëª¨ë“  ë‹¨ì§€ ëª©ë¡ ë¡œë“œ
        all_complexes = self.load_all_complexes()
        
        if not all_complexes:
            logger.error("í¬ë¡¤ë§í•  ë‹¨ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë‹¨ì§€ ë°œê²¬ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
            
        # 2. ë¯¸ì²˜ë¦¬ ë‹¨ì§€ í•„í„°ë§
        pending_complexes = self.get_pending_complexes(all_complexes)
        
        logger.info(f"ğŸ“Š í¬ë¡¤ë§ ëŒ€ìƒ: {len(pending_complexes)}ê°œ ë‹¨ì§€")
        logger.info(f"ğŸ“Š ì´ë¯¸ ì™„ë£Œ: {len(all_complexes) - len(pending_complexes)}ê°œ ë‹¨ì§€")
        
        if not pending_complexes:
            logger.info("ğŸ‰ ëª¨ë“  ë‹¨ì§€ í¬ë¡¤ë§ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return
            
        # 3. ë°°ì¹˜ ë‹¨ìœ„ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
        total_batches = (len(pending_complexes) + self.batch_size - 1) // self.batch_size
        start_time = time.time()
        total_success = 0
        total_failed = 0
        
        for i in range(0, len(pending_complexes), self.batch_size):
            batch_num = i // self.batch_size + 1
            batch = pending_complexes[i:i + self.batch_size]
            
            logger.info(f"ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ ë‹¨ì§€)")
            
            try:
                success_count, total_count = await self.crawl_batch(batch)
                failed_count = total_count - success_count
                
                total_success += success_count
                total_failed += failed_count
                
                # ì§„í–‰ë¥  ë¦¬í¬íŠ¸
                elapsed_time = time.time() - start_time
                processed_count = total_success + total_failed
                remaining_count = len(pending_complexes) - processed_count
                
                if processed_count > 0:
                    avg_time_per_complex = elapsed_time / processed_count
                    estimated_remaining_time = avg_time_per_complex * remaining_count
                    
                    logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {processed_count}/{len(pending_complexes)} ì™„ë£Œ")
                    logger.info(f"ğŸ“Š ì„±ê³µ: {total_success}, ì‹¤íŒ¨: {total_failed}")
                    logger.info(f"â° ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining_time/3600:.1f}ì‹œê°„")
                    
                # ë°°ì¹˜ ê°„ ëŒ€ê¸°
                if i + self.batch_size < len(pending_complexes):
                    wait_time = random.uniform(15, 30)
                    logger.info(f"â³ ë‹¤ìŒ ë°°ì¹˜ê¹Œì§€ {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
                    await asyncio.sleep(wait_time)
                    
            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                total_failed += len(batch)
                
        # 4. ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸
        total_time = time.time() - start_time
        success_rate = (total_success / (total_success + total_failed)) * 100 if (total_success + total_failed) > 0 else 0
        
        logger.info(f"\nğŸ‰ ì „êµ­ ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"â° ì´ ì†Œìš”ì‹œê°„: {total_time/3600:.2f}ì‹œê°„")
        logger.info(f"ğŸ“Š ì´ ë‹¨ì§€ìˆ˜: {len(pending_complexes)}ê°œ")
        logger.info(f"âœ… ì„±ê³µ: {total_success}ê°œ")
        logger.info(f"âŒ ì‹¤íŒ¨: {total_failed}ê°œ")
        logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        return {
            'total_complexes': len(pending_complexes),
            'successful': total_success,
            'failed': total_failed,
            'total_time': total_time,
            'success_rate': success_rate
        }
        
    def get_crawling_stats(self) -> Dict:
        """í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ"""
        conn = sqlite3.connect(self.progress_db)
        cursor = conn.cursor()
        
        # ì „ì²´ í†µê³„
        cursor.execute('''
            SELECT status, COUNT(*) as count
            FROM massive_crawling_progress
            GROUP BY status
        ''')
        status_stats = dict(cursor.fetchall())
        
        # ì§€ì—­ë³„ í†µê³„
        cursor.execute('''
            SELECT region_name, status, COUNT(*) as count
            FROM massive_crawling_progress p
            JOIN discovered_complexes d ON p.complex_id = d.complex_id
            GROUP BY region_name, status
            ORDER BY region_name
        ''', )
        region_stats = cursor.fetchall()
        
        # ì´ ë§¤ë¬¼ ìˆ˜
        cursor.execute('SELECT SUM(listings_count) FROM massive_crawling_progress WHERE status = "completed"')
        total_listings = cursor.fetchone()[0] or 0
        
        conn.close()
        
        return {
            'status_summary': status_stats,
            'region_stats': region_stats,
            'total_listings': total_listings
        }

# ì‹¤í–‰ í•¨ìˆ˜
async def run_complete_nationwide_crawling():
    """ì™„ì „í•œ ì „êµ­ í¬ë¡¤ë§ ì‹¤í–‰"""
    logger.info("ğŸ¯ ì™„ì „í•œ ì „êµ­ ì•„íŒŒíŠ¸ í¬ë¡¤ë§ ì‹œì‘")
    
    # 1. ë‹¨ì§€ ë°œê²¬
    discovery = ComplexDiscovery()
    stats = discovery.get_discovery_stats()
    
    if stats['total_complexes'] == 0:
        logger.info("ğŸ“ ë¨¼ì € ì „êµ­ ë‹¨ì§€ ë°œê²¬ ì‹¤í–‰...")
        await discovery.discover_all_complexes()
        
    # 2. ëŒ€ê·œëª¨ í¬ë¡¤ë§
    crawler = MassiveCrawler(max_concurrent=2, batch_size=5)
    result = await crawler.run_massive_crawling()
    
    # 3. ìµœì¢… í†µê³„
    crawling_stats = crawler.get_crawling_stats()
    
    print(f"\nğŸŠ ì „êµ­ í¬ë¡¤ë§ ìµœì¢… ê²°ê³¼:")
    print(f"  ğŸ“Š ì´ ë§¤ë¬¼: {crawling_stats['total_listings']:,}ê°œ")
    print(f"  ğŸ¢ ì™„ë£Œ ë‹¨ì§€: {crawling_stats['status_summary'].get('completed', 0)}ê°œ")
    print(f"  âŒ ì‹¤íŒ¨ ë‹¨ì§€: {crawling_stats['status_summary'].get('failed', 0)}ê°œ")
    
    return result

if __name__ == "__main__":
    asyncio.run(run_complete_nationwide_crawling())