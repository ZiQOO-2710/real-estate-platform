#!/usr/bin/env python3
"""
JSON í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ë„¤ì´ë²„ ì „ìš© ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ì‹œìŠ¤í…œ
í¬ë¡¤ë§ëœ JSON íŒŒì¼ë“¤ì„ ë¶„ì„í•´ì„œ ì •ê·œí™”ëœ DB ìŠ¤í‚¤ë§ˆë¡œ ì €ì¥
"""

import sqlite3
import json
import os
import re
from datetime import datetime
from pathlib import Path
import logging

class JsonToDbConverter:
    def __init__(self):
        self.data_dir = Path(__file__).parent.parent / "data"
        self.output_dir = self.data_dir / "output"
        self.db_path = self.data_dir / "naver_crawled_data.db"
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        self.stats = {
            'json_files_processed': 0,
            'complexes_created': 0,
            'listings_created': 0,
            'errors': 0
        }

    def initialize_database(self):
        """ë„¤ì´ë²„ í¬ë¡¤ë§ ë°ì´í„° ì „ìš© DB ìŠ¤í‚¤ë§ˆ ìƒì„±"""
        self.logger.info("ğŸ”§ ë„¤ì´ë²„ í¬ë¡¤ë§ DB ì´ˆê¸°í™” ì¤‘...")
        
        # ê¸°ì¡´ DB ì‚­ì œ (ìƒˆë¡œ ì‹œì‘)
        if self.db_path.exists():
            self.db_path.unlink()
        
        self.conn = sqlite3.connect(str(self.db_path))
        self.cursor = self.conn.cursor()
        
        # ì•„íŒŒíŠ¸ ë‹¨ì§€ í…Œì´ë¸”
        self.cursor.execute("""
            CREATE TABLE apartment_complexes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                complex_id TEXT UNIQUE NOT NULL,
                complex_name TEXT,
                address TEXT,
                latitude REAL,
                longitude REAL,
                completion_year INTEGER,
                total_households INTEGER,
                total_buildings INTEGER,
                area_range TEXT,
                source_url TEXT,
                crawled_at TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # ë§¤ë¬¼ ì •ë³´ í…Œì´ë¸”
        self.cursor.execute("""
            CREATE TABLE current_listings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                complex_id TEXT NOT NULL,
                listing_index INTEGER,
                deal_type TEXT,
                price_text TEXT,
                price_amount INTEGER,  -- ë§Œì› ë‹¨ìœ„
                area_info TEXT,
                floor_info TEXT,
                direction TEXT,
                description TEXT,
                original_text TEXT,
                extracted_at TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (complex_id) REFERENCES apartment_complexes(complex_id)
            )
        """)
        
        # í¬ë¡¤ë§ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
        self.cursor.execute("""
            CREATE TABLE crawling_metadata (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                complex_id TEXT NOT NULL,
                json_filename TEXT,
                crawler_version TEXT,
                crawl_method TEXT,
                crawled_at TIMESTAMP,
                processing_status TEXT DEFAULT 'processed',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # ì¸ë±ìŠ¤ ìƒì„±
        self.cursor.execute("CREATE INDEX idx_complex_id ON current_listings(complex_id)")
        self.cursor.execute("CREATE INDEX idx_deal_type ON current_listings(deal_type)")
        self.cursor.execute("CREATE INDEX idx_complex_name ON apartment_complexes(complex_name)")
        
        self.conn.commit()
        self.logger.info("âœ… DB ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ")

    def process_json_files(self, limit=None):
        """JSON íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•´ì„œ DBì— ì €ì¥"""
        self.logger.info("ğŸš€ JSON íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
        
        json_files = list(self.output_dir.glob("enhanced_complex_*.json"))
        
        if limit:
            json_files = json_files[:limit]
        
        self.logger.info(f"ğŸ“Š {len(json_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì˜ˆì •")
        
        for json_file in json_files:
            try:
                self.process_single_json(json_file)
                self.stats['json_files_processed'] += 1
                
                if self.stats['json_files_processed'] % 50 == 0:
                    self.logger.info(f"âœ… {self.stats['json_files_processed']}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
                    self.print_stats()
                    
            except Exception as e:
                self.logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({json_file.name}): {e}")
                self.stats['errors'] += 1
                continue
        
        self.conn.commit()
        self.print_final_stats()

    def process_single_json(self, json_file):
        """ë‹¨ì¼ JSON íŒŒì¼ ì²˜ë¦¬"""
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # 1. ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ
        complex_info = self.extract_complex_info(data)
        if not complex_info:
            return
        
        # 2. ë‹¨ì§€ ì •ë³´ ì €ì¥
        self.save_complex_info(complex_info)
        
        # 3. ë§¤ë¬¼ ì •ë³´ ì¶”ì¶œ ë° ì €ì¥
        listings = self.extract_listings(data, complex_info['complex_id'])
        for listing in listings:
            self.save_listing(listing)
        
        # 4. ë©”íƒ€ë°ì´í„° ì €ì¥
        self.save_metadata(data, json_file.name)

    def extract_complex_info(self, data):
        """JSONì—ì„œ ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ"""
        basic_info = data.get('basic_info', {})
        crawler_info = data.get('crawler_info', {})
        listings = data.get('current_listings', [])
        
        complex_id = basic_info.get('complexId') or crawler_info.get('complex_id')
        if not complex_id:
            return None
        
        # ë§¤ë¬¼ì—ì„œ ë‹¨ì§€ëª… ì¶”ì¶œ
        complex_name = self.extract_complex_name_from_listings(listings)
        
        # ì¢Œí‘œ ì¶”ì¶œ (URLì—ì„œ)
        lat, lng = self.extract_coordinates(basic_info.get('url', ''))
        
        return {
            'complex_id': str(complex_id),
            'complex_name': complex_name,
            'address': self.extract_address_from_url(basic_info.get('url', '')),
            'latitude': lat,
            'longitude': lng,
            'source_url': basic_info.get('source_url') or basic_info.get('url'),
            'crawled_at': crawler_info.get('crawled_at') or basic_info.get('extracted_at'),
            'listing_count': len(listings)
        }

    def extract_complex_name_from_listings(self, listings):
        """ë§¤ë¬¼ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì§€ëª… ì¶”ì¶œ"""
        if not listings:
            return 'ì •ë³´ì—†ìŒ'
        
        for listing in listings[:3]:  # ì²˜ìŒ 3ê°œ ë§¤ë¬¼ì—ì„œ ì‹œë„
            text = listing.get('text', '')
            if text:
                # "ì •ë“ í•œì§„6ì°¨ 601ë™ë§¤ë§¤14ì–µ..." íŒ¨í„´ì—ì„œ ë‹¨ì§€ëª… ì¶”ì¶œ
                match = re.match(r'^([^\s]+(?:\s*\d+ì°¨)?)', text)
                if match:
                    name = match.group(1).strip()
                    # ì¼ë°˜ì ì´ì§€ ì•Šì€ ì´ë¦„ í•„í„°ë§
                    if len(name) > 2 and 'ë™ë§¤ë§¤' not in name and 'ë™ì „ì„¸' not in name:
                        return name
        
        return 'ì •ë³´ì—†ìŒ'

    def extract_coordinates(self, url):
        """URLì—ì„œ ì¢Œí‘œ ì¶”ì¶œ"""
        if not url:
            return None, None
        
        # ms=37.36286,127.115578,17 íŒ¨í„´ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ
        match = re.search(r'ms=([0-9.]+),([0-9.]+)', url)
        if match:
            return float(match.group(1)), float(match.group(2))
        
        return None, None

    def extract_address_from_url(self, url):
        """URLì—ì„œ ì£¼ì†Œ ì •ë³´ ì¶”ì¶œ"""
        if not url:
            return ''
        
        lat, lng = self.extract_coordinates(url)
        if lat and lng:
            return f"ì¶”ì •ì¢Œí‘œ: {lat}, {lng}"
        
        return ''

    def extract_listings(self, data, complex_id):
        """JSONì—ì„œ ë§¤ë¬¼ ì •ë³´ ì¶”ì¶œ"""
        listings = data.get('current_listings', [])
        result = []
        
        for listing in listings:
            deal_type = self.normalize_deal_type(listing.get('deal_type'))
            if not deal_type:
                continue
            
            price_amount = self.parse_price(listing.get('price'))
            
            result.append({
                'complex_id': complex_id,
                'listing_index': listing.get('index'),
                'deal_type': deal_type,
                'price_text': listing.get('price'),
                'price_amount': price_amount,
                'area_info': listing.get('area'),
                'floor_info': listing.get('floor'),
                'description': self.clean_text(listing.get('text', '')),
                'original_text': listing.get('text'),
                'extracted_at': listing.get('extracted_at')
            })
        
        return result

    def normalize_deal_type(self, deal_type):
        """ê±°ë˜ìœ í˜• ì •ê·œí™”"""
        if not deal_type:
            return None
        
        deal_type = deal_type.lower().strip()
        if 'ë§¤ë§¤' in deal_type or deal_type == 'sale':
            return 'ë§¤ë§¤'
        elif 'ì „ì„¸' in deal_type or deal_type == 'jeonse':
            return 'ì „ì„¸'
        elif 'ì›”ì„¸' in deal_type or deal_type == 'monthly':
            return 'ì›”ì„¸'
        
        return None

    def parse_price(self, price_str):
        """ê°€ê²© ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜ (ë§Œì› ë‹¨ìœ„)"""
        if not price_str:
            return None
        
        # "14ì–µ 5,000", "8ì–µ", "22ì–µ" ë“±ì˜ í˜•ì‹ íŒŒì‹±
        clean_price = re.sub(r'[,\s]', '', price_str)
        
        # ì–µì› ë‹¨ìœ„ ì¶”ì¶œ
        billion_match = re.search(r'(\d+(?:\.\d+)?)ì–µ', clean_price)
        if billion_match:
            amount = float(billion_match.group(1)) * 10000  # ë§Œì› ë‹¨ìœ„ë¡œ ë³€í™˜
            
            # ì²œë§Œì› ë‹¨ìœ„ ì¶”ê°€
            thousand_match = re.search(r'(\d+)ì²œ', clean_price)
            if thousand_match:
                amount += int(thousand_match.group(1)) * 1000
            
            return int(amount)
        
        # ë§Œì› ë‹¨ìœ„ë§Œ ìˆëŠ” ê²½ìš°
        million_match = re.search(r'(\d+)ë§Œ', clean_price)
        if million_match:
            return int(million_match.group(1))
        
        return None

    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ''
        
        # ë‹¨ì§€ëª… ë¶€ë¶„ ì œê±°í•˜ê³  ì„¤ëª…ë§Œ ì¶”ì¶œ
        parts = text.split(' ', 2)
        if len(parts) >= 3:
            return parts[2]  # ë‹¨ì§€ëª…ê³¼ ë™í˜¸ìˆ˜ ì œê±°
        
        return text

    def save_complex_info(self, complex_info):
        """ë‹¨ì§€ ì •ë³´ DB ì €ì¥"""
        try:
            self.cursor.execute("""
                INSERT OR REPLACE INTO apartment_complexes 
                (complex_id, complex_name, address, latitude, longitude, 
                 source_url, crawled_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                complex_info['complex_id'],
                complex_info['complex_name'],
                complex_info['address'],
                complex_info['latitude'],
                complex_info['longitude'],
                complex_info['source_url'],
                complex_info['crawled_at']
            ))
            
            self.stats['complexes_created'] += 1
            
        except sqlite3.IntegrityError:
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì—…ë°ì´íŠ¸
            self.cursor.execute("""
                UPDATE apartment_complexes 
                SET complex_name = ?, address = ?, latitude = ?, longitude = ?
                WHERE complex_id = ?
            """, (
                complex_info['complex_name'],
                complex_info['address'],
                complex_info['latitude'],
                complex_info['longitude'],
                complex_info['complex_id']
            ))

    def save_listing(self, listing):
        """ë§¤ë¬¼ ì •ë³´ DB ì €ì¥"""
        self.cursor.execute("""
            INSERT INTO current_listings 
            (complex_id, listing_index, deal_type, price_text, price_amount,
             area_info, floor_info, description, original_text, extracted_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            listing['complex_id'],
            listing['listing_index'],
            listing['deal_type'],
            listing['price_text'],
            listing['price_amount'],
            listing['area_info'],
            listing['floor_info'],
            listing['description'],
            listing['original_text'],
            listing['extracted_at']
        ))
        
        self.stats['listings_created'] += 1

    def save_metadata(self, data, filename):
        """í¬ë¡¤ë§ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        basic_info = data.get('basic_info', {})
        crawler_info = data.get('crawler_info', {})
        
        complex_id = basic_info.get('complexId') or crawler_info.get('complex_id')
        
        self.cursor.execute("""
            INSERT INTO crawling_metadata 
            (complex_id, json_filename, crawler_version, crawl_method, crawled_at)
            VALUES (?, ?, ?, ?, ?)
        """, (
            str(complex_id),
            filename,
            crawler_info.get('version'),
            crawler_info.get('crawl_method'),
            crawler_info.get('crawled_at')
        ))

    def print_stats(self):
        """ì¤‘ê°„ í†µê³„ ì¶œë ¥"""
        self.logger.info(f"   ğŸ“Š í˜„ì¬ê¹Œì§€: ë‹¨ì§€ {self.stats['complexes_created']}ê°œ, "
                        f"ë§¤ë¬¼ {self.stats['listings_created']}ê°œ")

    def print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ‰ JSON â†’ DB ë³€í™˜ ì™„ë£Œ!")
        print("="*60)
        print(f"ğŸ“Š ìµœì¢… ë³€í™˜ ê²°ê³¼:")
        print(f"   â€¢ JSON íŒŒì¼ ì²˜ë¦¬: {self.stats['json_files_processed']}ê°œ")
        print(f"   â€¢ ì•„íŒŒíŠ¸ ë‹¨ì§€ ìƒì„±: {self.stats['complexes_created']}ê°œ")
        print(f"   â€¢ ë§¤ë¬¼ ì •ë³´ ìƒì„±: {self.stats['listings_created']}ê°œ")
        print(f"   â€¢ ì˜¤ë¥˜ ë°œìƒ: {self.stats['errors']}ê°œ")
        
        # DB ìµœì¢… í™•ì¸
        complex_count = self.cursor.execute("SELECT COUNT(*) FROM apartment_complexes").fetchone()[0]
        listing_count = self.cursor.execute("SELECT COUNT(*) FROM current_listings").fetchone()[0]
        
        print(f"\nâœ… DB ìµœì¢… ìƒíƒœ:")
        print(f"   â€¢ ì´ ë‹¨ì§€ ìˆ˜: {complex_count}ê°œ")
        print(f"   â€¢ ì´ ë§¤ë¬¼ ìˆ˜: {listing_count}ê°œ")
        print(f"   â€¢ DB íŒŒì¼: {self.db_path}")
        print("="*60)

    def run(self, limit=100):
        """ì „ì²´ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            self.initialize_database()
            self.process_json_files(limit=limit)
            
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
            self.logger.info("\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„° í™•ì¸:")
            self.cursor.execute("""
                SELECT complex_name, COUNT(*) as listing_count 
                FROM apartment_complexes c 
                LEFT JOIN current_listings l ON c.complex_id = l.complex_id 
                WHERE complex_name != 'ì •ë³´ì—†ìŒ'
                GROUP BY c.complex_id 
                ORDER BY listing_count DESC 
                LIMIT 5
            """)
            
            for row in self.cursor.fetchall():
                self.logger.info(f"   â€¢ {row[0]}: {row[1]}ê°œ ë§¤ë¬¼")
            
        except Exception as e:
            self.logger.error(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        finally:
            if hasattr(self, 'conn'):
                self.conn.close()

if __name__ == "__main__":
    converter = JsonToDbConverter()
    converter.run(limit=100)  # ì²˜ìŒ 100ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬