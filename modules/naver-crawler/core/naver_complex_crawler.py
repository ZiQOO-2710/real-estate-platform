"""
ÎÑ§Ïù¥Î≤Ñ Î∂ÄÎèôÏÇ∞ Îã®ÏßÄÎ≥Ñ Î™®ÎìàÌôî ÌÅ¨Î°§Îü¨
Playwright MCPÎ•º ÌôúÏö©Ìïú Î≤îÏö© Îã®ÏßÄ ÌÅ¨Î°§ÎßÅ Î™®Îìà

ÏÇ¨Ïö©Î≤ï:
    crawler = NaverComplexCrawler()
    result = await crawler.crawl_complex(complex_url)
"""

import asyncio
import json
import csv
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright
import re

class NaverComplexCrawler:
    """ÎÑ§Ïù¥Î≤Ñ Î∂ÄÎèôÏÇ∞ Îã®ÏßÄ ÌÅ¨Î°§Îü¨ Î™®Îìà"""
    
    def __init__(self, headless=False, screenshot=True):
        """
        Args:
            headless (bool): Ìó§ÎìúÎ¶¨Ïä§ Î™®Îìú Ïó¨Î∂Ä
            screenshot (bool): Ïä§ÌÅ¨Î¶∞ÏÉ∑ Ï†ÄÏû• Ïó¨Î∂Ä
        """
        self.browser = None
        self.page = None
        self.headless = headless
        self.screenshot = screenshot
        
    async def init_browser(self):
        """Î∏åÎùºÏö∞Ï†Ä Ï¥àÍ∏∞Ìôî (F12 Ï∞®Îã® Ïö∞Ìöå)"""
        playwright = await async_playwright().start()
        
        # ÏïàÌã∞ ÎîîÌÖçÏÖò Î∏åÎùºÏö∞Ï†Ä ÏÑ§Ï†ï
        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-features=VizDisplayCompositor',
                '--disable-ipc-flooding-protection',
                '--disable-renderer-backgrounding',
                '--disable-backgrounding-occluded-windows',
                '--disable-client-side-phishing-detection',
                '--disable-component-extensions-with-background-pages',
                '--disable-default-apps',
                '--disable-extensions',
                '--disable-features=TranslateUI',
                '--disable-hang-monitor',
                '--disable-web-security',
                '--no-first-run',
                '--no-default-browser-check',
                '--no-sandbox',
                '--disable-setuid-sandbox'
            ]
        )
        
        context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        
        self.page = await context.new_page()
        
        # Í∞úÎ∞úÏûêÎèÑÍµ¨ ÌÉêÏßÄ Ïö∞Ìöå
        await self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
            
            delete window.console.debug;
            delete window.console.clear;
            
            Object.defineProperty(navigator, 'languages', {
                get: () => ['ko-KR', 'ko', 'en-US', 'en'],
            });
            
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
        """)
        
    async def extract_complex_basic_info(self, url):
        """Îã®ÏßÄ Í∏∞Î≥∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        print(f"üîç Îã®ÏßÄ Í∏∞Î≥∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú: {url}")
        
        try:
            await self.page.goto(url, wait_until="networkidle", timeout=30000)
            await asyncio.sleep(5)
            
            # Îã®ÏßÄÎ™Ö Ï∂îÏ∂ú (URLÏóêÏÑúÎèÑ Ï∂îÏ∂ú)
            complex_id = self.extract_complex_id_from_url(url)
            
            basic_info = await self.page.evaluate("""
                () => {
                    const info = {};
                    
                    // ÌéòÏù¥ÏßÄ Ï†úÎ™©ÏóêÏÑú Îã®ÏßÄÎ™Ö Ï∂îÏ∂ú
                    const title = document.title;
                    if (title && !title.includes('ÎÑ§Ïù¥Î≤ÑÌéòÏù¥')) {
                        info.complexName = title.split('|')[0].trim();
                    }
                    
                    // Îã§ÏñëÌïú ÏÖÄÎ†âÌÑ∞Î°ú Îã®ÏßÄÎ™Ö Ï∞æÍ∏∞
                    const nameSelectors = [
                        'h1.complex_title',
                        '.complex_name', 
                        '.title',
                        'h1',
                        '[class*="title"]',
                        '[class*="name"]'
                    ];
                    
                    for (const selector of nameSelectors) {
                        const element = document.querySelector(selector);
                        if (element && element.textContent.trim() && 
                            !element.textContent.includes('Í±∞ÎûòÎ∞©Ïãù') &&
                            !element.textContent.includes('@naver')) {
                            info.complexName = element.textContent.trim();
                            break;
                        }
                    }
                    
                    // Ï£ºÏÜå Ï†ïÎ≥¥
                    const addressSelectors = [
                        '.complex_address',
                        '.address',
                        '[class*="address"]',
                        '[class*="location"]'
                    ];
                    
                    for (const selector of addressSelectors) {
                        const element = document.querySelector(selector);
                        if (element && element.textContent.trim() && 
                            !element.textContent.includes('@naver')) {
                            info.address = element.textContent.trim();
                            break;
                        }
                    }
                    
                    // Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ÏóêÏÑú Ï†ïÎ≥¥ Ï∂îÏ∂ú
                    const allText = document.body.textContent;
                    
                    // Ï§ÄÍ≥µÎÖÑÎèÑ
                    const yearPatterns = [
                        /(19|20)\\d{2}ÎÖÑ?\\s*Ï§Ä?Í≥µ?/,
                        /(19|20)\\d{2}[\\./]\\d{1,2}/,
                        /Ï§ÄÍ≥µ\\s*(19|20)\\d{2}/
                    ];
                    
                    for (const pattern of yearPatterns) {
                        const match = allText.match(pattern);
                        if (match) {
                            info.completionYear = match[0];
                            break;
                        }
                    }
                    
                    // ÏÑ∏ÎåÄÏàò
                    const householdMatches = allText.match(/(\\d+)\\s*ÏÑ∏ÎåÄ/);
                    if (householdMatches) {
                        info.totalHouseholds = householdMatches[1];
                    }
                    
                    // Î©¥Ï†Å Ï†ïÎ≥¥
                    const areaMatches = allText.match(/(\\d+\\.?\\d*)„é°/g);
                    if (areaMatches && areaMatches.length > 0) {
                        info.areas = [...new Set(areaMatches)].slice(0, 10);
                    }
                    
                    return info;
                }
            """)
            
            # URLÏóêÏÑú Ï∂îÏ∂úÌïú ID Ï∂îÍ∞Ä
            basic_info['complex_id'] = complex_id
            basic_info['source_url'] = url
            
            return basic_info
            
        except Exception as e:
            print(f"‚ùå Í∏∞Î≥∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú Ïò§Î•ò: {e}")
            return {'complex_id': self.extract_complex_id_from_url(url), 'source_url': url}
            
    def extract_complex_id_from_url(self, url):
        """URLÏóêÏÑú Îã®ÏßÄ ID Ï∂îÏ∂ú"""
        match = re.search(r'/complexes/(\d+)', url)
        return match.group(1) if match else 'unknown'
        
    async def extract_current_listings(self):
        """ÌòÑÏû¨ Îß§Î¨º Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        print("üè† ÌòÑÏû¨ Îß§Î¨º Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ë...")
        
        try:
            await self.page.wait_for_timeout(3000)
            
            listings = await self.page.evaluate("""
                () => {
                    const listings = [];
                    
                    // Îß§Î¨º Í¥ÄÎ†® ÏÖÄÎ†âÌÑ∞Îì§
                    const listingSelectors = [
                        '.item_link',
                        '.article_item',
                        '.property_item',
                        '[class*="item"]',
                        '[class*="article"]',
                        '[class*="property"]',
                        '.list_item'
                    ];
                    
                    for (const selector of listingSelectors) {
                        const elements = document.querySelectorAll(selector);
                        
                        elements.forEach((element, index) => {
                            const text = element.textContent.trim();
                            
                            // Î∂ÄÎèôÏÇ∞ Í¥ÄÎ†® ÌÇ§ÏõåÎìú ÌïÑÌÑ∞ÎßÅ
                            if (text && (
                                text.includes('Ïñµ') || 
                                text.includes('ÎßåÏõê') ||
                                text.includes('Ï†ÑÏÑ∏') ||
                                text.includes('ÏõîÏÑ∏') ||
                                text.includes('Îß§Îß§') ||
                                text.includes('„é°') ||
                                text.includes('Ìèâ') ||
                                text.includes('Ï∏µ')
                            ) && text.length > 20) {  // ÏµúÏÜå Í∏∏Ïù¥ Ï°∞Í±¥
                                
                                // ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú
                                const priceMatch = text.match(/(\\d+)Ïñµ\\s*(\\d+)?/);
                                const monthlyMatch = text.match(/ÏõîÏÑ∏\\s*(\\d+)/);
                                const areaMatch = text.match(/(\\d+\\.?\\d*)„é°/);
                                const floorMatch = text.match(/(\\d+)Ï∏µ/);
                                const typeMatch = text.match(/(Îß§Îß§|Ï†ÑÏÑ∏|ÏõîÏÑ∏)/);
                                
                                listings.push({
                                    index: index,
                                    selector: selector,
                                    text: text.substring(0, 300),  // Í∏∏Ïù¥ Ï†úÌïú
                                    price: priceMatch ? priceMatch[0] : null,
                                    monthly_rent: monthlyMatch ? monthlyMatch[0] : null,
                                    area: areaMatch ? areaMatch[0] : null,
                                    floor: floorMatch ? floorMatch[0] : null,
                                    deal_type: typeMatch ? typeMatch[0] : null,
                                    raw_text: text
                                });
                            }
                        });
                        
                        if (listings.length >= 30) break; // ÏµúÎåÄ 30Í∞ú
                    }
                    
                    // Ï§ëÎ≥µ Ï†úÍ±∞ (ÌÖçÏä§Ìä∏ Í∏∞Ï§Ä)
                    const unique_listings = [];
                    const seen_texts = new Set();
                    
                    for (const listing of listings) {
                        const short_text = listing.text.substring(0, 100);
                        if (!seen_texts.has(short_text)) {
                            seen_texts.add(short_text);
                            unique_listings.push(listing);
                        }
                    }
                    
                    return unique_listings.slice(0, 25); // ÏµúÎåÄ 25Í∞ú Î∞òÌôò
                }
            """)
            
            print(f"üìù Îß§Î¨º Ï†ïÎ≥¥ {len(listings)}Í∞ú Ï∂îÏ∂ú")
            return listings
            
        except Exception as e:
            print(f"‚ùå Îß§Î¨º Ï∂îÏ∂ú Ïò§Î•ò: {e}")
            return []
            
    async def extract_transaction_history(self):
        """Ïã§Í±∞ÎûòÍ∞Ä Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        print("üí∞ Ïã§Í±∞ÎûòÍ∞Ä Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ë...")
        
        try:
            # Ïã§Í±∞ÎûòÍ∞Ä ÌÉ≠ ÌÅ¥Î¶≠ ÏãúÎèÑ
            await self.page.wait_for_timeout(2000)
            
            tab_selectors = [
                'text="Ïã§Í±∞ÎûòÍ∞Ä"',
                'text="Í±∞Îûò"', 
                'text="Ïã§Í±∞Îûò"',
                '[class*="deal"]',
                '[class*="transaction"]'
            ]
            
            for selector in tab_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element:
                        await element.click()
                        print(f"‚úÖ Ïã§Í±∞ÎûòÍ∞Ä ÌÉ≠ ÌÅ¥Î¶≠ ÏÑ±Í≥µ")
                        await self.page.wait_for_timeout(3000)
                        break
                except:
                    continue
            
            # 60Í∞úÏõî Îç∞Ïù¥ÌÑ∞ ÏÑ†ÌÉù ÏãúÎèÑ
            print("‚è≥ 60Í∞úÏõî Îç∞Ïù¥ÌÑ∞ ÏÑ†ÌÉù ÏãúÎèÑ...")
            try:
                # "Í∏∞Í∞Ñ" Î≤ÑÌäº ÌÅ¥Î¶≠ (Ï†ïÌôïÌïú ÏÖÄÎ†âÌÑ∞Îäî ÏõπÌéòÏù¥ÏßÄ Íµ¨Ï°∞Ïóê Îî∞Îùº Îã§Î•º Ïàò ÏûàÏùå)
                await self.page.click('button:has-text("Í∏∞Í∞Ñ")')
                await self.page.wait_for_timeout(1000)
                # "60Í∞úÏõî" ÏòµÏÖò ÌÅ¥Î¶≠ (Ï†ïÌôïÌïú ÏÖÄÎ†âÌÑ∞Îäî ÏõπÌéòÏù¥ÏßÄ Íµ¨Ï°∞Ïóê Îî∞Îùº Îã§Î•º Ïàò ÏûàÏùå)
                await self.page.click('li:has-text("60Í∞úÏõî")')
                print("‚úÖ 60Í∞úÏõî Í∏∞Í∞Ñ ÏÑ†ÌÉù ÏÑ±Í≥µ")
                await self.page.wait_for_timeout(3000) # Îç∞Ïù¥ÌÑ∞ Î°úÎìúÎ•º ÏúÑÌï¥ ÎåÄÍ∏∞
            except Exception as e:
                print(f"‚ö†Ô∏è 60Í∞úÏõî Í∏∞Í∞Ñ ÏÑ†ÌÉù Ïã§Ìå® (ÏàòÎèô ÏÑ†ÌÉù ÌïÑÏöîÌï† Ïàò ÏûàÏùå): {e}")
                # Ïã§Ìå® Ïãú, Îã§Ïùå Î°úÏßÅÏúºÎ°ú ÏßÑÌñâ (Î™®Îì† Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏Ïò§ÏßÄ Î™ªÌï† Ïàò ÏûàÏùå)
                pass
                    
            # Ïã§Í±∞ÎûòÍ∞Ä Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
            transactions = await self.page.evaluate("""
                () => {
                    const transactions = [];
                    const text = document.body.textContent;
                    
                    // Ïã§Í±∞ÎûòÍ∞Ä Ìå®ÌÑ¥Îì§
                    const patterns = [
                        // ÎÇ†Ïßú + Í∞ÄÍ≤© Ìå®ÌÑ¥
                        /\\d{4}[\\.\\/]\\d{1,2}[\\.\\/]\\d{1,2}.*?(\\d+)Ïñµ\\s*(\\d+)?/g,
                        // Í∞ÄÍ≤© + Î©¥Ï†Å Ìå®ÌÑ¥  
                        /(\\d+)Ïñµ\\s*(\\d+)?.*?(\\d+\\.?\\d*)„é°/g,
                        // Í±∞ÎûòÏú†Ìòï + Í∞ÄÍ≤©
                        /(Îß§Îß§|Ï†ÑÏÑ∏).*?(\\d+)Ïñµ/g,
                        // Ï∏µÏàò + Í∞ÄÍ≤©
                        /(\\d+)Ï∏µ.*?(\\d+)Ïñµ/g
                    ];
                    
                    patterns.forEach((pattern, patternIndex) => {
                        let match;
                        // count < 15 Ï†úÌïú Ï†úÍ±∞
                        while ((match = pattern.exec(text)) !== null) {
                            transactions.push({
                                pattern_type: patternIndex,
                                match_text: match[0],
                                context: text.substring(
                                    Math.max(0, match.index - 100), 
                                    Math.min(text.length, match.index + 200)
                                )
                            });
                            // count++; // Ï†úÌïúÏù¥ ÏóÜÏúºÎØÄÎ°ú ÌïÑÏöî ÏóÜÏùå
                        }
                    });
                    
                    // ÌÖåÏù¥Î∏î Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
                    const tables = document.querySelectorAll('table, .table, [class*="table"]');
                    tables.forEach((table, tableIndex) => {
                        const tableText = table.textContent;
                        if ((tableText.includes('Ïñµ') || tableText.includes('ÎßåÏõê')) && 
                            tableText.length > 50) {
                            transactions.push({
                                type: 'table',
                                table_index: tableIndex,
                                content: tableText.substring(0, 800)
                            });
                        }
                    });
                    
                    return transactions; // slice(0, 40) Ï†úÌïú Ï†úÍ±∞
                }
            """)
            
            print(f"üí∏ Ïã§Í±∞ÎûòÍ∞Ä Ï†ïÎ≥¥ {len(transactions)}Í∞ú Ï∂îÏ∂ú")
            return transactions
            
        except Exception as e:
            print(f"‚ùå Ïã§Í±∞ÎûòÍ∞Ä Ï∂îÏ∂ú Ïò§Î•ò: {e}")
            return []
            
    async def extract_comprehensive_data(self, url):
        """Ï¢ÖÌï© Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú"""
        print("üìä Ï¢ÖÌï© Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú Ï§ë...")
        
        try:
            # Ïä§ÌÅ¨Î¶∞ÏÉ∑ Ï†ÄÏû•
            if self.screenshot:
                complex_id = self.extract_complex_id_from_url(url)
                screenshot_path = f"data/output/screenshot_{complex_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                Path("data/output").mkdir(parents=True, exist_ok=True)
                await self.page.screenshot(path=screenshot_path, full_page=True)
                print(f"üì∏ Ïä§ÌÅ¨Î¶∞ÏÉ∑ Ï†ÄÏû•: {screenshot_path}")
            else:
                screenshot_path = None
                
            # ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú
            detailed_info = await self.page.evaluate("""
                () => {
                    const info = {
                        page_title: document.title,
                        url: window.location.href,
                        extracted_at: new Date().toISOString()
                    };
                    
                    const fullText = document.body.textContent;
                    
                    // Î™®Îì† Í∞ÄÍ≤© Ï†ïÎ≥¥ Ï∂îÏ∂ú
                    const pricePatterns = [
                        /\\d+Ïñµ\\s*\\d*Ï≤ú?Îßå?Ïõê?/g,
                        /\\d+Ï≤úÎßåÏõê/g,
                        /\\d+ÎßåÏõê/g,
                        /ÏõîÏÑ∏\\s*\\d+Îßå?Ïõê?/g,
                        /Ï†ÑÏÑ∏\\s*\\d+Ïñµ?\\d*Ï≤ú?Îßå?Ïõê?/g,
                        /Îß§Îß§\\s*\\d+Ïñµ?\\d*Ï≤ú?Îßå?Ïõê?/g
                    ];
                    
                    info.all_prices = [];
                    pricePatterns.forEach(pattern => {
                        const matches = fullText.match(pattern) || [];
                        info.all_prices.push(...matches);
                    });
                    
                    // Ï§ëÎ≥µ Ï†úÍ±∞
                    info.all_prices = [...new Set(info.all_prices)];
                    
                    // Î©¥Ï†Å Ï†ïÎ≥¥
                    const areaMatches = fullText.match(/\\d+\\.?\\d*„é°/g) || [];
                    info.areas = [...new Set(areaMatches)];
                    
                    // Ï∏µÏàò Ï†ïÎ≥¥
                    const floorMatches = fullText.match(/\\d+Ï∏µ/g) || [];
                    info.floors = [...new Set(floorMatches)].slice(0, 15);
                    
                    // Í±∞ÎûòÏú†Ìòï
                    const dealMatches = fullText.match(/(Îß§Îß§|Ï†ÑÏÑ∏|ÏõîÏÑ∏)/g) || [];
                    info.deal_types = [...new Set(dealMatches)];
                    
                    return info;
                }
            """)
            
            detailed_info['screenshot_path'] = screenshot_path
            return detailed_info
            
        except Exception as e:
            print(f"‚ùå Ï¢ÖÌï© Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú Ïò§Î•ò: {e}")
            return {'screenshot_path': None}
            
    def analyze_price_data(self, listings, transactions, detailed_info):
        """Í∞ÄÍ≤© Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù"""
        analysis = {
            'listing_prices': [],
            'transaction_prices': [],
            'price_range': {'min': 0, 'max': 0},
            'avg_price': 0,
            'deal_type_count': {},
            'area_price_ratio': []
        }
        
        try:
            # Îß§Î¨º Í∞ÄÍ≤© Î∂ÑÏÑù
            for listing in listings:
                if listing.get('price'):
                    price_text = listing['price']
                    # Ïñµ Îã®ÏúÑÎ°ú Î≥ÄÌôò
                    match = re.search(r'(\d+)Ïñµ\s*(\d+)?', price_text)
                    if match:
                        price = int(match.group(1)) * 10000  # ÎßåÏõê Îã®ÏúÑ
                        if match.group(2):
                            price += int(match.group(2)) * 1000
                        analysis['listing_prices'].append(price)
                        
                # Í±∞ÎûòÏú†Ìòï Ïπ¥Ïö¥Ìä∏
                deal_type = listing.get('deal_type')
                if deal_type:
                    analysis['deal_type_count'][deal_type] = analysis['deal_type_count'].get(deal_type, 0) + 1
                    
            # Í∞ÄÍ≤© Î≤îÏúÑ Í≥ÑÏÇ∞
            if analysis['listing_prices']:
                analysis['price_range']['min'] = min(analysis['listing_prices'])
                analysis['price_range']['max'] = max(analysis['listing_prices'])
                analysis['avg_price'] = sum(analysis['listing_prices']) / len(analysis['listing_prices'])
                
        except Exception as e:
            print(f"‚ö†Ô∏è Í∞ÄÍ≤© Î∂ÑÏÑù Ïò§Î•ò: {e}")
            
        return analysis
        
    async def save_complex_data(self, complex_id, basic_info, listings, transactions, detailed_info, analysis):
        """Îã®ÏßÄ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        Path("data/output").mkdir(parents=True, exist_ok=True)
        
        # Ï¢ÖÌï© Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ±
        comprehensive_data = {
            'complex_basic_info': basic_info,
            'current_listings': listings,
            'transaction_history': transactions,
            'detailed_analysis': detailed_info,
            'price_analysis': analysis,
            'crawl_metadata': {
                'complex_id': complex_id,
                'crawled_at': datetime.now().isoformat(),
                'method': 'playwright_mcp_modular',
                'total_listings': len(listings),
                'total_transactions': len(transactions),
                'total_prices': len(detailed_info.get('all_prices', []))
            }
        }
        
        # JSON ÌååÏùº Ï†ÄÏû•
        json_file = f"data/output/complex_{complex_id}_comprehensive_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_data, f, ensure_ascii=False, indent=2)
            
        # CSV ÏöîÏïΩ Ï†ÄÏû•
        csv_file = f"data/output/complex_{complex_id}_summary_{timestamp}.csv"
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'complex_id', 'complex_name', 'address', 'completion_year', 'households',
                'total_listings', 'total_transactions', 'price_min', 'price_max', 'price_avg',
                'deal_types', 'areas_count', 'floors_count', 'source_url', 'screenshot_path'
            ])
            
            deal_types_str = ', '.join(analysis['deal_type_count'].keys()) if analysis['deal_type_count'] else 'Ï†ïÎ≥¥ÏóÜÏùå'
            
            writer.writerow([
                complex_id,
                basic_info.get('complexName', 'Ï†ïÎ≥¥ÏóÜÏùå'),
                basic_info.get('address', 'Ï†ïÎ≥¥ÏóÜÏùå'),
                basic_info.get('completionYear', 'Ï†ïÎ≥¥ÏóÜÏùå'),
                basic_info.get('totalHouseholds', 'Ï†ïÎ≥¥ÏóÜÏùå'),
                len(listings),
                len(transactions),
                analysis['price_range']['min'] if analysis['listing_prices'] else 0,
                analysis['price_range']['max'] if analysis['listing_prices'] else 0,
                round(analysis['avg_price']) if analysis['avg_price'] > 0 else 0,
                deal_types_str,
                len(detailed_info.get('areas', [])),
                len(detailed_info.get('floors', [])),
                basic_info.get('source_url', ''),
                detailed_info.get('screenshot_path', '')
            ])
            
        return {
            'json_file': json_file,
            'csv_file': csv_file,
            'screenshot': detailed_info.get('screenshot_path')
        }
        
    async def crawl_complex(self, complex_url, complex_name=None):
        """
        Îã®ÏßÄ ÌÅ¨Î°§ÎßÅ Î©îÏù∏ Ìï®Ïàò
        
        Args:
            complex_url (str): ÎÑ§Ïù¥Î≤Ñ Î∂ÄÎèôÏÇ∞ Îã®ÏßÄ URL
            complex_name (str): Îã®ÏßÄÎ™Ö (ÏÑ†ÌÉùÏÇ¨Ìï≠)
            
        Returns:
            dict: ÌÅ¨Î°§ÎßÅ Í≤∞Í≥º Î∞è ÌååÏùº Í≤ΩÎ°ú
        """
        try:
            await self.init_browser()
            
            complex_id = self.extract_complex_id_from_url(complex_url)
            display_name = complex_name or f"Îã®ÏßÄ_{complex_id}"
            
            print(f"\nüè† {display_name} ÌÅ¨Î°§ÎßÅ ÏãúÏûë!")
            print(f"üéØ URL: {complex_url}")
            
            # 1. Í∏∞Î≥∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú
            basic_info = await self.extract_complex_basic_info(complex_url)
            
            # 2. ÌòÑÏû¨ Îß§Î¨º Ï∂îÏ∂ú
            listings = await self.extract_current_listings()
            
            # 3. Ïã§Í±∞ÎûòÍ∞Ä Ï∂îÏ∂ú
            transactions = await self.extract_transaction_history()
            
            # 4. Ï¢ÖÌï© Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
            detailed_info = await self.extract_comprehensive_data(complex_url)
            
            # 5. Í∞ÄÍ≤© Î∂ÑÏÑù
            analysis = self.analyze_price_data(listings, transactions, detailed_info)
            
            # 6. Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
            file_paths = await self.save_complex_data(
                complex_id, basic_info, listings, transactions, detailed_info, analysis
            )
            
            # 7. Í≤∞Í≥º ÏöîÏïΩ
            print(f"\nüéâ {display_name} ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å!")
            print(f"üìä ÏàòÏßë Í≤∞Í≥º:")
            print(f"  üè¢ Îã®ÏßÄID: {complex_id}")
            print(f"  üè† Îß§Î¨º: {len(listings)}Í∞ú")
            print(f"  üí∞ Í±∞ÎûòÍ∏∞Î°ù: {len(transactions)}Í∞ú")
            print(f"  üìã Í∞ÄÍ≤©Ï†ïÎ≥¥: {len(detailed_info.get('all_prices', []))}Í∞ú")
            
            if analysis['listing_prices']:
                print(f"  üíµ Í∞ÄÍ≤©Î≤îÏúÑ: {analysis['price_range']['min']:,}~{analysis['price_range']['max']:,}ÎßåÏõê")
                print(f"  üìà ÌèâÍ∑†Í∞ÄÍ≤©: {analysis['avg_price']:,.0f}ÎßåÏõê")
                
            if analysis['deal_type_count']:
                print(f"  üè∑Ô∏è Í±∞ÎûòÏú†Ìòï: {', '.join(analysis['deal_type_count'].keys())}")
                
            return {
                'success': True,
                'complex_id': complex_id,
                'complex_name': basic_info.get('complexName', display_name),
                'data_summary': {
                    'listings_count': len(listings),
                    'transactions_count': len(transactions),
                    'prices_count': len(detailed_info.get('all_prices', [])),
                    'price_range': analysis['price_range'],
                    'avg_price': analysis['avg_price']
                },
                'files': file_paths
            }
            
        except Exception as e:
            print(f"‚ùå {display_name} ÌÅ¨Î°§ÎßÅ Ïò§Î•ò: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e),
                'complex_id': self.extract_complex_id_from_url(complex_url)
            }
            
        finally:
            if self.browser:
                await self.browser.close()

# ÏÇ¨Ïö© ÏòàÏãú Ìï®ÏàòÎì§
async def crawl_single_complex(url, name=None, headless=False):
    """Îã®Ïùº Îã®ÏßÄ ÌÅ¨Î°§ÎßÅ"""
    crawler = NaverComplexCrawler(headless=headless, screenshot=True)
    return await crawler.crawl_complex(url, name)

async def crawl_multiple_complexes(complex_list, headless=False):
    """
    Ïó¨Îü¨ Îã®ÏßÄ ÏàúÏ∞® ÌÅ¨Î°§ÎßÅ
    
    Args:
        complex_list (list): [{'url': 'URL', 'name': 'Îã®ÏßÄÎ™Ö'}, ...] ÌòïÏãùÏùò Î¶¨Ïä§Ìä∏
        headless (bool): Ìó§ÎìúÎ¶¨Ïä§ Î™®Îìú Ïó¨Î∂Ä
        
    Returns:
        list: Í∞Å Îã®ÏßÄÎ≥Ñ ÌÅ¨Î°§ÎßÅ Í≤∞Í≥º
    """
    results = []
    
    for i, complex_info in enumerate(complex_list, 1):
        print(f"\nüîÑ [{i}/{len(complex_list)}] ÌÅ¨Î°§ÎßÅ ÏßÑÌñâ Ï§ë...")
        
        crawler = NaverComplexCrawler(headless=headless, screenshot=True)
        result = await crawler.crawl_complex(
            complex_info['url'], 
            complex_info.get('name')
        )
        results.append(result)
        
        # ÏöîÏ≤≠ Í∞ÑÍ≤© (ÎÑàÎ¨¥ Îπ†Î•∏ Ïó∞ÏÜç ÏöîÏ≤≠ Î∞©ÏßÄ)
        if i < len(complex_list):
            print("‚è±Ô∏è Ïû†Ïãú ÎåÄÍ∏∞ Ï§ë...")
            await asyncio.sleep(5)
            
    return results

# Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò
async def main():
    """ÌÖåÏä§Ìä∏ Ïã§Ìñâ"""
    # Ï†ïÎì†ÌïúÏßÑ6Ï∞® ÌÖåÏä§Ìä∏
    test_url = "https://new.land.naver.com/complexes/2592?ms=37.36286,127.115578,17&a=APT:ABYG:JGC:PRE&e=RETAIL"
    
    print("üöÄ ÎÑ§Ïù¥Î≤Ñ Î∂ÄÎèôÏÇ∞ Î™®ÎìàÌôî ÌÅ¨Î°§Îü¨ ÌÖåÏä§Ìä∏")
    result = await crawl_single_complex(test_url, "Ï†ïÎì†ÌïúÏßÑ6Ï∞®", headless=False)
    
    if result['success']:
        print(f"\n‚úÖ ÌÖåÏä§Ìä∏ ÏÑ±Í≥µ!")
        print(f"üìÑ ÏÉùÏÑ±Îêú ÌååÏùº:")
        for key, path in result['files'].items():
            if path:
                print(f"  {key}: {path}")
    else:
        print(f"‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìå®: {result['error']}")

if __name__ == "__main__":
    asyncio.run(main())