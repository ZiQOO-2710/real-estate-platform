#!/usr/bin/env node

const sqlite3 = require('sqlite3').verbose()
const fs = require('fs')
const path = require('path')

class JsonToDbConverter {
  constructor() {
    this.dataDir = path.join(__dirname, '..', 'data')
    this.outputDir = path.join(this.dataDir, 'output')
    this.dbPath = path.join(this.dataDir, 'naver_crawled_data.db')
    
    this.stats = {
      json_files_processed: 0,
      complexes_created: 0,
      listings_created: 0,
      errors: 0
    }
  }

  async initializeDatabase() {
    console.log('ğŸ”§ ë„¤ì´ë²„ í¬ë¡¤ë§ DB ì´ˆê¸°í™” ì¤‘...')
    
    // ê¸°ì¡´ DB ì‚­ì œ (ìƒˆë¡œ ì‹œì‘)
    if (fs.existsSync(this.dbPath)) {
      fs.unlinkSync(this.dbPath)
    }
    
    this.db = new sqlite3.Database(this.dbPath)
    
    // ì•„íŒŒíŠ¸ ë‹¨ì§€ í…Œì´ë¸”
    await this.runQuery(`
      CREATE TABLE apartment_complexes (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        complex_id TEXT UNIQUE NOT NULL,
        complex_name TEXT,
        address TEXT,
        latitude REAL,
        longitude REAL,
        completion_year INTEGER,
        total_households INTEGER,
        total_buildings INTEGER,
        area_range TEXT,
        source_url TEXT,
        crawled_at TIMESTAMP,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `)
    
    // ë§¤ë¬¼ ì •ë³´ í…Œì´ë¸”
    await this.runQuery(`
      CREATE TABLE current_listings (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        complex_id TEXT NOT NULL,
        listing_index INTEGER,
        deal_type TEXT,
        price_text TEXT,
        price_amount INTEGER,
        area_info TEXT,
        floor_info TEXT,
        direction TEXT,
        description TEXT,
        original_text TEXT,
        extracted_at TIMESTAMP,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (complex_id) REFERENCES apartment_complexes(complex_id)
      )
    `)
    
    // í¬ë¡¤ë§ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
    await this.runQuery(`
      CREATE TABLE crawling_metadata (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        complex_id TEXT NOT NULL,
        json_filename TEXT,
        crawler_version TEXT,
        crawl_method TEXT,
        crawled_at TIMESTAMP,
        processing_status TEXT DEFAULT 'processed',
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `)
    
    // ì¸ë±ìŠ¤ ìƒì„±
    await this.runQuery("CREATE INDEX idx_complex_id ON current_listings(complex_id)")
    await this.runQuery("CREATE INDEX idx_deal_type ON current_listings(deal_type)")
    await this.runQuery("CREATE INDEX idx_complex_name ON apartment_complexes(complex_name)")
    
    console.log('âœ… DB ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ')
  }

  async runQuery(sql, params = []) {
    return new Promise((resolve, reject) => {
      this.db.run(sql, params, function(err) {
        if (err) reject(err)
        else resolve(this)
      })
    })
  }

  async getQuery(sql, params = []) {
    return new Promise((resolve, reject) => {
      this.db.get(sql, params, (err, row) => {
        if (err) reject(err)
        else resolve(row)
      })
    })
  }

  async processJsonFiles(limit = 100) {
    console.log('ğŸš€ JSON íŒŒì¼ ì²˜ë¦¬ ì‹œì‘')
    
    const jsonFiles = fs.readdirSync(this.outputDir)
      .filter(file => file.startsWith('enhanced_complex_') && file.endsWith('.json'))
      .slice(0, limit)
    
    console.log(`ğŸ“Š ${jsonFiles.length}ê°œ íŒŒì¼ ì²˜ë¦¬ ì˜ˆì •`)
    
    for (const fileName of jsonFiles) {
      try {
        await this.processSingleJson(fileName)
        this.stats.json_files_processed++
        
        if (this.stats.json_files_processed % 20 === 0) {
          console.log(`âœ… ${this.stats.json_files_processed}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ`)
          this.printStats()
        }
      } catch (error) {
        console.error(`âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ (${fileName}):`, error.message)
        this.stats.errors++
      }
    }
    
    this.printFinalStats()
  }

  async processSingleJson(fileName) {
    const filePath = path.join(this.outputDir, fileName)
    const fileContent = fs.readFileSync(filePath, 'utf8')
    const data = JSON.parse(fileContent)
    
    // 1. ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ
    const complexInfo = this.extractComplexInfo(data)
    if (!complexInfo) {
      return
    }
    
    // 2. ë‹¨ì§€ ì •ë³´ ì €ì¥
    await this.saveComplexInfo(complexInfo)
    
    // 3. ë§¤ë¬¼ ì •ë³´ ì¶”ì¶œ ë° ì €ì¥
    const listings = this.extractListings(data, complexInfo.complex_id)
    for (const listing of listings) {
      await this.saveListing(listing)
    }
    
    // 4. ë©”íƒ€ë°ì´í„° ì €ì¥
    await this.saveMetadata(data, fileName)
  }

  extractComplexInfo(data) {
    const basicInfo = data.basic_info || {}
    const crawlerInfo = data.crawler_info || {}
    const listings = data.current_listings || []
    
    const complexId = basicInfo.complexId || crawlerInfo.complex_id
    if (!complexId) {
      return null
    }
    
    // ë§¤ë¬¼ì—ì„œ ë‹¨ì§€ëª… ì¶”ì¶œ
    const complexName = this.extractComplexNameFromListings(listings)
    
    // ì¢Œí‘œ ì¶”ì¶œ (URLì—ì„œ)
    const coords = this.extractCoordinates(basicInfo.url || '')
    
    return {
      complex_id: String(complexId),
      complex_name: complexName,
      address: this.extractAddressFromUrl(basicInfo.url || ''),
      latitude: coords.lat,
      longitude: coords.lng,
      source_url: basicInfo.source_url || basicInfo.url,
      crawled_at: crawlerInfo.crawled_at || basicInfo.extracted_at,
      listing_count: listings.length
    }
  }

  extractComplexNameFromListings(listings) {
    if (!listings || listings.length === 0) {
      return 'ì •ë³´ì—†ìŒ'
    }
    
    for (const listing of listings.slice(0, 3)) {
      const text = listing.text || ''
      if (text) {
        // "ì •ë“ í•œì§„6ì°¨ 601ë™ë§¤ë§¤14ì–µ..." íŒ¨í„´ì—ì„œ ë‹¨ì§€ëª… ì¶”ì¶œ
        const match = text.match(/^([^\s]+(?:\s*\d+ì°¨)?)/)
        if (match) {
          const name = match[1].trim()
          // ì¼ë°˜ì ì´ì§€ ì•Šì€ ì´ë¦„ í•„í„°ë§
          if (name.length > 2 && !name.includes('ë™ë§¤ë§¤') && !name.includes('ë™ì „ì„¸')) {
            return name
          }
        }
      }
    }
    
    return 'ì •ë³´ì—†ìŒ'
  }

  extractCoordinates(url) {
    if (!url) {
      return { lat: null, lng: null }
    }
    
    // ms=37.36286,127.115578,17 íŒ¨í„´ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ
    const match = url.match(/ms=([0-9.]+),([0-9.]+)/)
    if (match) {
      return {
        lat: parseFloat(match[1]),
        lng: parseFloat(match[2])
      }
    }
    
    return { lat: null, lng: null }
  }

  extractAddressFromUrl(url) {
    if (!url) {
      return ''
    }
    
    const coords = this.extractCoordinates(url)
    if (coords.lat && coords.lng) {
      return `ì¶”ì •ì¢Œí‘œ: ${coords.lat}, ${coords.lng}`
    }
    
    return ''
  }

  extractListings(data, complexId) {
    const listings = data.current_listings || []
    const result = []
    
    for (const listing of listings) {
      const dealType = this.normalizeDealType(listing.deal_type)
      if (!dealType) {
        continue
      }
      
      const priceAmount = this.parsePrice(listing.price)
      
      result.push({
        complex_id: complexId,
        listing_index: listing.index,
        deal_type: dealType,
        price_text: listing.price,
        price_amount: priceAmount,
        area_info: listing.area,
        floor_info: listing.floor,
        description: this.cleanText(listing.text || ''),
        original_text: listing.text,
        extracted_at: listing.extracted_at
      })
    }
    
    return result
  }

  normalizeDealType(dealType) {
    if (!dealType) {
      return null
    }
    
    const normalized = dealType.toLowerCase().trim()
    if (normalized.includes('ë§¤ë§¤') || normalized === 'sale') {
      return 'ë§¤ë§¤'
    } else if (normalized.includes('ì „ì„¸') || normalized === 'jeonse') {
      return 'ì „ì„¸'
    } else if (normalized.includes('ì›”ì„¸') || normalized === 'monthly') {
      return 'ì›”ì„¸'
    }
    
    return null
  }

  parsePrice(priceStr) {
    if (!priceStr) {
      return null
    }
    
    // "14ì–µ 5,000", "8ì–µ", "22ì–µ" ë“±ì˜ í˜•ì‹ íŒŒì‹±
    const cleanPrice = priceStr.replace(/[,\s]/g, '')
    
    // ì–µì› ë‹¨ìœ„ ì¶”ì¶œ
    const billionMatch = cleanPrice.match(/(\d+(?:\.\d+)?)ì–µ/)
    if (billionMatch) {
      let amount = parseFloat(billionMatch[1]) * 10000 // ë§Œì› ë‹¨ìœ„ë¡œ ë³€í™˜
      
      // ì²œë§Œì› ë‹¨ìœ„ ì¶”ê°€
      const thousandMatch = cleanPrice.match(/(\d+)ì²œ/)
      if (thousandMatch) {
        amount += parseInt(thousandMatch[1]) * 1000
      }
      
      return Math.round(amount)
    }
    
    // ë§Œì› ë‹¨ìœ„ë§Œ ìˆëŠ” ê²½ìš°
    const millionMatch = cleanPrice.match(/(\d+)ë§Œ/)
    if (millionMatch) {
      return parseInt(millionMatch[1])
    }
    
    return null
  }

  cleanText(text) {
    if (!text) {
      return ''
    }
    
    // ë‹¨ì§€ëª… ë¶€ë¶„ ì œê±°í•˜ê³  ì„¤ëª…ë§Œ ì¶”ì¶œ
    const parts = text.split(' ')
    if (parts.length >= 3) {
      return parts.slice(2).join(' ') // ë‹¨ì§€ëª…ê³¼ ë™í˜¸ìˆ˜ ì œê±°
    }
    
    return text
  }

  async saveComplexInfo(complexInfo) {
    try {
      await this.runQuery(`
        INSERT OR REPLACE INTO apartment_complexes 
        (complex_id, complex_name, address, latitude, longitude, 
         source_url, crawled_at)
        VALUES (?, ?, ?, ?, ?, ?, ?)
      `, [
        complexInfo.complex_id,
        complexInfo.complex_name,
        complexInfo.address,
        complexInfo.latitude,
        complexInfo.longitude,
        complexInfo.source_url,
        complexInfo.crawled_at
      ])
      
      this.stats.complexes_created++
      
    } catch (error) {
      // ì¤‘ë³µ ì‹œ ì—…ë°ì´íŠ¸
      await this.runQuery(`
        UPDATE apartment_complexes 
        SET complex_name = ?, address = ?, latitude = ?, longitude = ?
        WHERE complex_id = ?
      `, [
        complexInfo.complex_name,
        complexInfo.address,
        complexInfo.latitude,
        complexInfo.longitude,
        complexInfo.complex_id
      ])
    }
  }

  async saveListing(listing) {
    await this.runQuery(`
      INSERT INTO current_listings 
      (complex_id, listing_index, deal_type, price_text, price_amount,
       area_info, floor_info, description, original_text, extracted_at)
      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    `, [
      listing.complex_id,
      listing.listing_index,
      listing.deal_type,
      listing.price_text,
      listing.price_amount,
      listing.area_info,
      listing.floor_info,
      listing.description,
      listing.original_text,
      listing.extracted_at
    ])
    
    this.stats.listings_created++
  }

  async saveMetadata(data, filename) {
    const basicInfo = data.basic_info || {}
    const crawlerInfo = data.crawler_info || {}
    
    const complexId = basicInfo.complexId || crawlerInfo.complex_id
    
    await this.runQuery(`
      INSERT INTO crawling_metadata 
      (complex_id, json_filename, crawler_version, crawl_method, crawled_at)
      VALUES (?, ?, ?, ?, ?)
    `, [
      String(complexId),
      filename,
      crawlerInfo.version,
      crawlerInfo.crawl_method,
      crawlerInfo.crawled_at
    ])
  }

  printStats() {
    console.log(`   ğŸ“Š í˜„ì¬ê¹Œì§€: ë‹¨ì§€ ${this.stats.complexes_created}ê°œ, ë§¤ë¬¼ ${this.stats.listings_created}ê°œ`)
  }

  async printFinalStats() {
    console.log('\n' + '='.repeat(60))
    console.log('ğŸ‰ JSON â†’ DB ë³€í™˜ ì™„ë£Œ!')
    console.log('='.repeat(60))
    console.log(`ğŸ“Š ìµœì¢… ë³€í™˜ ê²°ê³¼:`)
    console.log(`   â€¢ JSON íŒŒì¼ ì²˜ë¦¬: ${this.stats.json_files_processed}ê°œ`)
    console.log(`   â€¢ ì•„íŒŒíŠ¸ ë‹¨ì§€ ìƒì„±: ${this.stats.complexes_created}ê°œ`)
    console.log(`   â€¢ ë§¤ë¬¼ ì •ë³´ ìƒì„±: ${this.stats.listings_created}ê°œ`)
    console.log(`   â€¢ ì˜¤ë¥˜ ë°œìƒ: ${this.stats.errors}ê°œ`)
    
    // DB ìµœì¢… í™•ì¸
    const complexCount = await this.getQuery("SELECT COUNT(*) as count FROM apartment_complexes")
    const listingCount = await this.getQuery("SELECT COUNT(*) as count FROM current_listings")
    
    console.log(`\nâœ… DB ìµœì¢… ìƒíƒœ:`)
    console.log(`   â€¢ ì´ ë‹¨ì§€ ìˆ˜: ${complexCount.count}ê°œ`)
    console.log(`   â€¢ ì´ ë§¤ë¬¼ ìˆ˜: ${listingCount.count}ê°œ`)
    console.log(`   â€¢ DB íŒŒì¼: ${this.dbPath}`)
    
    // ìƒ˜í”Œ ë°ì´í„° í™•ì¸
    console.log('\nğŸ“‹ ì‹¤ì œ ë‹¨ì§€ëª… ìƒ˜í”Œ:')
    const samples = await new Promise((resolve, reject) => {
      this.db.all(`
        SELECT c.complex_name, COUNT(l.id) as listing_count 
        FROM apartment_complexes c 
        LEFT JOIN current_listings l ON c.complex_id = l.complex_id 
        WHERE c.complex_name != 'ì •ë³´ì—†ìŒ'
        GROUP BY c.complex_id 
        ORDER BY listing_count DESC 
        LIMIT 5
      `, [], (err, rows) => {
        if (err) reject(err)
        else resolve(rows)
      })
    })
    
    samples.forEach(row => {
      console.log(`   â€¢ ${row.complex_name}: ${row.listing_count}ê°œ ë§¤ë¬¼`)
    })
    
    console.log('='.repeat(60))
  }

  async run(limit = 100) {
    try {
      await this.initializeDatabase()
      await this.processJsonFiles(limit)
      
    } catch (error) {
      console.error(`âŒ ë³€í™˜ ì‹¤íŒ¨:`, error)
    } finally {
      if (this.db) {
        this.db.close()
      }
    }
  }
}

// ì‹¤í–‰
if (require.main === module) {
  const converter = new JsonToDbConverter()
  converter.run(100) // ì²˜ìŒ 100ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬
}

module.exports = JsonToDbConverter