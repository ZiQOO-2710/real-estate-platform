"""
ë„¤ì´ë²„ ë¶€ë™ì‚° ê°•í™” í¬ë¡¤ëŸ¬ (2025 ê°œì¸ìš©)
ìŠ¤í…”ìŠ¤ ëª¨ë“œ + ìš°íšŒ ê¸°ìˆ  ì ìš©

ê°œì¸ì  ì—°êµ¬ ëª©ì ìœ¼ë¡œ ì‚¬ìš©
"""

import asyncio
import json
import csv
import random
import time
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright
import re
from fake_useragent import UserAgent
import requests
from .duplicate_detector import DuplicateDetector, remove_duplicates_from_listings
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from database.simple_data_processor import process_json_file

class EnhancedNaverCrawler:
    """ë„¤ì´ë²„ ë¶€ë™ì‚° ê°•í™” í¬ë¡¤ëŸ¬ - ê°œì¸ìš©"""
    
    def __init__(self, headless=True, stealth_mode=True):
        self.browser = None
        self.page = None
        self.context = None
        self.headless = headless
        self.stealth_mode = stealth_mode
        self.ua = UserAgent()
        
        # ìŠ¤í…”ìŠ¤ ì„¤ì •
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15'
        ]
        
    async def init_stealth_browser(self):
        """ìŠ¤í…”ìŠ¤ ëª¨ë“œ ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
        print("ğŸ¥· ìŠ¤í…”ìŠ¤ ëª¨ë“œ ë¸Œë¼ìš°ì € ì‹œì‘...")
        
        playwright = await async_playwright().start()
        
        # ëœë¤ User-Agent ì„ íƒ
        current_ua = random.choice(self.user_agents)
        
        # ê³ ê¸‰ ìŠ¤í…”ìŠ¤ ì„¤ì •
        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-features=VizDisplayCompositor',
                '--disable-web-security',
                '--disable-features=TranslateUI',
                '--disable-ipc-flooding-protection',
                '--disable-renderer-backgrounding',
                '--disable-backgrounding-occluded-windows',
                '--disable-client-side-phishing-detection',
                '--disable-component-extensions-with-background-pages',
                '--disable-default-apps',
                '--disable-extensions',
                '--disable-hang-monitor',
                '--no-first-run',
                '--no-default-browser-check',
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--disable-software-rasterizer',
                '--hide-scrollbars',
                '--mute-audio',
                '--disable-background-timer-throttling',
                '--disable-backgrounding-occluded-windows',
                '--disable-renderer-backgrounding',
                '--disable-features=TranslateUI',
                '--disable-features=BlinkGenPropertyTrees',
                '--disable-accelerated-2d-canvas',
                '--disable-accelerated-jpeg-decoding',
                '--disable-accelerated-mjpeg-decode',
                '--disable-accelerated-video-decode',
                '--disable-app-list-dismiss-on-blur',
                '--disable-accelerated-video-encode'
            ]
        )
        
        # ìŠ¤í…”ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        self.context = await self.browser.new_context(
            viewport={'width': 1366, 'height': 768},
            user_agent=current_ua,
            locale='ko-KR',
            timezone_id='Asia/Seoul',
            color_scheme='light',
            extra_http_headers={
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0'
            }
        )
        
        self.page = await self.context.new_page()
        
        # ì§„ë³´ëœ ìŠ¤í…”ìŠ¤ ìŠ¤í¬ë¦½íŠ¸
        await self.page.add_init_script("""
            // webdriver ì œê±°
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
            
            // ë¸Œë¼ìš°ì € í”ŒëŸ¬ê·¸ì¸ ì‹œë®¬ë ˆì´ì…˜
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
            
            // ì–¸ì–´ ì„¤ì •
            Object.defineProperty(navigator, 'languages', {
                get: () => ['ko-KR', 'ko', 'en-US', 'en'],
            });
            
            // í”Œë«í¼ ì •ë³´
            Object.defineProperty(navigator, 'platform', {
                get: () => 'Win32',
            });
            
            // í™”ë©´ ì •ë³´
            Object.defineProperty(screen, 'width', {
                get: () => 1366,
            });
            Object.defineProperty(screen, 'height', {
                get: () => 768,
            });
            
            // ì½˜ì†” í•¨ìˆ˜ ì œê±°
            delete window.console.debug;
            delete window.console.clear;
            
            // Automation íƒì§€ ìš°íšŒ
            window.chrome = {
                runtime: {},
                loadTimes: function() {},
                csi: function() {},
                app: {}
            };
            
            // Permission API ì œê±°
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );
            
            // WebGL ë²¤ë” ì œê±°
            const getParameter = WebGLRenderingContext.getParameter;
            WebGLRenderingContext.prototype.getParameter = function(parameter) {
                if (parameter === 37445) {
                    return 'Intel Inc.';
                }
                if (parameter === 37446) {
                    return 'Intel Iris OpenGL Engine';
                }
                return getParameter(parameter);
            };
        """)
        
        print(f"âœ… ìŠ¤í…”ìŠ¤ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ (UA: {current_ua[:50]}...)")
        
    async def human_like_delay(self, min_seconds=2, max_seconds=5):
        """ì¸ê°„ì  ëŒ€ê¸° ì‹œê°„"""
        delay = random.uniform(min_seconds, max_seconds)
        print(f"â³ ì¸ê°„ì  ëŒ€ê¸°: {delay:.1f}ì´ˆ")
        await asyncio.sleep(delay)
        
    async def simulate_human_behavior(self):
        """ì¸ê°„ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜"""
        # ëœë¤ ë§ˆìš°ìŠ¤ ì´ë™
        await self.page.mouse.move(
            random.randint(100, 800), 
            random.randint(100, 600)
        )
        await asyncio.sleep(random.uniform(0.5, 1.5))
        
        # ëœë¤ ìŠ¤í¬ë¡¤
        await self.page.evaluate("""
            window.scrollTo({
                top: Math.random() * 500,
                behavior: 'smooth'
            });
        """)
        await asyncio.sleep(random.uniform(1, 2))
        
    async def safe_navigate(self, url, retries=3):
        """ì•ˆì „í•œ í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜"""
        for attempt in range(retries):
            try:
                print(f"ğŸŒ í˜ì´ì§€ ì´ë™ ì‹œë„ {attempt + 1}/{retries}: {url}")
                
                # ë„¤ë¹„ê²Œì´ì…˜ ì „ ëŒ€ê¸°
                if attempt > 0:
                    await self.human_like_delay(3, 7)
                
                # í˜ì´ì§€ ë¡œë“œ
                response = await self.page.goto(
                    url, 
                    wait_until="networkidle", 
                    timeout=30000
                )
                
                if response and response.status == 200:
                    print(f"âœ… í˜ì´ì§€ ë¡œë“œ ì„±ê³µ (HTTP {response.status})")
                    
                    # ì¸ê°„ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜
                    await self.simulate_human_behavior()
                    
                    # ì¶”ê°€ ëŒ€ê¸°
                    await self.human_like_delay(2, 4)
                    
                    return True
                else:
                    print(f"âš ï¸ HTTP ì˜¤ë¥˜: {response.status if response else 'No response'}")
                    
            except Exception as e:
                print(f"âŒ ë„¤ë¹„ê²Œì´ì…˜ ì˜¤ë¥˜ {attempt + 1}: {e}")
                if attempt < retries - 1:
                    await self.human_like_delay(5, 10)
                    
        return False
        
    async def extract_with_multiple_strategies(self, strategies):
        """ë‹¤ì¤‘ ì „ëµìœ¼ë¡œ ë°ì´í„° ì¶”ì¶œ"""
        results = []
        
        for i, strategy in enumerate(strategies):
            try:
                print(f"ğŸ” ì „ëµ {i+1} ì‹œë„: {strategy['name']}")
                
                # ì „ëµ ì‹¤í–‰
                data = await self.page.evaluate(strategy['script'])
                
                if data and len(data) > 0:
                    print(f"âœ… ì „ëµ {i+1} ì„±ê³µ: {len(data)}ê°œ ë°ì´í„° ì¶”ì¶œ")
                    results.extend(data)
                else:
                    print(f"âš ï¸ ì „ëµ {i+1} ë°ì´í„° ì—†ìŒ")
                    
                # ì „ëµ ê°„ ëŒ€ê¸°
                await asyncio.sleep(random.uniform(1, 3))
                
            except Exception as e:
                print(f"âŒ ì „ëµ {i+1} ì˜¤ë¥˜: {e}")
                
        # ì¤‘ë³µ ì œê±°
        unique_results = []
        seen_texts = set()
        
        for item in results:
            item_text = str(item).lower().replace(' ', '')[:100]
            if item_text not in seen_texts:
                seen_texts.add(item_text)
                unique_results.append(item)
                
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(unique_results)}ê°œ (ì „ì²´ {len(results)}ê°œì—ì„œ ì¤‘ë³µ ì œê±°)")
        return unique_results
        
    async def extract_complex_info(self, url):
        """ë‹¨ì§€ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ (ê°•í™”ë²„ì „)"""
        print(f"ğŸ¢ ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ ì‹œì‘...")
        
        # ë‹¤ì¤‘ ì „ëµ ì •ì˜
        strategies = [
            {
                'name': 'ê¸°ë³¸ ì…€ë ‰í„°',
                'script': """
                    () => {
                        const info = {};
                        
                        // ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ ë‹¨ì§€ëª… ì°¾ê¸°
                        const titleSelectors = [
                            'h1', '.complex_title', '.title', '.complex_name',
                            '[class*="title"]', '[class*="name"]', '.page-title'
                        ];
                        
                        for (const selector of titleSelectors) {
                            const element = document.querySelector(selector);
                            if (element && element.textContent.trim() && 
                                !element.textContent.includes('ë„¤ì´ë²„') &&
                                !element.textContent.includes('@') &&
                                element.textContent.length < 100) {
                                info.complexName = element.textContent.trim();
                                break;
                            }
                        }
                        
                        // ì£¼ì†Œ ì •ë³´
                        const addressSelectors = [
                            '.address', '.complex_address', '[class*="address"]',
                            '[class*="location"]', '.location'
                        ];
                        
                        for (const selector of addressSelectors) {
                            const element = document.querySelector(selector);
                            if (element && element.textContent.trim() &&
                                element.textContent.includes('ì‹œ') ||
                                element.textContent.includes('êµ¬') ||
                                element.textContent.includes('ë™')) {
                                info.address = element.textContent.trim();
                                break;
                            }
                        }
                        
                        // í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
                        const fullText = document.body.textContent;
                        
                        // ì¤€ê³µë…„ë„ íŒ¨í„´
                        const yearPatterns = [
                            /(19|20)\d{2}ë…„\s*ì¤€?ê³µ?/,
                            /ì¤€ê³µ\s*(19|20)\d{2}/,
                            /(19|20)\d{2}\.â€‹\d{1,2}/
                        ];
                        
                        for (const pattern of yearPatterns) {
                            const match = fullText.match(pattern);
                            if (match) {
                                info.completionYear = match[0];
                                break;
                            }
                        }
                        
                        // ì„¸ëŒ€ìˆ˜
                        const householdMatch = fullText.match(/(\d+)\s*ì„¸ëŒ€/);
                        if (householdMatch) {
                            info.totalHouseholds = householdMatch[1];
                        }
                        
                        return [info];
                    }
                """
            },
            {
                'name': 'URL ê¸°ë°˜ ì¶”ì¶œ',
                'script': """
                    () => {
                        const info = {
                            url: window.location.href,
                            title: document.title,
                            timestamp: new Date().toISOString()
                        };
                        
                        // URLì—ì„œ ë‹¨ì§€ ID ì¶”ì¶œ
                        const urlMatch = window.location.href.match(/complexes\/(\d+)/);
                        if (urlMatch) {
                            info.complexId = urlMatch[1];
                        }
                        
                        return [info];
                    }
                """
            }
        ]
        
        results = await self.extract_with_multiple_strategies(strategies)
        
        # ê²°ê³¼ ë³‘í•©
        combined_info = {
            'complex_id': self.extract_complex_id_from_url(url),
            'source_url': url,
            'extracted_at': datetime.now().isoformat()
        }
        
        for result in results:
            if isinstance(result, dict):
                combined_info.update(result)
                
        return combined_info
        
    def extract_complex_id_from_url(self, url):
        """ë‹¨ì§€ ID ì¶”ì¶œ"""
        match = re.search(r'/complexes/(\d+)', url)
        return match.group(1) if match else f'unknown_{int(time.time())}'
        
    async def extract_listings(self):
        """ë§¤ë¬¼ ì •ë³´ ì¶”ì¶œ (ê°•í™”ë²„ì „)"""
        print("ğŸ  ë§¤ë¬¼ ì •ë³´ ì¶”ì¶œ ì‹œì‘...")
        
        strategies = [
            {
                'name': 'ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ íƒì§€',
                'script': """
                    () => {
                        const listings = [];
                        
                        // ë‹¤ì–‘í•œ ë§¤ë¬¼ ì…€ë ‰í„°
                        const listingSelectors = [
                            '.item_link', '.article_item', '.property_item',
                            '[class*="item"]', '[class*="article"]', 
                            '[class*="property"]', '.list_item', '.listing',
                            '[class*="listing"]', '[class*="deal"]'
                        ];
                        
                        for (const selector of listingSelectors) {
                            const elements = document.querySelectorAll(selector);
                            
                            elements.forEach((element, index) => {
                                const text = element.textContent.trim();
                                
                                if (text && text.length > 30 && (
                                    text.includes('ì–µ') || 
                                    text.includes('ë§Œì›') ||
                                    text.includes('ì „ì„¸') ||
                                    text.includes('ì›”ì„¸') ||
                                    text.includes('ë§¤ë§¤') ||
                                    text.includes('ã¡') ||
                                    text.includes('ì¸µ')
                                )) {
                                    const priceMatch = text.match(/(\d+)ì–µ\s*(\d+)?/);
                                    const areaMatch = text.match(/(\d+\.?\d*)ã¡/);
                                    const floorMatch = text.match(/(\d+)ì¸µ/);
                                    const typeMatch = text.match(/(ë§¤ë§¤|ì „ì„¸|ì›”ì„¸)/);
                                    
                                    listings.push({
                                        index: index,
                                        selector: selector,
                                        text: text.substring(0, 300),
                                        price: priceMatch ? priceMatch[0] : null,
                                        area: areaMatch ? areaMatch[0] : null,
                                        floor: floorMatch ? floorMatch[0] : null,
                                        deal_type: typeMatch ? typeMatch[0] : null,
                                        extracted_at: new Date().toISOString()
                                    });
                                }
                            });
                            
                            if (listings.length >= 50) break;
                        }
                        
                        return listings;
                    }
                """
            },
            {
                'name': 'í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ',
                'script': """
                    () => {
                        const listings = [];
                        
                        // í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                        const tables = document.querySelectorAll('table, .table, [class*="table"]');
                        
                        tables.forEach((table, tableIndex) => {
                            const rows = table.querySelectorAll('tr');
                            
                            rows.forEach((row, rowIndex) => {
                                const text = row.textContent.trim();
                                
                                if (text && (
                                    text.includes('ì–µ') || 
                                    text.includes('ë§Œì›') ||
                                    text.includes('ã¡')
                                )) {
                                    listings.push({
                                        type: 'table_row',
                                        table_index: tableIndex,
                                        row_index: rowIndex,
                                        text: text.substring(0, 200),
                                        extracted_at: new Date().toISOString()
                                    });
                                }
                            });
                        });
                        
                        return listings;
                    }
                """
            }
        ]
        
        raw_listings = await self.extract_with_multiple_strategies(strategies)
        
        # ì¤‘ë³µ ì œê±° ì ìš©
        if raw_listings:
            print(f"ğŸ”„ ì¤‘ë³µ ë§¤ë¬¼ ì œê±° ì¤‘... (ì›ë³¸: {len(raw_listings)}ê°œ)")
            unique_listings, duplicate_report = remove_duplicates_from_listings(raw_listings)
            print(duplicate_report)
            return unique_listings
        
        return raw_listings
        
    async def extract_transactions(self):
        """ì‹¤ê±°ë˜ê°€ ì •ë³´ ì¶”ì¶œ (ê°•í™”ë²„ì „)"""
        print("ğŸ’° ì‹¤ê±°ë˜ê°€ ì •ë³´ ì¶”ì¶œ ì‹œì‘...")
        
        # ì‹¤ê±°ë˜ê°€ íƒ­ ì°¾ê¸° ë° í´ë¦­ ì‹œë„
        await self.try_click_transaction_tab()
        
        strategies = [
            {
                'name': 'ì‹¤ê±°ë˜ê°€ íŒ¨í„´ ì¶”ì¶œ',
                'script': """
                    () => {
                        const transactions = [];
                        const text = document.body.textContent;
                        
                        // ë‹¤ì–‘í•œ ì‹¤ê±°ë˜ê°€ íŒ¨í„´
                        const patterns = [
                            // ë‚ ì§œ + ê°€ê²© íŒ¨í„´
                            /\d{4}[\.\/]\d{1,2}[\.\/]\d{1,2}.*?(\d+)ì–µ\s*(\d+)?/g,
                            // ê°€ê²© + ë©´ì  íŒ¨í„´  
                            /(\d+)ì–µ\s*(\d+)?.*?(\d+\.?\d*)ã¡/g,
                            // ê±°ë˜ìœ í˜• + ê°€ê²©
                            /(ë§¤ë§¤|ì „ì„¸).*?(\d+)ì–µ/g,
                            // ì¸µìˆ˜ + ê°€ê²©
                            /(\d+)ì¸µ.*?(\d+)ì–µ/g,
                            // ì›”ë³„ ê±°ë˜ íŒ¨í„´
                            /(\d{4})\.(\d{1,2}).*?(\d+)ì–µ/g
                        ];
                        
                        patterns.forEach((pattern, patternIndex) => {
                            let match;
                            let count = 0;
                            while ((match = pattern.exec(text)) !== null && count < 30) {
                                transactions.push({
                                    pattern_type: patternIndex,
                                    match_text: match[0],
                                    context: text.substring(
                                        Math.max(0, match.index - 50), 
                                        Math.min(text.length, match.index + 150)
                                    ),
                                    extracted_at: new Date().toISOString()
                                });
                                count++;
                            }
                        });
                        
                        return transactions;
                    }
                """
            },
            {
                'name': 'ì‹¤ê±°ë˜ê°€ í…Œì´ë¸”',
                'script': """
                    () => {
                        const transactions = [];
                        
                        // ì‹¤ê±°ë˜ê°€ í…Œì´ë¸” ì°¾ê¸°
                        const tables = document.querySelectorAll('table, .table, [class*="table"]');
                        
                        tables.forEach((table, tableIndex) => {
                            const tableText = table.textContent;
                            
                            if ((tableText.includes('ì–µ') || tableText.includes('ë§Œì›')) && 
                                tableText.length > 100) {
                                
                                const rows = table.querySelectorAll('tr');
                                rows.forEach((row, rowIndex) => {
                                    const rowText = row.textContent.trim();
                                    
                                    if (rowText.includes('ì–µ') || rowText.includes('ë§Œì›')) {
                                        transactions.push({
                                            type: 'transaction_table',
                                            table_index: tableIndex,
                                            row_index: rowIndex,
                                            content: rowText.substring(0, 200),
                                            extracted_at: new Date().toISOString()
                                        });
                                    }
                                });
                            }
                        });
                        
                        return transactions;
                    }
                """
            }
        ]
        
        return await self.extract_with_multiple_strategies(strategies)
        
    async def try_click_transaction_tab(self):
        """ì‹¤ê±°ë˜ê°€ íƒ­ í´ë¦­ ì‹œë„"""
        tab_selectors = [
            'text="ì‹¤ê±°ë˜ê°€"',
            'text="ê±°ë˜"', 
            'text="ì‹¤ê±°ë˜"',
            '[class*="deal"]',
            '[class*="transaction"]',
            'button:ê¶Œ:has-text("ì‹¤ê±°ë˜")',
            'a:ê¶Œ:has-text("ì‹¤ê±°ë˜")',
            '.tab:ê¶Œ:has-text("ì‹¤ê±°ë˜")'
        ]
        
        for selector in tab_selectors:
            try:
                element = await self.page.query_selector(selector)
                if element:
                    print(f"ğŸ“ ì‹¤ê±°ë˜ê°€ íƒ­ ë°œê²¬: {selector}")
                    await element.click()
                    await self.human_like_delay(2, 4)
                    print("âœ… ì‹¤ê±°ë˜ê°€ íƒ­ í´ë¦­ ì„±ê³µ")
                    return True
            except Exception as e:
                print(f"âš ï¸ íƒ­ í´ë¦­ ì‹¤íŒ¨ ({selector}): {e}")
                continue
                
        print("âš ï¸ ì‹¤ê±°ë˜ê°€ íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return False
        
    async def take_enhanced_screenshot(self, complex_id):
        """ê°•í™”ëœ ìŠ¤í¬ë¦°ìƒ·"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            screenshot_path = f"data/output/enhanced_screenshot_{complex_id}_{timestamp}.png"
            Path("data/output").mkdir(parents=True, exist_ok=True)
            
            # ì „ì²´ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· (PNGëŠ” quality ì˜µì…˜ ë¶ˆê°€)
            await self.page.screenshot(
                path=screenshot_path, 
                full_page=True
            )
            
            print(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
            return screenshot_path
            
        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë¦°ìƒ· ì˜¤ë¥˜: {e}")
            return None
            
    async def save_enhanced_data(self, complex_id, basic_info, listings, transactions, screenshot_path):
        """ê°•í™”ëœ ë°ì´í„° ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        Path("data/output").mkdir(parents=True, exist_ok=True)
        
        # ì¢…í•© ë°ì´í„° êµ¬ì„±
        comprehensive_data = {
            'crawler_info': {
                'version': '2.0 Enhanced',
                'crawl_method': 'stealth_mode',
                'crawled_at': datetime.now().isoformat(),
                'complex_id': complex_id
            },
            'basic_info': basic_info,
            'current_listings': listings,
            'transaction_history': transactions,
            'statistics': {
                'total_listings': len(listings),
                'total_transactions': len(transactions),
                'data_quality': 'enhanced'
            },
            'files': {
                'screenshot': screenshot_path
            }
        }
        
        # JSON íŒŒì¼ ì €ì¥
        json_file = f"data/output/enhanced_complex_{complex_id}_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_data, f, ensure_ascii=False, indent=2)
            
        # CSV ìš”ì•½ ì €ì¥
        csv_file = f"data/output/enhanced_summary_{complex_id}_{timestamp}.csv"
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'complex_id', 'complex_name', 'address', 'completion_year',
                'total_listings', 'total_transactions', 'screenshot_path',
                'crawl_method', 'crawled_at'
            ])
            
            writer.writerow([
                complex_id,
                basic_info.get('complexName', 'ì •ë³´ì—†ìŒ'),
                basic_info.get('address', 'ì •ë³´ì—†ìŒ'),
                basic_info.get('completionYear', 'ì •ë³´ì—†ìŒ'),
                len(listings),
                len(transactions),
                screenshot_path,
                'enhanced_stealth',
                datetime.now().isoformat()
            ])
            
        return {
            'json_file': json_file,
            'csv_file': csv_file,
            'screenshot': screenshot_path
        }
        
    async def crawl_complex_enhanced(self, url, name=None):
        """ê°•í™”ëœ ë‹¨ì§€ í¬ë¡¤ë§"""
        try:
            # ìŠ¤í…”ìŠ¤ ë¸Œë¼ìš°ì € ì´ˆê¸°í™”
            await self.init_stealth_browser()
            
            complex_id = self.extract_complex_id_from_url(url)
            display_name = name or f"ë‹¨ì§€_{complex_id}"
            
            print(f"\nğŸ  {display_name} ê°•í™” í¬ë¡¤ë§ ì‹œì‘!")
            print(f"ğŸ† URL: {url}")
            
            # 1. ì•ˆì „í•œ í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜
            if not await self.safe_navigate(url):
                raise Exception("í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
                
            # 2. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            basic_info = await self.extract_complex_info(url)
            
            # 3. ë§¤ë¬¼ ì •ë³´ ì¶”ì¶œ
            listings = await self.extract_listings()
            
            # 4. ì‹¤ê±°ë˜ê°€ ì¶”ì¶œ (êµ­í† ë¶€ ë°ì´í„°ë¡œ ëŒ€ì²´ ì˜ˆì •ì´ë¯€ë¡œ ìŠ¤í‚µ)
            print("ğŸ’° ì‹¤ê±°ë˜ê°€ ì •ë³´ ì¶”ì¶œ ìŠ¤í‚µ (êµ­í† ë¶€ ë°ì´í„° ì‚¬ìš© ì˜ˆì •)")
            transactions = []
            
            # 5. ìŠ¤í¬ë¦°ìƒ· ì´¬ì˜
            screenshot_path = await self.take_enhanced_screenshot(complex_id)
            
            # 6. ë°ì´í„° ì €ì¥
            file_paths = await self.save_enhanced_data(
                complex_id, basic_info, listings, transactions, screenshot_path
            )
            
            # 7. DB ì €ì¥
            json_file = file_paths.get('json_file')
            db_success = False
            if json_file:
                db_success = process_json_file(json_file, {'database': 'data/naver_real_estate.db'})
            
            # 8. ê²°ê³¼ ìš”ì•½
            print(f"\nğŸ‰ {display_name} ê°•í™” í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
            print(f"  ğŸ¢ ë‹¨ì§€ID: {complex_id}")
            print(f"  ğŸ  ë§¤ë¬¼: {len(listings)}ê°œ")
            print(f"  ğŸ’° ê±°ë˜ê¸°ë¡: ìŠ¤í‚µë¨ (êµ­í† ë¶€ ë°ì´í„° ì‚¬ìš©)")
            print(f"  ğŸ“¸ ìŠ¤í¬ë¦°ìƒ·: {'âœ…' if screenshot_path else 'âŒ'}")
            print(f"  ğŸ—„ï¸ DB ì €ì¥: {'âœ…' if db_success else 'âŒ'}")
            
            return {
                'success': True,
                'method': 'enhanced_stealth',
                'complex_id': complex_id,
                'complex_name': basic_info.get('complexName', display_name),
                'data_summary': {
                    'listings_count': len(listings),
                    'transactions_count': len(transactions),
                    'has_screenshot': bool(screenshot_path),
                    'db_saved': db_success
                },
                'files': file_paths
            }
            
        except Exception as e:
            print(f"âŒ {display_name} ê°•í™” í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e),
                'method': 'enhanced_stealth',
                'complex_id': self.extract_complex_id_from_url(url)
            }
            
        finally:
            if self.browser:
                await self.browser.close()
                print("ğŸ”„ ë¸Œë¼ìš°ì € ì¢…ë£Œ")

# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜ë“¤
async def crawl_enhanced_single(url, name=None, headless=True):
    """ê°•í™”ëœ ë‹¨ì¼ ë‹¨ì§€ í¬ë¡¤ë§"""
    crawler = EnhancedNaverCrawler(headless=headless, stealth_mode=True)
    return await crawler.crawl_complex_enhanced(url, name)

async def test_enhanced_crawler():
    """ê°•í™”ëœ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    test_url = "https://new.land.naver.com/complexes/2592"
    
    print("ğŸš€ ê°•í™”ëœ ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("ğŸ”’ ìŠ¤í…”ìŠ¤ ëª¨ë“œ + ìš°íšŒ ê¸°ìˆ  ì ìš©")
    
    result = await crawl_enhanced_single(test_url, "ì •ë“ í•œì§„6ì°¨ (ê°•í™”ë²„ì „)", headless=False)
    
    if result['success']:
        print(f"\nâœ… ê°•í™” í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print(f"ğŸ“„ ìƒì„±ëœ íŒŒì¼:")
        for key, path in result['files'].items():
            if path:
                print(f"  {key}: {path}")
    else:
        print(f"âŒ ê°•í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result['error']}")
        
    return result

if __name__ == "__main__":
    asyncio.run(test_enhanced_crawler())
