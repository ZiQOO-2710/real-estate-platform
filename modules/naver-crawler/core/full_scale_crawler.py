"""
ë°œê²¬ëœ 4,873ê°œ ì „êµ­ ë‹¨ì§€ ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
"""

import asyncio
import logging
from datetime import datetime
from typing import List, Dict
import sqlite3
from pathlib import Path
import random
import time

from .enhanced_naver_crawler import crawl_enhanced_single
from database.simple_data_processor import process_json_file

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FullScaleCrawler:
    """ì „êµ­ 4,873ê°œ ë‹¨ì§€ ëŒ€ê·œëª¨ í¬ë¡¤ë§"""
    
    def __init__(self, max_concurrent=3, batch_size=20):
        self.max_concurrent = max_concurrent
        self.batch_size = batch_size
        self.progress_db = 'data/full_scale_progress.db'
        self.real_complexes_db = 'data/real_complexes.db'
        self.init_progress_db()
        
    def init_progress_db(self):
        """ì§„í–‰ë¥  ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        Path(self.progress_db).parent.mkdir(parents=True, exist_ok=True)
        
        conn = sqlite3.connect(self.progress_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS full_scale_progress (
                complex_id TEXT PRIMARY KEY,
                complex_name TEXT,
                status TEXT DEFAULT 'pending',
                listings_count INTEGER DEFAULT 0,
                error_message TEXT,
                attempt_count INTEGER DEFAULT 0,
                last_attempt TIMESTAMP,
                completed_at TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info(f"ì „êµ­ í¬ë¡¤ë§ ì§„í–‰ë¥  DB ì´ˆê¸°í™”: {self.progress_db}")
        
    def load_discovered_complexes(self) -> List[str]:
        """ë°œê²¬ëœ ëª¨ë“  ë‹¨ì§€ ID ë¡œë“œ"""
        try:
            conn = sqlite3.connect(self.real_complexes_db)
            cursor = conn.cursor()
            
            cursor.execute('SELECT complex_id FROM real_complexes ORDER BY complex_id')
            complex_ids = [row[0] for row in cursor.fetchall()]
            
            conn.close()
            
            logger.info(f"ë°œê²¬ëœ ë‹¨ì§€ ë¡œë“œ ì™„ë£Œ: {len(complex_ids)}ê°œ")
            return complex_ids
            
        except Exception as e:
            logger.error(f"ë°œê²¬ëœ ë‹¨ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
            
    def get_pending_complexes(self) -> List[str]:
        """ë¯¸ì²˜ë¦¬ ë‹¨ì§€ ëª©ë¡ ë°˜í™˜"""
        all_complexes = self.load_discovered_complexes()
        
        if not all_complexes:
            logger.error("ë°œê²¬ëœ ë‹¨ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
            
        # ì´ë¯¸ ì™„ë£Œëœ ë‹¨ì§€ í™•ì¸
        conn = sqlite3.connect(self.progress_db)
        cursor = conn.cursor()
        
        cursor.execute("SELECT complex_id FROM full_scale_progress WHERE status = 'completed'")
        completed_ids = {row[0] for row in cursor.fetchall()}
        
        conn.close()
        
        # ë¯¸ì²˜ë¦¬ ë‹¨ì§€ í•„í„°ë§
        pending = [cid for cid in all_complexes if cid not in completed_ids]
        
        logger.info(f"ë¯¸ì²˜ë¦¬ ë‹¨ì§€: {len(pending)}ê°œ, ì™„ë£Œ: {len(completed_ids)}ê°œ")
        return pending
        
    def update_progress(self, complex_id: str, status: str, listings_count: int = 0, 
                       error_message: str = None):
        """ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        conn = sqlite3.connect(self.progress_db)
        cursor = conn.cursor()
        
        now = datetime.now().isoformat()
        
        cursor.execute('''
            INSERT OR REPLACE INTO full_scale_progress
            (complex_id, status, listings_count, error_message, last_attempt, completed_at)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            complex_id,
            status,
            listings_count,
            error_message,
            now,
            now if status == 'completed' else None
        ))
        
        conn.commit()
        conn.close()
        
    async def crawl_single_complex(self, complex_id: str) -> bool:
        """ë‹¨ì¼ ë‹¨ì§€ í¬ë¡¤ë§"""
        try:
            url = f"https://new.land.naver.com/complexes/{complex_id}"
            complex_name = f"ë‹¨ì§€_{complex_id}"
            
            logger.info(f"ğŸ¢ í¬ë¡¤ë§ ì‹œì‘: {complex_name} ({complex_id})")
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            result = await crawl_enhanced_single(url, complex_name, headless=True)
            
            if result['success']:
                # DB ì €ì¥
                json_file = result['files']['json_file']
                db_success = process_json_file(json_file, {'database': 'data/naver_real_estate.db'})
                
                if db_success:
                    listings_count = result['data_summary']['listings_count']
                    self.update_progress(complex_id, 'completed', listings_count)
                    logger.info(f"âœ… ì„±ê³µ: {complex_name} - ë§¤ë¬¼ {listings_count}ê°œ")
                    return True
                else:
                    self.update_progress(complex_id, 'failed', error_message="DB ì €ì¥ ì‹¤íŒ¨")
                    logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {complex_name}")
                    return False
            else:
                error_msg = result.get('error', 'Unknown error')
                self.update_progress(complex_id, 'failed', error_message=error_msg)
                logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {complex_name} - {error_msg}")
                return False
                
        except Exception as e:
            self.update_progress(complex_id, 'failed', error_message=str(e))
            logger.error(f"âŒ ì˜ˆì™¸ ë°œìƒ: {complex_id} - {e}")
            return False
            
    async def crawl_batch(self, batch: List[str]):
        """ë°°ì¹˜ ë‹¨ìœ„ ë³‘ë ¬ í¬ë¡¤ë§"""
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def crawl_with_semaphore(complex_id):
            async with semaphore:
                success = await self.crawl_single_complex(complex_id)
                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ë” ê¸´ ê°„ê²©)
                await asyncio.sleep(random.uniform(8, 15))
                return success
                
        tasks = [crawl_with_semaphore(complex_id) for complex_id in batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        success_count = sum(1 for r in results if r is True)
        return success_count, len(results)
        
    async def run_full_scale_crawling(self):
        """ì „êµ­ ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("ğŸš€ ì „êµ­ 4,873ê°œ ë‹¨ì§€ ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹œì‘")
        
        # ë¯¸ì²˜ë¦¬ ë‹¨ì§€ ë¡œë“œ
        pending_complexes = self.get_pending_complexes()
        
        if not pending_complexes:
            logger.info("ğŸ‰ ëª¨ë“  ë‹¨ì§€ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return
            
        logger.info(f"ğŸ“Š í¬ë¡¤ë§ ëŒ€ìƒ: {len(pending_complexes)}ê°œ ë‹¨ì§€")
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        total_batches = (len(pending_complexes) + self.batch_size - 1) // self.batch_size
        start_time = time.time()
        total_success = 0
        total_failed = 0
        
        for i in range(0, len(pending_complexes), self.batch_size):
            batch_num = i // self.batch_size + 1
            batch = pending_complexes[i:i + self.batch_size]
            
            logger.info(f"ğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ ë‹¨ì§€)")
            
            try:
                success_count, total_count = await self.crawl_batch(batch)
                failed_count = total_count - success_count
                
                total_success += success_count
                total_failed += failed_count
                
                # ì§„í–‰ë¥  ë¦¬í¬íŠ¸
                elapsed_time = time.time() - start_time
                processed_count = total_success + total_failed
                remaining_count = len(pending_complexes) - processed_count
                
                if processed_count > 0:
                    avg_time_per_complex = elapsed_time / processed_count
                    estimated_remaining_time = avg_time_per_complex * remaining_count
                    
                    logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {processed_count}/{len(pending_complexes)} ({processed_count/len(pending_complexes)*100:.1f}%)")
                    logger.info(f"ğŸ“Š ì„±ê³µ: {total_success}, ì‹¤íŒ¨: {total_failed}")
                    logger.info(f"â° ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining_time/3600:.1f}ì‹œê°„")
                    
                    # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
                    self.print_current_stats()
                    
                # ë°°ì¹˜ ê°„ ëŒ€ê¸° (ë” ê¸´ ê°„ê²©)
                if i + self.batch_size < len(pending_complexes):
                    wait_time = random.uniform(30, 60)
                    logger.info(f"â³ ë‹¤ìŒ ë°°ì¹˜ê¹Œì§€ {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
                    await asyncio.sleep(wait_time)
                    
            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                total_failed += len(batch)
                
        # ìµœì¢… ê²°ê³¼
        total_time = time.time() - start_time
        success_rate = (total_success / (total_success + total_failed)) * 100 if (total_success + total_failed) > 0 else 0
        
        logger.info(f"\\nğŸ‰ ì „êµ­ ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"â° ì´ ì†Œìš”ì‹œê°„: {total_time/3600:.2f}ì‹œê°„")
        logger.info(f"ğŸ“Š ì´ ë‹¨ì§€ìˆ˜: {len(pending_complexes)}ê°œ")
        logger.info(f"âœ… ì„±ê³µ: {total_success}ê°œ")
        logger.info(f"âŒ ì‹¤íŒ¨: {total_failed}ê°œ")
        logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        return {
            'total_complexes': len(pending_complexes),
            'successful': total_success,
            'failed': total_failed,
            'total_time': total_time,
            'success_rate': success_rate
        }
        
    def print_current_stats(self):
        """í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¶œë ¥"""
        try:
            import sys
            sys.path.append('.')
            from database.simple_data_processor import get_database_statistics
            
            stats = get_database_statistics('data/naver_real_estate.db')
            logger.info(f"ğŸ  í˜„ì¬ DB: ë‹¨ì§€ {stats['complexes_count']}ê°œ, ë§¤ë¬¼ {stats['listings_count']:,}ê°œ")
            
        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")

# ì‹¤í–‰ í•¨ìˆ˜
async def run_full_scale_crawling():
    """ì „êµ­ ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹¤í–‰"""
    crawler = FullScaleCrawler(max_concurrent=2, batch_size=10)
    result = await crawler.run_full_scale_crawling()
    return result

if __name__ == "__main__":
    asyncio.run(run_full_scale_crawling())