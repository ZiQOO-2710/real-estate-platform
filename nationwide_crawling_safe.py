#!/usr/bin/env python3
"""
ì „êµ­ ì•„íŒŒíŠ¸ ì•ˆì „ í¬ë¡¤ë§ (13ì‹œê°„ ì—¬ìœ )
- ì¶©ë¶„í•œ ëŒ€ê¸° ì‹œê°„ìœ¼ë¡œ ì°¨ë‹¨ ë°©ì§€
- VPN ë°±ì—… ì‹œìŠ¤í…œ í¬í•¨
- ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ì €ì¥
"""

import asyncio
import sys
import os
import time
import json
from datetime import datetime, timedelta
from pathlib import Path

sys.path.append('/home/ksj27/projects/real-estate-platform/modules/naver-crawler')

from core.crawler import NaverRealEstateCrawler
from setup_supabase_complete import SupabaseCompleteSetup

class SafeNationwideCrawler:
    def __init__(self):
        self.setup = SupabaseCompleteSetup(
            'https://heatmxifhwxppprdzaqf.supabase.co',
            'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImhlYXRteGlmaHd4cHBwcmR6YXFmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTI0NzQ3OTMsImV4cCI6MjA2ODA1MDc5M30.NRcHrm5Y7ooHGkCxrD2QdStea-KnyVBIUCTpFB9x89o'
        )
        self.start_time = datetime.now()
        self.deadline = datetime.now() + timedelta(hours=13)  # ë‚´ì¼ 8ì‹œê¹Œì§€
        self.progress_file = "crawling_progress.json"
        self.total_saved = 0
        
        # ì „êµ­ ì§€ì—­ ì •ì˜ (ì„œìš¸ ì œì™¸ - ì´ë¯¸ ì™„ë£Œ)
        self.regions = {
            "ë¶€ì‚°": {
                "í•´ìš´ëŒ€êµ¬": "2644010100", "ìˆ˜ì˜êµ¬": "2644010200", "ë‚¨êµ¬": "2644010300",
                "ë™êµ¬": "2644010400", "ì„œêµ¬": "2644010500", "ì¤‘êµ¬": "2644010600",
                "ì˜ë„êµ¬": "2644010700", "ë¶€ì‚°ì§„êµ¬": "2644010800", "ë™ë˜êµ¬": "2644010900",
                "ë¶êµ¬": "2644011000", "ê¸ˆì •êµ¬": "2644011100", "ê°•ì„œêµ¬": "2644011200",
                "ì—°ì œêµ¬": "2644011300", "ì‚¬ìƒêµ¬": "2644011400", "ì‚¬í•˜êµ¬": "2644011500",
                "ê¸°ì¥êµ°": "2644011600"
            },
            "ì¸ì²œ": {
                "ì¤‘êµ¬": "2811010100", "ë™êµ¬": "2811010200", "ë¯¸ì¶”í™€êµ¬": "2811010300",
                "ì—°ìˆ˜êµ¬": "2811010400", "ë‚¨ë™êµ¬": "2811010500", "ë¶€í‰êµ¬": "2811010600",
                "ê³„ì–‘êµ¬": "2811010700", "ì„œêµ¬": "2811010800", "ê°•í™”êµ°": "2811010900",
                "ì˜¹ì§„êµ°": "2811011000"
            },
            "ëŒ€êµ¬": {
                "ì¤‘êµ¬": "2711010100", "ë™êµ¬": "2711010200", "ì„œêµ¬": "2711010300",
                "ë‚¨êµ¬": "2711010400", "ë¶êµ¬": "2711010500", "ìˆ˜ì„±êµ¬": "2711010600",
                "ë‹¬ì„œêµ¬": "2711010700", "ë‹¬ì„±êµ°": "2711010800"
            },
            "ê´‘ì£¼": {
                "ë™êµ¬": "2911010100", "ì„œêµ¬": "2911010200", "ë‚¨êµ¬": "2911010300",
                "ë¶êµ¬": "2911010400", "ê´‘ì‚°êµ¬": "2911010500"
            },
            "ëŒ€ì „": {
                "ë™êµ¬": "3011010100", "ì¤‘êµ¬": "3011010200", "ì„œêµ¬": "3011010300",
                "ìœ ì„±êµ¬": "3011010400", "ëŒ€ë•êµ¬": "3011010500"
            },
            "ìš¸ì‚°": {
                "ì¤‘êµ¬": "3111010100", "ë‚¨êµ¬": "3111010200", "ë™êµ¬": "3111010300",
                "ë¶êµ¬": "3111010400", "ìš¸ì£¼êµ°": "3111010500"
            }
        }
        
        # ì„œìš¸ ë‚˜ë¨¸ì§€ êµ¬ë“¤ (ê°•ë‚¨, ê°•ë™, ê°•ë¶ ì œì™¸)
        self.seoul_remaining = {
            "ì„œìš¸": {
                "ê°•ì„œêµ¬": "1168010800", "ê´€ì•…êµ¬": "1168010900", "ê´‘ì§„êµ¬": "1168011000",
                "êµ¬ë¡œêµ¬": "1168011100", "ê¸ˆì²œêµ¬": "1168011200", "ë…¸ì›êµ¬": "1168011300",
                "ë„ë´‰êµ¬": "1168011400", "ë™ëŒ€ë¬¸êµ¬": "1168011500", "ë™ì‘êµ¬": "1168011600",
                "ë§ˆí¬êµ¬": "1168011700", "ì„œëŒ€ë¬¸êµ¬": "1168011800", "ì„œì´ˆêµ¬": "1168011900",
                "ì„±ë™êµ¬": "1168012000", "ì„±ë¶êµ¬": "1168012100", "ì†¡íŒŒêµ¬": "1168012200",
                "ì–‘ì²œêµ¬": "1168012300", "ì˜ë“±í¬êµ¬": "1168012400", "ìš©ì‚°êµ¬": "1168012500",
                "ì€í‰êµ¬": "1168012600", "ì¢…ë¡œêµ¬": "1168012700", "ì¤‘êµ¬": "1168012800",
                "ì¤‘ë‘êµ¬": "1168012900"
            }
        }
    
    def load_progress(self):
        """ì§„í–‰ ìƒí™© ë¡œë“œ"""
        if Path(self.progress_file).exists():
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"completed_regions": [], "last_update": None}
    
    def save_progress(self, progress):
        """ì§„í–‰ ìƒí™© ì €ì¥"""
        progress["last_update"] = datetime.now().isoformat()
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    
    def convert_apartment_to_db_format(self, apt, region_name):
        """ì•„íŒŒíŠ¸ ë°ì´í„°ë¥¼ DB í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        avg_price = None
        if apt.min_deal_price and apt.max_deal_price:
            avg_price = (apt.min_deal_price + apt.max_deal_price) // 2
        elif apt.min_deal_price:
            avg_price = apt.min_deal_price
        elif apt.max_deal_price:
            avg_price = apt.max_deal_price
        
        construction_year = None
        if apt.completion_year_month:
            try:
                year_str = str(apt.completion_year_month)[:4]
                construction_year = int(year_str)
            except:
                pass
        
        city, gu = region_name.split('_') if '_' in region_name else (region_name, '')
        
        return {
            'complex_id': f"naver_{apt.complex_no}" if apt.complex_no else f"naver_unknown_{id(apt)}",
            'complex_name': apt.complex_name or 'ì´ë¦„ì—†ìŒ',
            'address_road': apt.address or '',
            'city': city,
            'gu': gu,
            'latitude': apt.latitude,
            'longitude': apt.longitude,
            'total_units': apt.total_household_count,
            'construction_year': construction_year,
            'last_transaction_price': avg_price,
            'source_url': f"https://new.land.naver.com/complexes/{apt.complex_no}" if apt.complex_no else ""
        }
    
    async def crawl_and_save_region(self, city, district):
        """ì§€ì—­ í¬ë¡¤ë§ í›„ DB ì €ì¥ (ì•ˆì „ ëª¨ë“œ)"""
        region_key = f"{city}_{district}"
        
        print(f"ğŸ—ï¸ {city} {district} í¬ë¡¤ë§ ì‹œì‘...")
        
        async with NaverRealEstateCrawler() as crawler:
            try:
                # í¬ë¡¤ë§ ì‹¤í–‰
                apartments = await crawler.get_apartments(city, district, 'ë§¤ë§¤')
                print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(apartments)}ê°œ ì•„íŒŒíŠ¸")
                
                if not apartments:
                    print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return 0
                
                # DB ì €ì¥
                saved_count = 0
                
                for apt in apartments:
                    try:
                        db_data = self.convert_apartment_to_db_format(apt, region_key)
                        
                        result = self.setup.supabase.table('apartment_complexes')\
                            .upsert(db_data, on_conflict='complex_id')\
                            .execute()
                        
                        saved_count += 1
                        
                        if saved_count % 100 == 0:
                            print(f"ğŸ“¤ ì§„í–‰ë¥ : {saved_count}/{len(apartments)}")
                            
                    except Exception as e:
                        print(f"âš ï¸ ê°œë³„ ì €ì¥ ì˜¤ë¥˜: {e}")
                        continue
                
                print(f"ğŸ‰ DB ì €ì¥ ì™„ë£Œ: {saved_count}/{len(apartments)}ê°œ")
                return saved_count
                
            except Exception as e:
                print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                return 0
    
    def calculate_delay(self, remaining_regions, time_left_hours):
        """ë‚¨ì€ ì‹œê°„ì— ë”°ë¥¸ ìµœì  ëŒ€ê¸° ì‹œê°„ ê³„ì‚°"""
        if remaining_regions == 0:
            return 10  # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„
        
        # ì‹œê°„ë‹¹ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì§€ì—­ ìˆ˜
        regions_per_hour = remaining_regions / time_left_hours
        
        if regions_per_hour <= 2:
            return 30  # ì—¬ìœ ë¡œì›€ - 30ì´ˆ ëŒ€ê¸°
        elif regions_per_hour <= 4:
            return 15  # ë³´í†µ - 15ì´ˆ ëŒ€ê¸°  
        elif regions_per_hour <= 8:
            return 8   # ë¹ ë¦„ - 8ì´ˆ ëŒ€ê¸°
        else:
            return 5   # ìµœì†Œ - 5ì´ˆ ëŒ€ê¸°
    
    async def run_safe_crawling(self):
        """ì•ˆì „í•œ ì „êµ­ í¬ë¡¤ë§ ì‹¤í–‰"""
        print("ğŸš€ ì „êµ­ ì•„íŒŒíŠ¸ ì•ˆì „ í¬ë¡¤ë§ ì‹œì‘!")
        print(f"â° ì‹œì‘ ì‹œê°„: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â° ë§ˆê° ì‹œê°„: {self.deadline.strftime('%Y-%m-%d %H:%M:%S')}")
        print("ğŸ”’ VPN ë°±ì—… ì‹œìŠ¤í…œ í™œì„±í™” ìƒíƒœ")
        print("=" * 60)
        
        # ì§„í–‰ ìƒí™© ë¡œë“œ
        progress = self.load_progress()
        completed_regions = set(progress.get("completed_regions", []))
        
        # ì „ì²´ ì§€ì—­ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        all_regions = []
        
        # ì„œìš¸ ë‚˜ë¨¸ì§€ êµ¬ë“¤ ì¶”ê°€
        for city, districts in self.seoul_remaining.items():
            for district in districts.keys():
                region_key = f"{city}_{district}"
                if region_key not in completed_regions:
                    all_regions.append((city, district))
        
        # ë‹¤ë¥¸ ë„ì‹œë“¤ ì¶”ê°€
        for city, districts in self.regions.items():
            for district in districts.keys():
                region_key = f"{city}_{district}"
                if region_key not in completed_regions:
                    all_regions.append((city, district))
        
        print(f"ğŸ“‹ ì²˜ë¦¬í•  ì§€ì—­: {len(all_regions)}ê°œ")
        print(f"ğŸ“‹ ì™„ë£Œëœ ì§€ì—­: {len(completed_regions)}ê°œ")
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        for i, (city, district) in enumerate(all_regions, 1):
            # ì‹œê°„ í™•ì¸
            current_time = datetime.now()
            time_left = self.deadline - current_time
            
            if time_left.total_seconds() <= 0:
                print("â° ë§ˆê° ì‹œê°„ ë„ë‹¬!")
                break
            
            time_left_hours = time_left.total_seconds() / 3600
            remaining_regions = len(all_regions) - i + 1
            
            print(f"\n[{i}/{len(all_regions)}] {city} {district}")
            print(f"â° ë‚¨ì€ ì‹œê°„: {time_left_hours:.1f}ì‹œê°„ | ë‚¨ì€ ì§€ì—­: {remaining_regions}ê°œ")
            
            try:
                saved = await self.crawl_and_save_region(city, district)
                self.total_saved += saved
                
                # ì§„í–‰ ìƒí™© ì €ì¥
                region_key = f"{city}_{district}"
                completed_regions.add(region_key)
                progress["completed_regions"] = list(completed_regions)
                self.save_progress(progress)
                
                # ë™ì  ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
                delay_time = self.calculate_delay(remaining_regions, time_left_hours)
                print(f"â±ï¸ ì•ˆì „ ëŒ€ê¸°: {delay_time}ì´ˆ...")
                await asyncio.sleep(delay_time)
                
                # ì£¼ê¸°ì  ìƒíƒœ ì¶œë ¥
                if i % 5 == 0:
                    print(f"\nğŸ“Š í˜„ì¬ê¹Œì§€ ì§„í–‰ ìƒí™©:")
                    print(f"   ì²˜ë¦¬ ì™„ë£Œ: {i}/{len(all_regions)} ì§€ì—­")
                    print(f"   ì´ ì €ì¥: {self.total_saved}ê°œ ì•„íŒŒíŠ¸")
                    print(f"   ì§„í–‰ë¥ : {(i/len(all_regions)*100):.1f}%")
                
            except Exception as e:
                print(f"âŒ {city} {district} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        # ìµœì¢… ê²°ê³¼
        end_time = datetime.now()
        duration = end_time - self.start_time
        
        print(f"\nğŸ‰ ì „êµ­ í¬ë¡¤ë§ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(f"   ì´ ì†Œìš” ì‹œê°„: {duration}")
        print(f"   ì²˜ë¦¬ëœ ì§€ì—­: {len(completed_regions)}ê°œ")
        print(f"   ì´ ì €ì¥ ì•„íŒŒíŠ¸: {self.total_saved}ê°œ")
        print(f"   ì¢…ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")

async def main():
    crawler = SafeNationwideCrawler()
    await crawler.run_safe_crawling()

if __name__ == "__main__":
    asyncio.run(main())