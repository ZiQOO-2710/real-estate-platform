#!/usr/bin/env python3
"""
í™•ì‹¤í•œ ì „êµ­ ì•„íŒŒíŠ¸ í¬ë¡¤ëŸ¬ v3.0
- ë„¤ì´ë²„ ë¶€ë™ì‚° API ì§ì ‘ í˜¸ì¶œ
- ë‹¨ê³„ë³„ ê²€ì¦ ë° í™•ì¸
- ëŠë ¤ë„ í™•ì‹¤í•˜ê²Œ ìˆ˜ì§‘
"""

import requests
import sqlite3
import json
import time
import random
import logging
from datetime import datetime
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
import asyncio
import aiohttp

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('reliable_crawling.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ApartmentUnit:
    """ì•„íŒŒíŠ¸ ë§¤ë¬¼ ì •ë³´"""
    complex_id: str
    complex_name: str
    deal_type: str  # ë§¤ë§¤, ì „ì„¸, ì›”ì„¸
    price: str
    area: str
    floor: str
    direction: str
    region: str
    address: str
    crawled_at: str

class ReliableNationwideCrawler:
    """í™•ì‹¤í•œ ì „êµ­ ì•„íŒŒíŠ¸ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.session = requests.Session()
        self.db_path = "reliable_real_estate.db"
        self.setup_headers()
        self.init_database()
        
        # ì „êµ­ ì§€ì—­ ì½”ë“œ (í™•ì‹¤í•œ ì§€ì—­ì½”ë“œ)
        self.regions = {
            '11': {'name': 'ì„œìš¸íŠ¹ë³„ì‹œ', 'districts': ['11110', '11140', '11170', '11200', '11215', '11230', '11260', '11290', '11305', '11320', '11350', '11380', '11410', '11440', '11470', '11500', '11530', '11545', '11560', '11590', '11620', '11650', '11680', '11710', '11740']},
            '26': {'name': 'ë¶€ì‚°ê´‘ì—­ì‹œ', 'districts': ['26110', '26140', '26170', '26200', '26230', '26260', '26290', '26320', '26350', '26380', '26410', '26440', '26470', '26500', '26530', '26710']},
            '27': {'name': 'ëŒ€êµ¬ê´‘ì—­ì‹œ', 'districts': ['27110', '27140', '27170', '27200', '27230', '27260', '27290', '27710']},
            '28': {'name': 'ì¸ì²œê´‘ì—­ì‹œ', 'districts': ['28110', '28140', '28177', '28185', '28200', '28237', '28245', '28260', '28710', '28720']},
            '29': {'name': 'ê´‘ì£¼ê´‘ì—­ì‹œ', 'districts': ['29110', '29140', '29155', '29170', '29200']},
            '30': {'name': 'ëŒ€ì „ê´‘ì—­ì‹œ', 'districts': ['30110', '30140', '30170', '30200', '30230']},
            '31': {'name': 'ìš¸ì‚°ê´‘ì—­ì‹œ', 'districts': ['31110', '31140', '31170', '31200', '31710']},
            '41': {'name': 'ê²½ê¸°ë„', 'districts': ['41110', '41130', '41150', '41170', '41190', '41210', '41220', '41250', '41270', '41280', '41290', '41310', '41360', '41370', '41390', '41410', '41430', '41450', '41460', '41480', '41500', '41550', '41570', '41590', '41610', '41630', '41650', '41670', '41720', '41730', '41750', '41770', '41800', '41820', '41830']},
        }
        
        # í†µê³„
        self.stats = {
            'total_complexes': 0,
            'total_listings': 0,
            'regions_completed': 0,
            'start_time': datetime.now()
        }
    
    def setup_headers(self):
        """í—¤ë” ì„¤ì •"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'Referer': 'https://new.land.naver.com/complexes',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"'
        })
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ì•„íŒŒíŠ¸ ë‹¨ì§€ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS apartment_complexes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                complex_id TEXT UNIQUE NOT NULL,
                complex_name TEXT,
                address TEXT,
                region_code TEXT,
                region_name TEXT,
                district_code TEXT,
                latitude REAL,
                longitude REAL,
                total_households INTEGER,
                completion_year TEXT,
                min_area REAL,
                max_area REAL,
                min_price INTEGER,
                max_price INTEGER,
                api_data TEXT,
                crawled_at TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # ë§¤ë¬¼ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS property_listings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                complex_id TEXT,
                complex_name TEXT,
                deal_type TEXT,
                price TEXT,
                area TEXT,
                floor TEXT,
                direction TEXT,
                region TEXT,
                address TEXT,
                listing_data TEXT,
                crawled_at TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # í¬ë¡¤ë§ ì§„í–‰ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS crawling_progress (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                region_code TEXT,
                region_name TEXT,
                district_code TEXT,
                status TEXT,
                complexes_found INTEGER DEFAULT 0,
                listings_found INTEGER DEFAULT 0,
                error_message TEXT,
                started_at TEXT,
                completed_at TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.commit()
        conn.close()
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_complex_list_by_district(self, region_code: str, district_code: str) -> List[Dict[str, Any]]:
        """ì§€ì—­ë³„ ì•„íŒŒíŠ¸ ë‹¨ì§€ ëª©ë¡ API í˜¸ì¶œ"""
        logger.info(f"ì§€ì—­ {region_code}-{district_code} ì•„íŒŒíŠ¸ ë‹¨ì§€ ëª©ë¡ ì¡°íšŒ ì¤‘...")
        
        # ë„¤ì´ë²„ ë¶€ë™ì‚° API URL 
        url = "https://new.land.naver.com/api/complexes"
        
        params = {
            'cortarNo': district_code,
            'ptpNo': 'APT',  # ì•„íŒŒíŠ¸
            'rletTpCd': 'A01',  # ë§¤ë§¤
            'tradTpCd': 'A1',
            'z': '12',
            'lat': '37.5665',
            'lon': '126.9780',
            'btm': '37.4',
            'lft': '126.7',
            'top': '37.7',
            'rgt': '127.2'
        }
        
        try:
            response = self.session.get(url, params=params, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                
                # ì‘ë‹µ êµ¬ì¡° í™•ì¸
                if 'complexList' in data:
                    complexes = data['complexList']
                elif isinstance(data, list):
                    complexes = data
                else:
                    logger.warning(f"ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì‘ë‹µ êµ¬ì¡°: {list(data.keys()) if isinstance(data, dict) else type(data)}")
                    complexes = []
                
                logger.info(f"ì§€ì—­ {district_code}: {len(complexes)}ê°œ ë‹¨ì§€ ë°œê²¬")
                return complexes
                
            else:
                logger.error(f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"ì§€ì—­ {district_code} API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return []
        
        finally:
            # ìš”ì²­ ê°„ê²© (ì•ˆì „í•œ í¬ë¡¤ë§)
            time.sleep(random.uniform(2, 4))
    
    def get_complex_detail(self, complex_id: str) -> Dict[str, Any]:
        """ì•„íŒŒíŠ¸ ë‹¨ì§€ ìƒì„¸ ì •ë³´ API í˜¸ì¶œ"""
        logger.info(f"ë‹¨ì§€ {complex_id} ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘...")
        
        url = f"https://new.land.naver.com/api/complexes/{complex_id}"
        
        try:
            response = self.session.get(url, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"ë‹¨ì§€ {complex_id} ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
                return data
            else:
                logger.error(f"ë‹¨ì§€ {complex_id} ìƒì„¸ ì •ë³´ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                return {}
                
        except Exception as e:
            logger.error(f"ë‹¨ì§€ {complex_id} ìƒì„¸ ì •ë³´ ì˜¤ë¥˜: {e}")
            return {}
        
        finally:
            time.sleep(random.uniform(1, 2))
    
    def get_complex_listings(self, complex_id: str) -> List[Dict[str, Any]]:
        """ì•„íŒŒíŠ¸ ë‹¨ì§€ ë§¤ë¬¼ ëª©ë¡ API í˜¸ì¶œ"""
        logger.info(f"ë‹¨ì§€ {complex_id} ë§¤ë¬¼ ëª©ë¡ ì¡°íšŒ ì¤‘...")
        
        all_listings = []
        
        # ê±°ë˜ ìœ í˜•ë³„ë¡œ ì¡°íšŒ (ë§¤ë§¤, ì „ì„¸, ì›”ì„¸)
        trade_types = [
            {'code': 'A1', 'name': 'ë§¤ë§¤'},
            {'code': 'B1', 'name': 'ì „ì„¸'},
            {'code': 'B2', 'name': 'ì›”ì„¸'}
        ]
        
        for trade_type in trade_types:
            logger.info(f"  {trade_type['name']} ë§¤ë¬¼ ì¡°íšŒ ì¤‘...")
            
            url = f"https://new.land.naver.com/api/complexes/{complex_id}/prices"
            
            params = {
                'tradTpCd': trade_type['code'],
                'tag': '{}',
                'rentPrc': '0',
                'spc': '{}',
                'dp': '0',
                'year': '0',
                'floor': '{}',
                'page': '1',
                'size': '200'
            }
            
            try:
                response = self.session.get(url, params=params, timeout=30)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # ë§¤ë¬¼ ë°ì´í„° ì¶”ì¶œ
                    if 'list' in data:
                        listings = data['list']
                        for listing in listings:
                            listing['deal_type'] = trade_type['name']
                        all_listings.extend(listings)
                        logger.info(f"    {trade_type['name']}: {len(listings)}ê°œ ë§¤ë¬¼")
                    
                else:
                    logger.warning(f"  {trade_type['name']} ë§¤ë¬¼ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                    
            except Exception as e:
                logger.error(f"  {trade_type['name']} ë§¤ë¬¼ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            
            # ê±°ë˜ìœ í˜•ë³„ ë”œë ˆì´
            time.sleep(random.uniform(1, 2))
        
        logger.info(f"ë‹¨ì§€ {complex_id} ì´ ë§¤ë¬¼: {len(all_listings)}ê°œ")
        return all_listings
    
    def save_complex(self, complex_data: Dict[str, Any], region_code: str, district_code: str):
        """ì•„íŒŒíŠ¸ ë‹¨ì§€ ì •ë³´ ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            complex_id = str(complex_data.get('complexNo', ''))
            complex_name = complex_data.get('complexName', '')
            address = complex_data.get('roadAddress', complex_data.get('address', ''))
            region_name = self.regions.get(region_code, {}).get('name', '')
            
            cursor.execute("""
                INSERT OR REPLACE INTO apartment_complexes 
                (complex_id, complex_name, address, region_code, region_name, district_code,
                 latitude, longitude, total_households, completion_year, min_area, max_area,
                 min_price, max_price, api_data, crawled_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                complex_id, complex_name, address, region_code, region_name, district_code,
                complex_data.get('lat'), complex_data.get('lng'),
                complex_data.get('householdCnt'), complex_data.get('useApproveDate'),
                complex_data.get('minSupplyArea'), complex_data.get('maxSupplyArea'),
                complex_data.get('dealPrice'), complex_data.get('rentPrice'),
                json.dumps(complex_data, ensure_ascii=False),
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
            self.stats['total_complexes'] += 1
            logger.info(f"ë‹¨ì§€ ì €ì¥ ì™„ë£Œ: {complex_name} ({complex_id})")
            
        except Exception as e:
            logger.error(f"ë‹¨ì§€ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def save_listings(self, complex_id: str, complex_name: str, listings: List[Dict[str, Any]], region: str):
        """ë§¤ë¬¼ ì •ë³´ ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for listing in listings:
                cursor.execute("""
                    INSERT INTO property_listings 
                    (complex_id, complex_name, deal_type, price, area, floor, direction,
                     region, address, listing_data, crawled_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    complex_id, complex_name, listing.get('deal_type', ''),
                    listing.get('dealPrice', listing.get('rentPrice', '')),
                    listing.get('supplyArea', ''), listing.get('floor', ''),
                    listing.get('direction', ''), region, '',
                    json.dumps(listing, ensure_ascii=False),
                    datetime.now().isoformat()
                ))
            
            conn.commit()
            conn.close()
            
            self.stats['total_listings'] += len(listings)
            logger.info(f"ë§¤ë¬¼ {len(listings)}ê°œ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë§¤ë¬¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def record_progress(self, region_code: str, district_code: str, status: str, 
                       complexes_found: int = 0, listings_found: int = 0, error_msg: str = None):
        """ì§„í–‰ ìƒí™© ê¸°ë¡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            region_name = self.regions.get(region_code, {}).get('name', '')
            
            cursor.execute("""
                INSERT OR REPLACE INTO crawling_progress 
                (region_code, region_name, district_code, status, complexes_found, listings_found,
                 error_message, started_at, completed_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                region_code, region_name, district_code, status, complexes_found, listings_found,
                error_msg, datetime.now().isoformat(),
                datetime.now().isoformat() if status == 'ì™„ë£Œ' else None
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ì§„í–‰ ìƒí™© ê¸°ë¡ ì˜¤ë¥˜: {e}")
    
    def crawl_district(self, region_code: str, district_code: str):
        """ì§€ì—­ë³„ í¬ë¡¤ë§"""
        region_name = self.regions.get(region_code, {}).get('name', '')
        logger.info(f"\n=== {region_name} {district_code} í¬ë¡¤ë§ ì‹œì‘ ===")
        
        try:
            self.record_progress(region_code, district_code, 'ì‹œì‘')
            
            # 1ë‹¨ê³„: ë‹¨ì§€ ëª©ë¡ ì¡°íšŒ
            complexes = self.get_complex_list_by_district(region_code, district_code)
            
            if not complexes:
                logger.warning(f"ì§€ì—­ {district_code}: ì•„íŒŒíŠ¸ ë‹¨ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                self.record_progress(region_code, district_code, 'ì™„ë£Œ', 0, 0)
                return
            
            logger.info(f"ì§€ì—­ {district_code}: {len(complexes)}ê°œ ë‹¨ì§€ ë°œê²¬")
            
            total_listings = 0
            
            # 2ë‹¨ê³„: ê° ë‹¨ì§€ë³„ ìƒì„¸ ì •ë³´ ë° ë§¤ë¬¼ ìˆ˜ì§‘
            for i, complex_basic in enumerate(complexes, 1):
                try:
                    complex_id = str(complex_basic.get('complexNo', ''))
                    complex_name = complex_basic.get('complexName', f'ë‹¨ì§€_{complex_id}')
                    
                    logger.info(f"  [{i}/{len(complexes)}] {complex_name} ì²˜ë¦¬ ì¤‘...")
                    
                    # ë‹¨ì§€ ìƒì„¸ ì •ë³´ ì¡°íšŒ
                    complex_detail = self.get_complex_detail(complex_id)
                    
                    # ê¸°ë³¸ ì •ë³´ì™€ ìƒì„¸ ì •ë³´ í•©ë³‘
                    combined_data = {**complex_basic, **complex_detail}
                    
                    # ë‹¨ì§€ ì •ë³´ ì €ì¥
                    self.save_complex(combined_data, region_code, district_code)
                    
                    # ë§¤ë¬¼ ì •ë³´ ì¡°íšŒ ë° ì €ì¥
                    listings = self.get_complex_listings(complex_id)
                    if listings:
                        self.save_listings(complex_id, complex_name, listings, region_name)
                        total_listings += len(listings)
                    
                    logger.info(f"    ì™„ë£Œ: ë‹¨ì§€ì •ë³´ ì €ì¥, ë§¤ë¬¼ {len(listings)}ê°œ")
                    
                    # ë‹¨ì§€ë³„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                    time.sleep(random.uniform(3, 6))
                    
                except Exception as e:
                    logger.error(f"    ë‹¨ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            # ì™„ë£Œ ê¸°ë¡
            self.record_progress(region_code, district_code, 'ì™„ë£Œ', len(complexes), total_listings)
            logger.info(f"=== {region_name} {district_code} ì™„ë£Œ: ë‹¨ì§€ {len(complexes)}ê°œ, ë§¤ë¬¼ {total_listings}ê°œ ===")
            
        except Exception as e:
            logger.error(f"ì§€ì—­ {district_code} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            self.record_progress(region_code, district_code, 'ì˜¤ë¥˜', 0, 0, str(e))
    
    def start_comprehensive_crawling(self):
        """ì „êµ­ ì¢…í•© í¬ë¡¤ë§ ì‹œì‘"""
        logger.info("ğŸš€ í™•ì‹¤í•œ ì „êµ­ ì•„íŒŒíŠ¸ í¬ë¡¤ë§ ì‹œì‘!")
        logger.info(f"ğŸ“Š ëŒ€ìƒ: {len(self.regions)}ê°œ ì‹œë„")
        
        # ì„œìš¸ë¶€í„° ì‹œì‘ (í…ŒìŠ¤íŠ¸)
        for region_code, region_info in list(self.regions.items())[:1]:  # ì¼ë‹¨ ì„œìš¸ë§Œ
            region_name = region_info['name']
            districts = region_info['districts']
            
            logger.info(f"\nğŸ™ï¸ {region_name} í¬ë¡¤ë§ ì‹œì‘ ({len(districts)}ê°œ êµ¬/êµ°)")
            
            for district_code in districts[:3]:  # ì¼ë‹¨ 3ê°œ êµ¬ë§Œ í…ŒìŠ¤íŠ¸
                self.crawl_district(region_code, district_code)
                
                # ì§€ì—­ê°„ ë”œë ˆì´
                time.sleep(random.uniform(5, 10))
            
            self.stats['regions_completed'] += 1
            logger.info(f"âœ… {region_name} ì™„ë£Œ")
        
        # ìµœì¢… í†µê³„
        self.print_final_stats()
    
    def print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        end_time = datetime.now()
        duration = end_time - self.stats['start_time']
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ í™•ì‹¤í•œ ì „êµ­ ì•„íŒŒíŠ¸ í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info("="*60)
        logger.info(f"ğŸ“Š ìµœì¢… í†µê³„:")
        logger.info(f"  ğŸ¢ ìˆ˜ì§‘ëœ ë‹¨ì§€: {self.stats['total_complexes']:,}ê°œ")
        logger.info(f"  ğŸ  ìˆ˜ì§‘ëœ ë§¤ë¬¼: {self.stats['total_listings']:,}ê°œ")
        logger.info(f"  ğŸ™ï¸ ì™„ë£Œëœ ì§€ì—­: {self.stats['regions_completed']}ê°œ")
        logger.info(f"  â±ï¸ ì†Œìš” ì‹œê°„: {duration}")
        logger.info(f"  ğŸ“ˆ ì‹œê°„ë‹¹ í‰ê· : ë‹¨ì§€ {self.stats['total_complexes']/(duration.total_seconds()/3600):.1f}ê°œ/ì‹œê°„")
        logger.info("="*60)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("í™•ì‹¤í•œ ì „êµ­ ì•„íŒŒíŠ¸ í¬ë¡¤ëŸ¬ v3.0")
    print("="*40)
    print("- ë„¤ì´ë²„ ë¶€ë™ì‚° API ì§ì ‘ í˜¸ì¶œ")
    print("- ë‹¨ê³„ë³„ ê²€ì¦ ë° í™•ì¸")
    print("- ëŠë ¤ë„ í™•ì‹¤í•˜ê²Œ ìˆ˜ì§‘")
    print("="*40)
    
    crawler = ReliableNationwideCrawler()
    
    try:
        crawler.start_comprehensive_crawling()
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()