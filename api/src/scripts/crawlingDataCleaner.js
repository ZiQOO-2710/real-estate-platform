#!/usr/bin/env node

const fs = require('fs')
const path = require('path')

class CrawlingDataCleaner {
  constructor() {
    this.outputDir = '/Users/seongjunkim/projects/real-estate-platform/modules/naver-crawler/data/output'
    this.cleanupDir = '/Users/seongjunkim/projects/real-estate-platform/backup/cleanup'
    
    this.stats = {
      total_files: 0,
      unique_complexes: 0,
      duplicate_files: 0,
      small_files: 0,
      large_files: 0,
      deleted_files: 0,
      moved_files: 0,
      kept_files: 0
    }
  }

  async analyzeFiles() {
    console.log('ğŸ” í¬ë¡¤ë§ íŒŒì¼ ë¶„ì„ ì¤‘...')
    console.log('='.repeat(50))

    const files = fs.readdirSync(this.outputDir)
      .filter(file => file.startsWith('enhanced_complex_') && file.endsWith('.json'))
      .sort()

    this.stats.total_files = files.length

    // íŒŒì¼ë³„ ì •ë³´ ìˆ˜ì§‘
    const fileInfo = new Map()
    const complexGroups = new Map()

    for (const fileName of files) {
      const match = fileName.match(/enhanced_complex_(\d+)_(\d{8}_\d{6})\.json/)
      if (!match) continue

      const complexId = match[1]
      const timestamp = match[2]
      const filePath = path.join(this.outputDir, fileName)
      const stats = fs.statSync(filePath)

      const info = {
        fileName,
        complexId,
        timestamp,
        size: stats.size,
        date: new Date(`${timestamp.substring(0,4)}-${timestamp.substring(4,6)}-${timestamp.substring(6,8)} ${timestamp.substring(9,11)}:${timestamp.substring(11,13)}:${timestamp.substring(13,15)}`),
        hasContent: this.hasValidContent(filePath)
      }

      fileInfo.set(fileName, info)

      // ê°™ì€ complex_idë³„ë¡œ ê·¸ë£¹í•‘
      if (!complexGroups.has(complexId)) {
        complexGroups.set(complexId, [])
      }
      complexGroups.get(complexId).push(info)
    }

    this.stats.unique_complexes = complexGroups.size

    console.log(`ğŸ“Š ë¶„ì„ ê²°ê³¼:`)
    console.log(`   â€¢ ì´ íŒŒì¼ ìˆ˜: ${this.stats.total_files}ê°œ`)
    console.log(`   â€¢ ê³ ìœ  ë‹¨ì§€ ìˆ˜: ${this.stats.unique_complexes}ê°œ`)
    console.log(`   â€¢ í‰ê·  ì¤‘ë³µë„: ${(this.stats.total_files / this.stats.unique_complexes).toFixed(1)}ê°œ/ë‹¨ì§€`)

    return { fileInfo, complexGroups }
  }

  hasValidContent(filePath) {
    try {
      const content = fs.readFileSync(filePath, 'utf8')
      const data = JSON.parse(content)
      
      // ìœ íš¨í•œ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
      const listings = data.current_listings || []
      const hasListings = listings.length > 0
      const hasValidListings = listings.some(listing => 
        listing.text && listing.text.length > 10 && 
        !listing.text.includes('ì •ë³´ì—†ìŒ')
      )
      
      return hasListings && hasValidListings
    } catch (error) {
      return false
    }
  }

  async identifyCleanupCandidates(complexGroups) {
    console.log('\nğŸ—‚ï¸ ì •ë¦¬ ëŒ€ìƒ íŒŒì¼ ì‹ë³„ ì¤‘...')
    
    const toDelete = []
    const toKeep = []
    const duplicateGroups = []

    for (const [complexId, files] of complexGroups) {
      if (files.length <= 1) {
        // ë‹¨ì¼ íŒŒì¼ì€ ìœ ì§€
        toKeep.push(...files)
        continue
      }

      // ì¤‘ë³µ íŒŒì¼ë“¤ ë¶„ì„
      duplicateGroups.push({ complexId, count: files.length, files })

      // íŒŒì¼ë“¤ì„ í’ˆì§ˆ ë° ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬
      const sortedFiles = files.sort((a, b) => {
        // 1. ìœ íš¨í•œ ì½˜í…ì¸ ê°€ ìˆëŠ” íŒŒì¼ ìš°ì„ 
        if (a.hasContent !== b.hasContent) {
          return b.hasContent - a.hasContent
        }
        // 2. íŒŒì¼ í¬ê¸°ê°€ í° íŒŒì¼ ìš°ì„  (ë” ë§ì€ ë°ì´í„°)
        if (Math.abs(a.size - b.size) > 1000) {
          return b.size - a.size
        }
        // 3. ìµœì‹  íŒŒì¼ ìš°ì„ 
        return b.date - a.date
      })

      // ìµœê³  í’ˆì§ˆ íŒŒì¼ 1ê°œë§Œ ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” ì‚­ì œ ëŒ€ìƒ
      const bestFile = sortedFiles[0]
      const duplicateFiles = sortedFiles.slice(1)

      toKeep.push(bestFile)
      toDelete.push(...duplicateFiles)

      console.log(`   ğŸ“ ë‹¨ì§€ ${complexId}: ${files.length}ê°œ â†’ 1ê°œ ìœ ì§€ (${duplicateFiles.length}ê°œ ì‚­ì œ)`)
      if (files.length > 5) {
        console.log(`      â­ ìœ ì§€: ${bestFile.fileName} (${(bestFile.size/1024).toFixed(1)}KB, ${bestFile.hasContent ? 'ìœ íš¨' : 'ë¬´íš¨'})`)
      }
    }

    // ì‘ì€ íŒŒì¼ë“¤ (1KB ë¯¸ë§Œ) ì¶”ê°€ ì‹ë³„
    const smallFiles = toKeep.filter(file => file.size < 1000)
    console.log(`\nğŸ” ì¶”ê°€ ì •ë¦¬ ëŒ€ìƒ:`)
    console.log(`   â€¢ ì‘ì€ íŒŒì¼ (1KB ë¯¸ë§Œ): ${smallFiles.length}ê°œ`)
    
    this.stats.duplicate_files = toDelete.length
    this.stats.small_files = smallFiles.length

    return {
      toDelete,
      toKeep: toKeep.filter(file => file.size >= 1000), // ì‘ì€ íŒŒì¼ ì œì™¸
      smallFiles,
      duplicateGroups: duplicateGroups.filter(group => group.count > 3) // 3ê°œ ì´ìƒ ì¤‘ë³µëœ ê²ƒë§Œ
    }
  }

  async createCleanupPlan(cleanupCandidates) {
    console.log('\nğŸ“‹ ì •ë¦¬ ê³„íš ìˆ˜ë¦½ ì¤‘...')

    const plan = {
      summary: {
        current_files: this.stats.total_files,
        files_to_delete: cleanupCandidates.toDelete.length + cleanupCandidates.smallFiles.length,
        files_to_keep: cleanupCandidates.toKeep.length,
        space_to_free: 0
      },
      actions: {
        delete_duplicates: cleanupCandidates.toDelete,
        delete_small_files: cleanupCandidates.smallFiles,
        keep_files: cleanupCandidates.toKeep
      },
      top_duplicates: cleanupCandidates.duplicateGroups.slice(0, 10)
    }

    // ì ˆì•½ë  ê³µê°„ ê³„ì‚°
    const duplicateSize = cleanupCandidates.toDelete.reduce((sum, file) => sum + file.size, 0)
    const smallFileSize = cleanupCandidates.smallFiles.reduce((sum, file) => sum + file.size, 0)
    plan.summary.space_to_free = duplicateSize + smallFileSize

    console.log(`ğŸ“Š ì •ë¦¬ ê³„íš ìš”ì•½:`)
    console.log(`   â€¢ í˜„ì¬ íŒŒì¼: ${plan.summary.current_files}ê°œ`)
    console.log(`   â€¢ ì‚­ì œ ì˜ˆì •: ${plan.summary.files_to_delete}ê°œ`)
    console.log(`   â€¢ ìœ ì§€ ì˜ˆì •: ${plan.summary.files_to_keep}ê°œ`)
    console.log(`   â€¢ ì ˆì•½ ê³µê°„: ${(plan.summary.space_to_free / 1024 / 1024).toFixed(1)}MB`)
    console.log(`   â€¢ ì••ì¶•ë¥ : ${((plan.summary.files_to_delete / plan.summary.current_files) * 100).toFixed(1)}%`)

    return plan
  }

  async executeCleanup(plan, dryRun = true) {
    console.log(`\n${dryRun ? 'ğŸ§ª ì‹œë®¬ë ˆì´ì…˜' : 'ğŸš€ ì‹¤ì œ ì •ë¦¬'} ì‹¤í–‰ ì¤‘...`)

    if (!dryRun) {
      // ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
      if (!fs.existsSync(this.cleanupDir)) {
        fs.mkdirSync(this.cleanupDir, { recursive: true })
      }
    }

    let deletedCount = 0
    let deletedSize = 0

    // ì¤‘ë³µ íŒŒì¼ ì‚­ì œ
    for (const file of plan.actions.delete_duplicates) {
      const filePath = path.join(this.outputDir, file.fileName)
      
      if (dryRun) {
        console.log(`   ğŸ—‘ï¸ [ì‹œë®¬ë ˆì´ì…˜] ì‚­ì œ: ${file.fileName} (${(file.size/1024).toFixed(1)}KB)`)
      } else {
        try {
          // ë°±ì—… í›„ ì‚­ì œ
          const backupPath = path.join(this.cleanupDir, 'duplicates', file.fileName)
          fs.mkdirSync(path.dirname(backupPath), { recursive: true })
          fs.copyFileSync(filePath, backupPath)
          fs.unlinkSync(filePath)
          console.log(`   âœ… ì‚­ì œ: ${file.fileName}`)
        } catch (error) {
          console.error(`   âŒ ì‚­ì œ ì‹¤íŒ¨: ${file.fileName} - ${error.message}`)
          continue
        }
      }
      
      deletedCount++
      deletedSize += file.size
    }

    // ì‘ì€ íŒŒì¼ ì‚­ì œ
    for (const file of plan.actions.delete_small_files) {
      const filePath = path.join(this.outputDir, file.fileName)
      
      if (dryRun) {
        console.log(`   ğŸ—‘ï¸ [ì‹œë®¬ë ˆì´ì…˜] ì‘ì€ íŒŒì¼ ì‚­ì œ: ${file.fileName} (${file.size}B)`)
      } else {
        try {
          const backupPath = path.join(this.cleanupDir, 'small_files', file.fileName)
          fs.mkdirSync(path.dirname(backupPath), { recursive: true })
          fs.copyFileSync(filePath, backupPath)
          fs.unlinkSync(filePath)
          console.log(`   âœ… ì‘ì€ íŒŒì¼ ì‚­ì œ: ${file.fileName}`)
        } catch (error) {
          console.error(`   âŒ ì‚­ì œ ì‹¤íŒ¨: ${file.fileName} - ${error.message}`)
          continue
        }
      }
      
      deletedCount++
      deletedSize += file.size
    }

    this.stats.deleted_files = deletedCount
    this.stats.kept_files = plan.actions.keep_files.length

    console.log(`\n${dryRun ? 'ğŸ§ª ì‹œë®¬ë ˆì´ì…˜' : 'âœ… ì •ë¦¬'} ì™„ë£Œ:`)
    console.log(`   â€¢ ì²˜ë¦¬ëœ íŒŒì¼: ${deletedCount}ê°œ`)
    console.log(`   â€¢ ì ˆì•½ëœ ê³µê°„: ${(deletedSize / 1024 / 1024).toFixed(1)}MB`)
    console.log(`   â€¢ ë‚¨ì€ íŒŒì¼: ${plan.actions.keep_files.length}ê°œ`)

    return {
      deleted: deletedCount,
      size_freed: deletedSize,
      remaining: plan.actions.keep_files.length
    }
  }

  async generateReport(plan, result) {
    const reportPath = path.join(this.cleanupDir, 'cleanup_report.txt')
    
    const report = `
í¬ë¡¤ë§ ë°ì´í„° ì •ë¦¬ ë³´ê³ ì„œ
========================
ìƒì„±ì¼ì‹œ: ${new Date().toLocaleString('ko-KR')}

ğŸ“Š ì •ë¦¬ ì „ í˜„í™©:
- ì´ íŒŒì¼ ìˆ˜: ${this.stats.total_files}ê°œ
- ê³ ìœ  ë‹¨ì§€ ìˆ˜: ${this.stats.unique_complexes}ê°œ
- í‰ê·  ì¤‘ë³µë„: ${(this.stats.total_files / this.stats.unique_complexes).toFixed(1)}ê°œ/ë‹¨ì§€

ğŸ—‘ï¸ ì •ë¦¬ ê²°ê³¼:
- ì‚­ì œëœ íŒŒì¼: ${result.deleted}ê°œ
- ì ˆì•½ëœ ê³µê°„: ${(result.size_freed / 1024 / 1024).toFixed(1)}MB
- ë‚¨ì€ íŒŒì¼: ${result.remaining}ê°œ
- ì •ë¦¬ìœ¨: ${((result.deleted / this.stats.total_files) * 100).toFixed(1)}%

ğŸ” ì£¼ìš” ì¤‘ë³µ ë‹¨ì§€ (ì‚­ì œ ì „):
${plan.top_duplicates.map(group => 
  `- ë‹¨ì§€ ${group.complexId}: ${group.count}ê°œ íŒŒì¼`
).join('\n')}

ğŸ’¾ ë°±ì—… ìœ„ì¹˜:
- ì¤‘ë³µ íŒŒì¼: ${this.cleanupDir}/duplicates/
- ì‘ì€ íŒŒì¼: ${this.cleanupDir}/small_files/

âš ï¸ ë³µì› ë°©ë²•:
ìœ„ ë°±ì—… ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ë“¤ì„ ì›ë³¸ ìœ„ì¹˜ë¡œ ë³µì‚¬í•˜ë©´ ë³µì› ê°€ëŠ¥í•©ë‹ˆë‹¤.
`

    if (!fs.existsSync(this.cleanupDir)) {
      fs.mkdirSync(this.cleanupDir, { recursive: true })
    }
    
    fs.writeFileSync(reportPath, report)
    console.log(`\nğŸ“‹ ì •ë¦¬ ë³´ê³ ì„œ ìƒì„±: ${reportPath}`)
    
    return reportPath
  }

  async run(options = {}) {
    const { dryRun = true, autoConfirm = false } = options
    
    try {
      console.log('ğŸ§¹ í¬ë¡¤ë§ ë°ì´í„° ì •ë¦¬ ë„êµ¬')
      console.log('='.repeat(50))
      
      // 1. íŒŒì¼ ë¶„ì„
      const { fileInfo, complexGroups } = await this.analyzeFiles()
      
      // 2. ì •ë¦¬ ëŒ€ìƒ ì‹ë³„
      const cleanupCandidates = await this.identifyCleanupCandidates(complexGroups)
      
      // 3. ì •ë¦¬ ê³„íš ìˆ˜ë¦½
      const plan = await this.createCleanupPlan(cleanupCandidates)
      
      // 4. ì‚¬ìš©ì í™•ì¸ (ì‹¤ì œ ì‚­ì œì‹œ)
      if (!dryRun && !autoConfirm) {
        console.log('\nâš ï¸  ì‹¤ì œ íŒŒì¼ ì‚­ì œë¥¼ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?')
        console.log('   ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë°±ì—…ì€ ìƒì„±ë©ë‹ˆë‹¤)')
        console.log('   ê³„ì†í•˜ë ¤ë©´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”: node crawlingDataCleaner.js --execute')
        return
      }
      
      // 5. ì •ë¦¬ ì‹¤í–‰
      const result = await this.executeCleanup(plan, dryRun)
      
      // 6. ë³´ê³ ì„œ ìƒì„±
      if (!dryRun) {
        await this.generateReport(plan, result)
      }
      
      console.log(`\nğŸ‰ ${dryRun ? 'ì‹œë®¬ë ˆì´ì…˜' : 'ì •ë¦¬'} ì™„ë£Œ!`)
      
    } catch (error) {
      console.error('âŒ ì •ë¦¬ ì‹¤íŒ¨:', error)
    }
  }
}

// ëª…ë ¹ì¤„ ì¸ìˆ˜ ì²˜ë¦¬
const args = process.argv.slice(2)
const dryRun = !args.includes('--execute')
const autoConfirm = args.includes('--confirm')

// ì‹¤í–‰
if (require.main === module) {
  const cleaner = new CrawlingDataCleaner()
  cleaner.run({ dryRun, autoConfirm })
}

module.exports = CrawlingDataCleaner